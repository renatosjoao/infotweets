{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### https://towardsdatascience.com/bert-text-classification-using-pytorch-723dfb8b6b5b\n",
    "    \n",
    "#!/usr/bin/env python3\n",
    "import math \n",
    "import numpy as np\n",
    "import json\n",
    "import pandas as pd\n",
    "import random\n",
    "import time\n",
    "import datetime\n",
    "import re\n",
    "import tensorflow as tf\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F \n",
    "from transformers import BertModel\n",
    "from torch.utils.data import DataLoader, Dataset, TensorDataset,random_split, RandomSampler, SequentialSampler\n",
    "from transformers import BertTokenizer\n",
    "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from sklearn.metrics import classification_report\n",
    "from torch.utils.data import IterableDataset\n",
    "from torch.utils.data import DataLoader, Dataset, TensorDataset,random_split, RandomSampler, SequentialSampler\n",
    "from sklearn.model_selection import cross_val_score, train_test_split, GridSearchCV\n",
    "import torch.optim as optim\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "torch.cuda.set_device(2)\n",
    "torch.cuda.current_device()\n",
    "\n",
    "\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
    "\n",
    "def format_time(elapsed):\n",
    "    # Round to the nearest second.\n",
    "    elapsed_rounded = int(round((elapsed)))    \n",
    "    # Format as hh:mm:ss\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='binary')\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall\n",
    "    }\n",
    "\n",
    "def report_average(report_list):\n",
    "    r_list = list()\n",
    "    for report in report_list:\n",
    "        splited = [' '.join(x.split()) for x in report.split('\\n\\n')]\n",
    "        header = [x for x in splited[0].split(' ')]\n",
    "        data = np.array(splited[1].split(' ')).reshape(-1, len(header) + 1)\n",
    "        data = np.delete(data, 0, 1).astype(float)\n",
    "        df = pd.DataFrame(data, columns=header)\n",
    "        r_list.append(df)\n",
    "    tmp = pd.DataFrame()\n",
    "    for df in r_list:\n",
    "        tmp = tmp.add(df, fill_value=0)           \n",
    "    report_ave =  tmp/len(r_list)\n",
    "    return(report_ave)\n",
    "\n",
    "def initialize_model(dataloader, epochs=4):\n",
    "    bert_classifier = ourBertBinaryClassifier()\n",
    "    bert_classifier.cuda()\n",
    "    optimizer = AdamW(bert_classifier.parameters(),lr=0.1,eps=1e-8)\n",
    "    total_steps = len(dataloader) * epochs\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer,num_warmup_steps=0,num_training_steps=total_steps)\n",
    "    return bert_classifier, optimizer, scheduler\n",
    "\n",
    "# Specify loss function\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self,filename,name):\n",
    "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "        if name == 'covid':\n",
    "            self.df = pd.read_csv(filename,delimiter='\\t',encoding='utf-8')\n",
    "            #self.train_df = pd.read_csv(\"/home/joao/COVID19Tweet-master/train.tsv\",delimiter='\\t',encoding='utf-8')  \n",
    "            #self.val_df = pd.read_csv(\"/home/joao/COVID19Tweet-master/valid.tsv\",delimiter='\\t',encoding='utf-8')   \n",
    "            self.df = self.df.rename(columns={'Text': 'sentence'})\n",
    "            self.df = self.df.rename(columns={'Label': 'label'})\n",
    "            self.df['label'].replace('INFORMATIVE', 1)\n",
    "            self.df['label'] = self.df['label'].replace('INFORMATIVE', 1)\n",
    "            self.df['label'].replace('UNINFORMATIVE', 0)\n",
    "            self.df['label'] = self.df['label'].replace('UNINFORMATIVE', 0)\n",
    "        if name == 'crisislext26':\n",
    "            self.usr_features = pd.DataFrame(columns=['tweet_id','usr_id','usr_vrf','num_followers','num_friends','num_tweets'])\n",
    "            self.listOfSeries = []\n",
    "            with open(\"/home/joao/crisisLexT26.json\") as json_file:\n",
    "                for line in json_file:\n",
    "                    data = json.loads(line)\n",
    "                    tweet_id  = data['id_str']\n",
    "                    user_obj = data['user']\n",
    "                    usr_id = user_obj['id_str']\n",
    "                    usr_vrf = int(user_obj['verified'])#.astype(int)\n",
    "                    num_followers = user_obj['followers_count']\n",
    "                    if num_followers != 0:\n",
    "                        num_followers = math.log10(num_followers)\n",
    "                    num_friends = user_obj['friends_count']\n",
    "                    if num_friends != 0:\n",
    "                        num_friends = math.log10(num_friends)\n",
    "                    num_tweets = user_obj['statuses_count']\n",
    "                    if num_tweets != 0:\n",
    "                        num_tweets = math.log10(num_tweets)\n",
    "                    self.listOfSeries.append(pd.Series([int(tweet_id), usr_id, usr_vrf, num_followers, num_friends, num_tweets ], index=self.usr_features.columns ))\n",
    "            self.usr_features = self.usr_features.append(self.listOfSeries , ignore_index=True)\n",
    "            self.usr_features = self.usr_features.astype(int)\n",
    "            self.df = pd.read_csv(\"/home/joao/crisisLexT26.csv\", encoding='utf-8')\n",
    "            self.df = self.df.rename(columns={'Tweet ID': 'tweet_id'})\n",
    "            self.df = self.df.drop([' Information Source', ' Information Type' ], axis=1)\n",
    "            #Relabelling the columns titles to remove white spaces\n",
    "            self.df = self.df.rename(columns={' Tweet Text': 'sentence'})\n",
    "            self.df = self.df.rename(columns={' Informativeness': 'label'})\n",
    "            self.fd = pd.DataFrame(columns=list(['sentence', 'usr_vrf', 'num_followers', 'num_friends', 'num_tweets', 'label']))\n",
    "            series = []\n",
    "            for index, row in self.usr_features.iterrows():\n",
    "                tweet_id = row['tweet_id']\n",
    "                usr_vrf, num_followers, num_friends, num_tweets = row['usr_vrf'],row['num_followers'],row['num_friends'],row['num_tweets']\n",
    "                idx = self.df[self.df['tweet_id']==tweet_id].index.values.astype(int)[0]\n",
    "                tweet_id, sentence, label = self.df.loc[idx]\n",
    "                series.append(pd.Series([ sentence , usr_vrf, num_followers, num_friends, num_tweets, label ],index=list(['sentence', 'usr_vrf', 'num_followers', 'num_friends', 'num_tweets', 'label'])))                \n",
    "            self.fd = self.fd.append(series,ignore_index = True )\n",
    "            self.df = self.fd\n",
    "            self.df = self.df.reset_index(drop=True)                             \n",
    "            self.df = self.df[self.df.label!= 'Not related']\n",
    "            self.df = self.df[self.df.label!= 'Not applicable']\n",
    "            self.df['label'].replace('Related and informative', 1)\n",
    "            self.df['label'] = self.df['label'].replace('Related and informative', 1)\n",
    "            self.df['label'].replace('Related - but not informative', 0)\n",
    "            self.df['label'] = self.df['label'].replace('Related - but not informative', 0)\n",
    "            self.df = self.df.reset_index(drop=True)                       \n",
    "        if name == 'crisislext6':\n",
    "            self.df = pd.read_csv(\"/home/joao/crisisLexT6.csv\", encoding='utf-8')\n",
    "            self.df = self.df.rename(columns={' tweet': 'sentence'})\n",
    "            self.df = self.df.rename(columns={' label': 'label'})\n",
    "            self.df['label'].replace('on-topic', 1)\n",
    "            self.df['label'] = self.df['label'].replace('on-topic', 1)\n",
    "            self.df['label'].replace('off-topic', 0)\n",
    "            self.df['label'] = self.df['label'].replace('off-topic', 0)\n",
    "        self.df = self.df[['sentence','label']]\n",
    "        self.df['nchars'] = self.df['sentence'].str.len()\n",
    "        self.df['nwords'] = self.df['sentence'].str.split().str.len()\n",
    "        self.df['bhash'] = self.df[\"sentence\"].str.contains(pat = '#',flags=re.IGNORECASE, regex = True).astype(int) \n",
    "        self.df['nhash'] = self.df[\"sentence\"].str.count('#') \n",
    "        self.df['blink']  = self.df[\"sentence\"].str.contains(pat = 'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', flags=re.IGNORECASE, regex = True) .astype(int)\n",
    "        self.df['nlink'] = self.df[\"sentence\"].str.count(pat = 'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', flags=re.IGNORECASE) \n",
    "        self.df['bat'] = self.df[\"sentence\"].str.contains(pat = '@',flags=re.IGNORECASE, regex = True).astype(int) \n",
    "        self.df['nat'] = self.df[\"sentence\"].str.count(pat = '@') \n",
    "        self.df['rt'] = self.df[\"sentence\"].str.contains(pat = '@rt|rt@',flags=re.IGNORECASE, regex = True).astype(int) \n",
    "        #df['phone'] = df[\"sentence\"].str.contains(pat = '\\(?([0-9]{3})\\)?([ .-]?)([0-9]{3})\\2([0-9]{4})',flags=re.IGNORECASE, regex = True).astype(int) \n",
    "        self.df['dlex'] = self.df[\"sentence\"].apply(self.lexical_diversity)\n",
    "        self.df[\"sentence\"] = self.df[\"sentence\"].str.lower()\n",
    "        ## List of  US slangs.\n",
    "        slangList = ['ASAP','BBIAB','BBL','BBS','BF','BFF','BFFL','BRB','CYA','DS','FAQ','FB','FITBLR','FLBP','FML','FTFY','FTW','FYI','G2G','GF','GR8','GTFO','HBIC','HML','HRU','HTH','IDK','IGHT','IMO','IMHO','IMY','IRL','ISTG','JK','JMHO','KTHX','L8R','LMAO','LMFAO','LMK','LOL','MWF','NM','NOOB','NP','NSFW','OOAK','OFC','OMG','ORLY','OTOH','RN','ROFL','RUH','SFW','SOML','SOZ','STFU','TFTI','TIL','TMI','TTFN','TTYL','TWSS','U','W/','WB','W/O','WYD','WTH','WTF','WYM','WYSIWYG','Y','YMMV','YW','YWA']\n",
    "        slangList = [x.lower() for x in slangList]\n",
    "        #happy emojis\n",
    "        happy_emojis = [':\\)', ';\\)', '\\(:']\n",
    "        #sad emojis\n",
    "        sad_emojis = [':\\(', ';\\(', '\\):']\n",
    "        punctuation = ['.',',','...','?','!',':',';']    \n",
    "        #','-','+','*','_','=','/','','%',' &','{','}','[',']','(',')','\n",
    "        #Checks if the sentence contains slang\n",
    "        mask = self.df.iloc[:, 0].str.contains(r'\\b(?:{})\\b'.format('|'.join(slangList)))\n",
    "        df1 = self.df[~mask]\n",
    "        self.df['slang'] = mask.astype(int) \n",
    "        #Checks if the sentence contains happy emojis\n",
    "        mask = self.df.iloc[:, 0].str.contains(r'\\b(?:{})\\b'.format('|'.join(happy_emojis)), regex = True)\n",
    "        df1 = self.df[~mask]\n",
    "        self.df['hemojis'] = mask.astype(int) \n",
    "        #Checks if the sentence contains happy emojis\n",
    "        mask = self.df.iloc[:, 0].str.contains(r'\\b(?:{})\\b'.format('|'.join(sad_emojis)), regex = True)\n",
    "        df1 = self.df[~mask]\n",
    "        self.df['semojis'] = mask.astype(int) \n",
    "        self.hand_features =  self.df[['nchars', 'nwords','bhash','nhash','blink','nlink','bat','nat','rt','slang','dlex']]\n",
    "        self.hand_features_DF = pd.DataFrame(self.hand_features)\n",
    "        #################\n",
    "        self.df['sentence'] = self.df['sentence'].str.replace(r'http(\\S)+', r'')\n",
    "        self.df['sentence'] = self.df['sentence'].str.replace(r'http(\\S)+', r'')\n",
    "        self.df['sentence'] = self.df['sentence'].str.replace(r'http ...', r'')\n",
    "        self.df['sentence'] = self.df['sentence'].str.replace(r'(RT|rt)[ ]*@[ ]*[\\S]+',r'')\n",
    "        self.df['sentence'] = self.df['sentence'].str.replace(r'@[\\S]+',r'')\n",
    "        self.df['sentence'] = self.df['sentence'].str.replace(r'_[\\S]?',r'')\n",
    "        self.df['sentence'] = self.df['sentence'].str.replace(r'[ ]{2, }',r' ')\n",
    "        self.df['sentence'] = self.df['sentence'].str.replace(r'&amp;?',r'and')\n",
    "        self.df['sentence'] = self.df['sentence'].str.replace(r'&lt;',r'<')\n",
    "        self.df['sentence'] = self.df['sentence'].str.replace(r'&gt;',r'>')\n",
    "        self.df['sentence'] = self.df['sentence'].str.replace(r'([\\w\\d]+)([^\\w\\d ]+)', r'\\1 \\2')\n",
    "        self.df['sentence'] = self.df['sentence'].str.replace(r'([^\\w\\d ]+)([\\w\\d]+)', r'\\1 \\2')\n",
    "        self.df['sentence'] = self.df['sentence'].str.lower()\n",
    "        self.df['sentence'] = self.df['sentence'].str.strip()\n",
    "        self.sentences = self.df['sentence']\n",
    "        self.labels = self.df['label'].values\n",
    "        self.maxlen = 0\n",
    "        if name == 'covid':\n",
    "            self.maxlen = 512\n",
    "        else:\n",
    "            for sent in self.sentences:\n",
    "                input_ids = self.tokenizer.encode(sent, add_special_tokens=True)\n",
    "                self.maxlen = max(self.maxlen, len(input_ids))\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    def __getitem__(self, idx):\n",
    "        sentence = self.df.loc[idx, 'sentence']\n",
    "        label = self.df.loc[idx, 'label']\n",
    "        #h_features = self.hand_features_DF.loc[idx,['nchars', 'nwords','bhash','nhash','blink','nlink','bat','nat','rt','slang','dlex','usr_vrf','num_followers','num_friends','num_tweets']]\n",
    "        h_features = self.hand_features_DF.loc[idx,['nchars', 'nwords','bhash','nhash','blink','nlink','bat','nat','rt','slang','dlex']]\n",
    "        h_features_tensor = torch.tensor(h_features).to(device)\n",
    "        tokens = self.tokenizer.tokenize(sentence)\n",
    "        encoded_dict = self.tokenizer.encode_plus(tokens, add_special_tokens = True, max_length = self.maxlen, pad_to_max_length = True,return_attention_mask = True)\n",
    "        tokens_ids = encoded_dict['input_ids']\n",
    "        tokens_ids_tensor = torch.tensor(tokens_ids).to(device) #Converting the list to a pytorch tensor\n",
    "        attn_mask = encoded_dict['attention_mask']\n",
    "        attn_mask_tensor = torch.tensor(attn_mask).to(device)\n",
    "        label_tensor = torch.tensor(label).to(device)\n",
    "        return tokens_ids_tensor, attn_mask_tensor, label_tensor,h_features_tensor\n",
    "    def lexical_diversity(self,text):\n",
    "        return len(set(text.split())) / len(text.split())\n",
    "    \n",
    "class ourBertBinaryClassifier(nn.Module):\n",
    "    def __init__(self, B_in = 768, B_out = 64, H_in = 11, H_out = 32 ):\n",
    "        super(ourBertBinaryClassifier, self).__init__()\n",
    "        self.bert_layer = BertModel.from_pretrained('bert-base-uncased')\n",
    "        C_in = B_out + H_out \n",
    "        C_out = 2\n",
    "        self.linear_B = nn.Linear(B_in, B_out)\n",
    "        self.linear_H = nn.Linear(H_in, H_out)\n",
    "        #self.classifier = nn.Linear(C_in, C_out)\n",
    "        #self.classifier = nn.Sequential(nn.Linear(B_in, B_out),nn.ReLU(),nn.Linear(B_out, C_out))\n",
    "        self.classifier = nn.Sequential(nn.Linear(B_in, B_out),nn.ReLU(),nn.Linear(H_in,H_out),nn.ReLU(),nn.Linear(C_in, C_out))\n",
    "    def forward(self, seq, attn_masks, hand_features):\n",
    "        # Feed input to BERT\n",
    "        outputs = self.bert_layer(input_ids=seq,attention_mask=attn_masks)\n",
    "        last_hidden_state_cls = outputs[0][:, 0, :]\n",
    "        logits_B = self.linear_B(last_hidden_state_cls.float())\n",
    "        logits_H = self.linear_H(hand_features.float())\n",
    "        cat_features = torch.cat([logits_B.float(),logits_H.float()], dim=1)\n",
    "        # Feed input to classifier to compute logits\n",
    "        logits = self.classifier(cat_features.float())\n",
    "        return logits   \n",
    "\n",
    "def train(model, train_dataloader, validation_dataloader=None, epochs=4, evaluation=False):\n",
    "    print(\"Start training...\\n\")\n",
    "    for epoch_i in range(epochs):\n",
    "        # =======================================\n",
    "        #               Training\n",
    "        # =======================================\n",
    "        # Print the header of the result table\n",
    "        print(f\"{'Epoch':^7} | {'Batch':^7} | {'Train Loss':^12} | {'Val Loss':^10} | {'Val Acc':^9} | {'Elapsed':^9}\")\n",
    "        print(\"-\"*70)\n",
    "        # Measure the elapsed time of each epoch\n",
    "        t0_epoch, t0_batch = time.time(), time.time()\n",
    "        # Reset tracking variables at the beginning of each epoch\n",
    "        total_loss, batch_loss, batch_counts = 0, 0, 0\n",
    "        # Put the model into the training mode\n",
    "        model.train()\n",
    "        # For each batch of training data...\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            batch_counts +=1\n",
    "            # Load batch to GPU\n",
    "            b_input_ids, b_attn_mask, b_labels, h_features = tuple(t.to(device) for t in batch)\n",
    "            # Zero out any previously calculated gradients\n",
    "            model.zero_grad()\n",
    "            # Perform a forward pass. This will return logits.\n",
    "            logits = model(b_input_ids, b_attn_mask, h_features)\n",
    "            # Compute loss and accumulate the loss values\n",
    "            loss = loss_fn(logits, b_labels)\n",
    "            batch_loss += loss.item()\n",
    "            total_loss += loss.item()\n",
    "            # Perform a backward pass to calculate gradients\n",
    "            loss.backward()\n",
    "            # Clip the norm of the gradients to 1.0 to prevent \"exploding gradients\"\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            # Update parameters and the learning rate\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            # Print the loss values and time elapsed for every 20 batches\n",
    "            if (step % 100 == 0 and step != 0) or (step == len(train_dataloader) - 1):\n",
    "                # Calculate time elapsed for 100 batches\n",
    "                time_elapsed = time.time() - t0_batch\n",
    "                # Print training results\n",
    "                print(f\"{epoch_i + 1:^7} | {step:^7} | {batch_loss / batch_counts:^12.6f} | {'-':^10} | {'-':^9} | {time_elapsed:^9.2f}\")\n",
    "                # Reset batch tracking variables\n",
    "                batch_loss, batch_counts = 0, 0\n",
    "                t0_batch = time.time()\n",
    "        # Calculate the average loss over the entire training data\n",
    "        avg_train_loss = total_loss / len(train_dataloader)\n",
    "        print(\"-\"*70)\n",
    "        # =======================================\n",
    "        #               Evaluation\n",
    "        # =======================================\n",
    "        if evaluation == True:\n",
    "            model.eval()\n",
    "            val_accuracy = []\n",
    "            val_loss = []\n",
    "            report_list  = []\n",
    "            for batch in validation_dataloader:\n",
    "                b_input_ids, b_attn_mask, b_labels, h_features = tuple(t.to(device) for t in batch)\n",
    "                b_input_ids = batch[0].to(device)\n",
    "                with torch.no_grad():\n",
    "                    logits = model(b_input_ids, b_attn_mask, h_features)\n",
    "                loss = loss_fn(logits, b_labels)\n",
    "                val_loss.append(loss.item())\n",
    "                preds = torch.argmax(logits, dim=1).flatten()\n",
    "                accuracy = (preds == b_labels).cpu().numpy().mean() * 100\n",
    "                val_accuracy.append(accuracy)\n",
    "                probs =  F.softmax(logits)\n",
    "                probs = probs.detach().cpu().numpy()\n",
    "                predictions  = np.argmax(probs, axis=1).flatten()\n",
    "                label_ids = b_labels.to('cpu').numpy()\n",
    "                labels_flat = label_ids.flatten()\n",
    "                report = classification_report(labels_flat, predictions)\n",
    "                report_list.append(report)\n",
    "            val_loss = np.mean(val_loss)\n",
    "            val_accuracy = np.mean(val_accuracy)\n",
    "            time_elapsed = time.time() - t0_epoch\n",
    "            print(f\"{epoch_i + 1:^7} | {'-':^7} | {avg_train_loss:^12.6f} | {val_loss:^10.6f} | {val_accuracy:^9.2f} | {time_elapsed:^9.2f}\")\n",
    "            print(\"-\"*70)\n",
    "            print(\"\\n\")\n",
    "            print(\"Training complete!\")\n",
    "            print()\n",
    "            print(report_average(report_list))\n",
    "            print()\n",
    "\n",
    "            \n",
    "        \n",
    "def train_and_evalCV(model, dataset, cv=10, epochs=nepochs, batch_size = batch_size):\n",
    "    training_stats = []\n",
    "    stats = []\n",
    "    total_t0 = time.time()\n",
    "    train_dataloader = DataLoader(dataset,sampler = SequentialSampler(dataset), batch_size = batch_size )\n",
    "    kf = KFold(n_splits=cv)\n",
    "    for train_index, test_index in kf.split(dataset): # For each fold...\n",
    "        train_dataset = torch.utils.data.Subset(dataset,train_index)\n",
    "        val_dataset  = torch.utils.data.Subset(dataset, test_index)\n",
    "        train_dataloader = DataLoader(train_dataset, sampler = SequentialSampler(train_dataset),batch_size = batch_size)\n",
    "        val_dataloader = DataLoader(val_dataset, sampler = SequentialSampler(val_dataset),batch_size = batch_size)    \n",
    "        model, optimizer, scheduler = initialize_model(epochs=nepochs)\n",
    "        model.cuda()\n",
    "        for epoch_i in range(0, nepochs):  # For each epoch...\n",
    "            #print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "            t0 = time.time()\n",
    "            total_train_loss = 0\n",
    "            model.train()\n",
    "            for step, batch in enumerate(train_dataloader):\n",
    "                b_input_ids, b_attn_mask, b_labels, h_features = tuple(t.to(device) for t in batch)\n",
    "                model.zero_grad()\n",
    "                logits = model(b_input_ids, b_attn_mask, h_features)\n",
    "                loss = loss_fn(logits, b_labels)\n",
    "                batch_loss += loss.item()\n",
    "                total_train_los += loss.item()\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "            avg_train_loss = total_train_loss / len(train_dataloader)\n",
    "            training_time = format_time(time.time() - t0)\n",
    "            training_stats.append({'epoch': epoch_i + 1,'Training Loss': avg_train_loss,'Training Time': training_time})\n",
    "            print({'epoch': epoch_i + 1,'Training Loss': avg_train_loss,'Training Time': training_time})\n",
    "#    sss = StratifiedShuffleSplit(n_splits=cv, test_size=0.5, random_state=0)\n",
    "#    skf = StratifiedKFold(n_splits=cv)\n",
    "        predictions , true_labels = [], []\n",
    "        model.eval()\n",
    "        for step, batch in enumerate(val_dataloader):\n",
    "            b_input_ids, b_attn_mask, b_labels, h_features = tuple(t.to(device) for t in batch)\n",
    "            with torch.no_grad():\n",
    "                logits = model(b_input_ids, b_attn_mask, h_features)\n",
    "            logits = logits.detach().cpu().numpy()\n",
    "            label_ids = b_labels.to('cpu').numpy()\n",
    "            pred_flat = np.argmax(logits, axis=1).flatten()\n",
    "            labels_flat = label_ids.flatten()\n",
    "            true_labels = np.concatenate((true_labels,labels_flat))\n",
    "            predictions = np.concatenate((predictions,pred_flat))\n",
    "        stats.append(precision_recall_fscore_support(true_labels, predictions, average='macro'))\n",
    "        print(precision_recall_fscore_support(true_labels, predictions, average='macro'))\n",
    "        print(classification_report(true_labels,predictions))\n",
    "    print(stats)\n",
    "    \n",
    "    \n",
    " \n",
    "train_dataset =  CustomDataset(\"/home/joao/COVID19Tweet-master/train.tsv\",\"covid\")\n",
    "val_dataset =  CustomDataset(\"/home/joao/COVID19Tweet-master/valid.tsv\",\"covid\")\n",
    "datasets = [train_dataset,val_dataset]\n",
    "dataset = torch.utils.data.ConcatDataset(datasets)\n",
    "\n",
    "dataset =  CustomDataset(None,\"crisislext6\")\n",
    "\n",
    "dataset =  CustomDataset(None,\"crisislext26\")\n",
    "\n",
    "\n",
    "train_size = int(0.9 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "\n",
    "batch_size = 8\n",
    "train_dataloader = DataLoader(train_dataset,sampler = SequentialSampler(train_dataset), batch_size = batch_size )\n",
    "validation_dataloader = DataLoader(val_dataset,sampler = SequentialSampler(val_dataset),batch_size = batch_size)\n",
    "\n",
    "\n",
    "#complete data set\n",
    "dataloader = DataLoader(dataset,sampler = SequentialSampler(dataset), batch_size = batch_size )\n",
    "\n",
    "\n",
    "\n",
    "set_seed(42)    # Set seed for reproducibility\n",
    "our_bert_classifier, optimizer, scheduler = initialize_model(dataloader,epochs=4)\n",
    "train(our_bert_classifier, dataloader, validation_dataloader, epochs=4, evaluation=False)\n",
    "\n",
    "\n",
    "\n",
    "train_and_evalCV(our_bert_classifier, dataset, cv=10, epochs=10, batch_size = 8)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
