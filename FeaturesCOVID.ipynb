{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score, train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import make_moons, make_circles, make_classification\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, confusion_matrix\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "import tensorflow as tf\n",
    "import torch \n",
    "from torch.utils.data import DataLoader, Dataset, TensorDataset\n",
    "from torch.utils.data.sampler import SequentialSampler\n",
    "from transformers import BertTokenizer\n",
    "import transformers as ppb \n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "class CovidDataset(Dataset):\n",
    "    def __init__(self,filename):\n",
    "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "        self.df = pd.read_csv(filename,delimiter='\\t',encoding='utf-8')  \n",
    "        #self.train_df = pd.read_csv(\"/home/joao/COVID19Tweet-master/train.tsv\",delimiter='\\t',encoding='utf-8')  \n",
    "        #self.val_df = pd.read_csv(\"/home/joao/COVID19Tweet-master/valid.tsv\",delimiter='\\t',encoding='utf-8')   \n",
    "        self.df = self.df.rename(columns={'Text': 'sentence'})\n",
    "        self.df = self.df.rename(columns={'Label': 'label'})\n",
    "        self.df['label'].replace('INFORMATIVE', 1)\n",
    "        self.df['label'] = self.df['label'].replace('INFORMATIVE', 1)\n",
    "        self.df['label'].replace('UNINFORMATIVE', 0)\n",
    "        self.df['label'] = self.df['label'].replace('UNINFORMATIVE', 0)\n",
    "        self.df = self.df[['sentence','label']]\n",
    "        self.df['nchars'] = self.df['sentence'].str.len()\n",
    "        self.df['nwords'] = self.df['sentence'].str.split().str.len()\n",
    "        self.df['bhash'] = self.df[\"sentence\"].str.contains(pat = '#',flags=re.IGNORECASE, regex = True).astype(int) \n",
    "        self.df['nhash'] = self.df[\"sentence\"].str.count('#') \n",
    "        self.df['blink']  = self.df[\"sentence\"].str.contains(pat = 'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', flags=re.IGNORECASE, regex = True) .astype(int)\n",
    "        self.df['nlink'] = self.df[\"sentence\"].str.count(pat = 'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', flags=re.IGNORECASE) \n",
    "        self.df['bat'] = self.df[\"sentence\"].str.contains(pat = '@',flags=re.IGNORECASE, regex = True).astype(int) \n",
    "        self.df['nat'] = self.df[\"sentence\"].str.count(pat = '@') \n",
    "        self.df['rt'] = self.df[\"sentence\"].str.contains(pat = '@rt|rt@',flags=re.IGNORECASE, regex = True).astype(int) \n",
    "        #df['phone'] = df[\"sentence\"].str.contains(pat = '\\(?([0-9]{3})\\)?([ .-]?)([0-9]{3})\\2([0-9]{4})',flags=re.IGNORECASE, regex = True).astype(int) \n",
    "        self.df['dlex'] = self.df[\"sentence\"].apply(self.lexical_diversity)\n",
    "        self.df[\"sentence\"] = self.df[\"sentence\"].str.lower()\n",
    "        ## List of  US slangs.\n",
    "        slangList = ['ASAP','BBIAB','BBL','BBS','BF','BFF','BFFL','BRB','CYA','DS','FAQ','FB','FITBLR','FLBP','FML','FTFY','FTW','FYI','G2G','GF','GR8','GTFO','HBIC','HML','HRU','HTH','IDK','IGHT','IMO','IMHO','IMY','IRL','ISTG','JK','JMHO','KTHX','L8R','LMAO','LMFAO','LMK','LOL','MWF','NM','NOOB','NP','NSFW','OOAK','OFC','OMG','ORLY','OTOH','RN','ROFL','RUH','SFW','SOML','SOZ','STFU','TFTI','TIL','TMI','TTFN','TTYL','TWSS','U','W/','WB','W/O','WYD','WTH','WTF','WYM','WYSIWYG','Y','YMMV','YW','YWA']\n",
    "        slangList = [x.lower() for x in slangList]\n",
    "        #happy emojis\n",
    "        happy_emojis = [':\\)', ';\\)', '\\(:']\n",
    "        #sad emojis\n",
    "        sad_emojis = [':\\(', ';\\(', '\\):']\n",
    "        punctuation = ['.',',','...','?','!',':',';']    \n",
    "        #','-','+','*','_','=','/','','%',' &','{','}','[',']','(',')','\n",
    "        #Checks if the sentence contains slang\n",
    "        mask = self.df.iloc[:, 0].str.contains(r'\\b(?:{})\\b'.format('|'.join(slangList)))\n",
    "        df1 = self.df[~mask]\n",
    "        self.df['slang'] = mask.astype(int) \n",
    "        #Checks if the sentence contains happy emojis\n",
    "        mask = self.df.iloc[:, 0].str.contains(r'\\b(?:{})\\b'.format('|'.join(happy_emojis)), regex = True)\n",
    "        df1 = self.df[~mask]\n",
    "        self.df['hemojis'] = mask.astype(int) \n",
    "        #Checks if the sentence contains happy emojis\n",
    "        mask = self.df.iloc[:, 0].str.contains(r'\\b(?:{})\\b'.format('|'.join(sad_emojis)), regex = True)\n",
    "        df1 = self.df[~mask]\n",
    "        self.df['semojis'] = mask.astype(int) \n",
    "        self.hand_features =  self.df[['nchars', 'nwords','bhash','nhash','blink','nlink','bat','nat','rt','slang','dlex']]\n",
    "        self.hand_features_DF = pd.DataFrame(self.hand_features)\n",
    "        #################\n",
    "        self.df['sentence'] = self.df['sentence'].str.replace(r'http(\\S)+', r'')\n",
    "        self.df['sentence'] = self.df['sentence'].str.replace(r'http(\\S)+', r'')\n",
    "        self.df['sentence'] = self.df['sentence'].str.replace(r'http ...', r'')\n",
    "        self.df['sentence'] = self.df['sentence'].str.replace(r'(RT|rt)[ ]*@[ ]*[\\S]+',r'')\n",
    "        self.df['sentence'] = self.df['sentence'].str.replace(r'@[\\S]+',r'')\n",
    "        self.df['sentence'] = self.df['sentence'].str.replace(r'_[\\S]?',r'')\n",
    "        self.df['sentence'] = self.df['sentence'].str.replace(r'[ ]{2, }',r' ')\n",
    "        self.df['sentence'] = self.df['sentence'].str.replace(r'&amp;?',r'and')\n",
    "        self.df['sentence'] = self.df['sentence'].str.replace(r'&lt;',r'<')\n",
    "        self.df['sentence'] = self.df['sentence'].str.replace(r'&gt;',r'>')\n",
    "        self.df['sentence'] = self.df['sentence'].str.replace(r'([\\w\\d]+)([^\\w\\d ]+)', r'\\1 \\2')\n",
    "        self.df['sentence'] = self.df['sentence'].str.replace(r'([^\\w\\d ]+)([\\w\\d]+)', r'\\1 \\2')\n",
    "        self.df['sentence'] = self.df['sentence'].str.lower()\n",
    "        self.df['sentence'] = self.df['sentence'].str.strip()\n",
    "        self.sentences = self.df['sentence']\n",
    "        self.labels = self.df['label'].values\n",
    "        self.maxlen = 512\n",
    "        #for sent in self.sentences:\n",
    "        #    input_ids = self.tokenizer.encode(sent, add_special_tokens=True)\n",
    "        #    self.maxlen = max(self.maxlen, len(input_ids))\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    def __getitem__(self, idx):\n",
    "        sentence = self.df.loc[idx, 'sentence']\n",
    "        label = self.df.loc[idx, 'label']\n",
    "        h_features = self.hand_features_DF.loc[idx,['nchars', 'nwords','bhash','nhash','blink','nlink','bat','nat','rt','slang','dlex']]\n",
    "        h_features_tensor = torch.tensor(h_features).to(device)\n",
    "        tokens = self.tokenizer.tokenize(sentence)\n",
    "        tokens = ['[CLS]'] + tokens + ['[SEP]']\n",
    "        if len(tokens) < self.maxlen:\n",
    "            tokens = tokens + ['[PAD]' for _ in range(self.maxlen - len(tokens))] #Padding sentences\n",
    "        else:\n",
    "            tokens = tokens[:self.maxlen-1] + ['[SEP]'] #Prunning the list to be of specified max length\n",
    "        tokens_ids = self.tokenizer.convert_tokens_to_ids(tokens) #Obtaining the indices of the tokens in the BERT Vocabulary\n",
    "        tokens_ids_tensor = torch.tensor(tokens_ids).to(device) #Converting the list to a pytorch tensor\n",
    "        attn_mask = (tokens_ids_tensor != 0).float()\n",
    "        attn_mask_tensor = torch.tensor(attn_mask).to(device)\n",
    "        label_tensor = torch.tensor(label).to(device)\n",
    "        return tokens_ids_tensor, attn_mask_tensor, label_tensor,h_features_tensor\n",
    "    def lexical_diversity(self,text):\n",
    "        return len(set(text.split())) / len(text.split())\n",
    "\n",
    "\n",
    "train_dataset =  CovidDataset(\"/home/joao/COVID19Tweet-master/train.tsv\")\n",
    "val_dataset =  CovidDataset(\"/home/joao/COVID19Tweet-master/valid.tsv\")\n",
    "\n",
    "\n",
    "# Load pretrained model/tokenizer\n",
    "model_class, tokenizer_class, pretrained_weights = (ppb.BertModel, ppb.BertTokenizer, 'bert-base-uncased')\n",
    "model = model_class.from_pretrained(pretrained_weights)\n",
    "\n",
    "# Tell pytorch to run this model on the GPU.\n",
    "model.cuda()\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "# Create the DataLoaders for our training and validation sets.\n",
    "train_dataloader = DataLoader(train_dataset, sampler = SequentialSampler(train_dataset),batch_size = batch_size)\n",
    "val_dataloader = DataLoader(val_dataset, sampler = SequentialSampler(val_dataset),batch_size = batch_size)\n",
    "\n",
    "train_BertdfLabels = pd.DataFrame()\n",
    "train_BertdfFeatures = pd.DataFrame()\n",
    "train_h_dfFeatures = pd.DataFrame()\n",
    "# For each batch of training data...\n",
    "for batch in train_dataloader:\n",
    "    with torch.no_grad():\n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "        b_h_features = batch[3].to(device)\n",
    "        last_hidden_states = model(b_input_ids,attention_mask = b_input_mask)\n",
    "        bertfeatures = last_hidden_states[0][:,0,:]#Let's slice only the part of the output that we need. That is the output corresponding the first token of each sentence.  The way BERT does sentence classification, is that it adds a token called [CLS] (for classification) at the beginning of every sentence. The output corresponding to that token can be thought of as an embedding for the entire sentence.\n",
    "        bertfeatures = bertfeatures.cpu().detach().numpy()        \n",
    "        labels = b_labels.cpu().detach().numpy()\n",
    "        h_features = b_h_features.cpu().detach().numpy()\n",
    "        train_BertdfLabels = train_BertdfLabels.append(pd.DataFrame(labels),ignore_index = True)\n",
    "        train_BertdfFeatures = train_BertdfFeatures.append(pd.DataFrame(bertfeatures),ignore_index = True)\n",
    "        train_h_dfFeatures = train_h_dfFeatures.append(pd.DataFrame(h_features),ignore_index = True)\n",
    "\n",
    "        \n",
    "val_BertdfLabels = pd.DataFrame()\n",
    "val_BertdfFeatures = pd.DataFrame()\n",
    "val_h_dfFeatures = pd.DataFrame()\n",
    "# For each batch of validation data...\n",
    "for batch in val_dataloader:\n",
    "    with torch.no_grad():\n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "        b_h_features = batch[3].to(device)\n",
    "        last_hidden_states = model(b_input_ids,attention_mask = b_input_mask)\n",
    "        bertfeatures = last_hidden_states[0][:,0,:]#Let's slice only the part of the output that we need. That is the output corresponding the first token of each sentence.  The way BERT does sentence classification, is that it adds a token called [CLS] (for classification) at the beginning of every sentence. The output corresponding to that token can be thought of as an embedding for the entire sentence.\n",
    "        bertfeatures = bertfeatures.cpu().detach().numpy()        \n",
    "        labels = b_labels.cpu().detach().numpy()\n",
    "        h_features = b_h_features.cpu().detach().numpy()\n",
    "        val_BertdfLabels = val_BertdfLabels.append(pd.DataFrame(labels),ignore_index = True)\n",
    "        val_BertdfFeatures = val_BertdfFeatures.append(pd.DataFrame(bertfeatures),ignore_index = True)\n",
    "        val_h_dfFeatures = val_h_dfFeatures.append(pd.DataFrame(h_features),ignore_index = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Model 2. Train and Test Split \n",
    "# The output from BERT is going to be input to SKLEARN\n",
    "\n",
    "### Hand designed Features  only\n",
    "features = features.reset_index(drop=True)\n",
    "train_features, test_features, train_labels, test_labels = train_test_split(features, labels, test_size=0.33, random_state=42)\n",
    "\n",
    "train_labels = train_BertdfLabels\n",
    "train_features = train_BertdfFeatures\n",
    "\n",
    "train_h_dfFeatures\n",
    "\n",
    "test_features = val_BertdfFeatures\n",
    "test_labels = val_BertdfLabels\n",
    "\n",
    "val_h_dfFeatures \n",
    "#\n",
    "\n",
    "### Bert features only ( BertModel [CLS] token )\n",
    "train_features, test_features, train_labels, test_labels = train_test_split(dfFeatures, dfLabels,test_size=0.33, random_state=42)\n",
    "\n",
    "\n",
    "### Combined featues  BertModel [CLS] token  + Hand designed Features\n",
    "combine_df = pd.concat([dfFeatures, features],axis=1, ignore_index=True)\n",
    "train_features, test_features, train_labels, test_labels = train_test_split(combine_df, dfLabels,test_size=0.33, random_state=42)\n",
    "\n",
    "\n",
    "lr_clf = LogisticRegression()\n",
    "lr_clf.fit(train_features, train_labels)\n",
    "\n",
    "dt_clf = DecisionTreeClassifier()\n",
    "dt_clf.fit(train_features, train_labels)\n",
    "\n",
    "svm_clf = svm.SVC(gamma=0.001, C=100.)\n",
    "svm_clf.fit(train_features, train_labels)\n",
    "\n",
    "rf_clf = RandomForestClassifier()\n",
    "rf_clf.fit(train_features, train_labels)\n",
    "\n",
    "ab_clf = AdaBoostClassifier(n_estimators=100, random_state=0)\n",
    "ab_clf.fit(train_features, train_labels)\n",
    "\n",
    "nb_clf = GaussianNB()\n",
    "nb_clf.fit(train_features, train_labels)\n",
    "\n",
    "nn_clf = MLPClassifier(random_state=1, max_iter=300)\n",
    "nn_clf.fit(train_features, train_labels)\n",
    "\n",
    "\n",
    "#Evaluating Model #2\n",
    "#So how well does our model do in classifying sentences? One way is to check the accuracy against the testing dataset:\n",
    "lr_clf.score(test_features, test_labels)\n",
    "dt_clf.score(test_features, test_labels)\n",
    "rf_clf.score(test_features, test_labels)\n",
    "ab_clf.score(test_features, test_labels)\n",
    "nb_clf.score(test_features, test_labels)\n",
    "nn_clf.score(test_features, test_labels)\n",
    "svm_clf.score(test_features, test_labels)\n",
    "\n",
    "y_pred = lr_clf.predict(test_features)\n",
    "y_pred = dt_clf.predict(test_features)\n",
    "y_pred = rf_clf.predict(test_features)\n",
    "y_pred = ab_clf.predict(test_features)\n",
    "y_pred = nb_clf.predict(test_features)\n",
    "y_pred = nn_clf.predict(test_features)\n",
    "y_pred = svm_clf.predict(test_features)\n",
    "\n",
    "print('Accuracy:', accuracy_score(test_labels, y_pred))\n",
    "print('F1 score:', f1_score(test_labels, y_pred, average='macro'))\n",
    "print('Recall:', recall_score(test_labels, y_pred, average='macro'))\n",
    "print('Precision:', precision_score(test_labels, y_pred, average='macro'))\n",
    "print('\\n clasification report:\\n', classification_report(test_labels,y_pred))\n",
    "print('\\n confussion matrix:\\n',confusion_matrix(test_labels, y_pred))\n",
    "\n",
    "\n",
    "\n",
    "##### Grid Search for Parameters\n",
    "#We can dive into Logistic regression directly with the Scikit Learn default parameters, but sometimes it's worth searching for the best value of the C parameter, which determines regularization strength.\n",
    "parameters = {'C': np.linspace(0.0001, 100, 20)}\n",
    "grid_search = GridSearchCV(LogisticRegression(), parameters)\n",
    "grid_search.fit(train_features, train_labels)\n",
    "\n",
    "print('best parameters: ', grid_search.best_params_)\n",
    "print('best scrores: ', grid_search.best_score_)\n",
    "\n",
    "target_names = ['class 0', 'class 1']\n",
    "print(classification_report(test_labels, y_pred, target_names=target_names))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
