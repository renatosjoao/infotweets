{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/renato/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/renato/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/renato/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/renato/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/renato/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/renato/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/renato/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/renato/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/renato/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/renato/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/renato/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/renato/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading \n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/joao/crisisLexT26.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-c0e26b2fad24>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;31m#df = pd.read_csv(\"/home/renato/Datasets/CrisisLexT6-v1.0/CrisisLexT6/2012_Sandy_Hurricane/2012_Sandy_Hurricane-ontopic_offtopic.csv\", encoding='utf-8')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/home/joao/crisisLexT26.csv\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    674\u001b[0m         )\n\u001b[1;32m    675\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 676\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    678\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    446\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 448\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    449\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    878\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 880\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    881\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    882\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1112\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1113\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1114\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1115\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1116\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1872\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"compression\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1873\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1874\u001b[0;31m                 \u001b[0msrc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1875\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1876\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/joao/crisisLexT26.csv'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score, train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import make_moons, make_circles, make_classification\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, confusion_matrix\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "import tensorflow as tf\n",
    "import torch \n",
    "from torch.utils.data import DataLoader, Dataset, TensorDataset\n",
    "from torch.utils.data.sampler import SequentialSampler\n",
    "import transformers as ppb \n",
    "from transformers import BertTokenizer\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "\n",
    "# Loading the complete dataset into a pandas dataframe.\n",
    "print(\"Loading \")\n",
    "df = pd.read_csv(\"/home/joao/crisisLexT26.csv\",encoding='utf-8')\n",
    "\n",
    "df.head(5)\n",
    "print(df.keys())\n",
    "# Report the number of sentences.\n",
    "print()\n",
    "print('Number of sentences in the original dataset: {:,}\\n'.format(df.shape[0]))\n",
    "\n",
    "\n",
    "\n",
    "#Dropping useless columns as I will only be using the tweet text and the corresponding labeli#df = df[['sentence','label']]\n",
    "df = df.drop(['Tweet ID', ' Information Source', ' Information Type' ], axis=1)\n",
    "\n",
    "\n",
    "#Relabelling the columns titles to remove white spaces\n",
    "df = df.rename(columns={' Tweet Text': 'sentence'})\n",
    "df = df.rename(columns={' Informativeness': 'label'})\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df = df[df.label!= 'Not related']\n",
    "df = df[df.label!= 'Not applicable']\n",
    "\n",
    "df['label'].replace('Related and informative', 1)\n",
    "df['label'] = df['label'].replace('Related and informative', 1)\n",
    "\n",
    "df['label'].replace('Related - but not informative', 0)\n",
    "df['label'] = df['label'].replace('Related - but not informative', 0)\n",
    "\n",
    "\n",
    "\n",
    "#Dropping useless columns as I will only be using the tweet text and the corresponding label\n",
    "df = df[['sentence','label']]\n",
    "print(df.keys())\n",
    "print(df['label'].value_counts())\n",
    "\n",
    "df.head(5)\n",
    "\n",
    "\n",
    "print(df['label'].value_counts())\n",
    "print(df.dtypes)\n",
    "#print(df.head())\n",
    "\n",
    "labels = df['label'].values\n",
    "\n",
    "sentences = df['sentence']\n",
    "\n",
    "def lexical_diversity(text):\n",
    "    return len(set(text.split())) / len(text.split()) \n",
    "\n",
    "#Returns the number of characters in a string.\n",
    "df['nchars'] = df['sentence'].str.len()\n",
    "\n",
    "#Returns the number of words in a string.\n",
    "df['nwords'] = df['sentence'].str.split().str.len()\n",
    "\n",
    "# Checks whether the sentence contains # hashtags\n",
    "df['bhash'] = df[\"sentence\"].str.contains(pat = '#',flags=re.IGNORECASE, regex = True).astype(int) \n",
    "\n",
    "# Count occurrences of #\n",
    "df['nhash'] = df[\"sentence\"].str.count('#') \n",
    "\n",
    "# Check whether the sentence contains URLs\n",
    "df['blink']  = df[\"sentence\"].str.contains(pat = 'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', flags=re.IGNORECASE, regex = True) .astype(int)\n",
    "\n",
    "# Count occurrences of URLs\n",
    "df['nlink'] = df[\"sentence\"].str.count(pat = 'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', flags=re.IGNORECASE) \n",
    "\n",
    "#Checks whether the sentence contains @\n",
    "df['bat'] = df[\"sentence\"].str.contains(pat = '@',flags=re.IGNORECASE, regex = True).astype(int) \n",
    "\n",
    "#Count occurrences of  @\n",
    "df['nat'] = df[\"sentence\"].str.count(pat = '@') \n",
    "\n",
    "#Checks whether the sentence has retweet or not  \n",
    "df['rt'] = df[\"sentence\"].str.contains(pat = '@rt|rt@',flags=re.IGNORECASE, regex = True).astype(int) \n",
    "\n",
    "# Checks whether the sentence contains phone number\n",
    "#df['phone'] = df[\"sentence\"].str.contains(pat = '\\(?([0-9]{3})\\)?([ .-]?)([0-9]{3})\\2([0-9]{4})',flags=re.IGNORECASE, regex = True).astype(int) \n",
    "\n",
    "\n",
    "df['dlex'] = df[\"sentence\"].apply(lexical_diversity)\n",
    "\n",
    "# Lowering case\n",
    "df[\"sentence\"] = df[\"sentence\"].str.lower()\n",
    "\n",
    "# List of  US slangs.\n",
    "slangList = ['ASAP','BBIAB','BBL','BBS','BF','BFF','BFFL','BRB','CYA','DS','FAQ','FB','FITBLR','FLBP','FML','FTFY','FTW','FYI','G2G','GF','GR8','GTFO','HBIC','HML','HRU','HTH','IDK','IGHT','IMO','IMHO','IMY','IRL','ISTG','JK','JMHO','KTHX','L8R','LMAO','LMFAO','LMK','LOL','MWF','NM','NOOB','NP','NSFW','OOAK','OFC','OMG','ORLY','OTOH','RN','ROFL','RUH','SFW','SOML','SOZ','STFU','TFTI','TIL','TMI','TTFN','TTYL','TWSS','U','W/','WB','W/O','WYD','WTH','WTF','WYM','WYSIWYG','Y','YMMV','YW','YWA']\n",
    "slangList = [x.lower() for x in slangList]\n",
    "\n",
    "#happy emojis\n",
    "happy_emojis = [':\\)', ';\\)', '\\(:']\n",
    "\n",
    "#sad emojis\n",
    "sad_emojis = [':\\(', ';\\(', '\\):']\n",
    "\n",
    "\n",
    "punctuation = ['.',',','...','?','!',':',';']    \n",
    "#','-','+','*','_','=','/','','%',' &','{','}','[',']','(',')','\n",
    "\n",
    "#Checks if the sentence contains slang\n",
    "mask = df.iloc[:, 0].str.contains(r'\\b(?:{})\\b'.format('|'.join(slangList)))\n",
    "df1 = df[~mask]\n",
    "df['slang'] = mask.astype(int) \n",
    "\n",
    "#Checks if the sentence contains happy emojis\n",
    "mask = df.iloc[:, 0].str.contains(r'\\b(?:{})\\b'.format('|'.join(happy_emojis)), regex = True)\n",
    "df1 = df[~mask]\n",
    "df['hemojis'] = mask.astype(int) \n",
    "\n",
    "#Checks if the sentence contains happy emojis\n",
    "mask = df.iloc[:, 0].str.contains(r'\\b(?:{})\\b'.format('|'.join(sad_emojis)), regex = True)\n",
    "df1 = df[~mask]\n",
    "df['semojis'] = mask.astype(int) \n",
    "\n",
    "\n",
    "features =  df[['nchars', 'nwords','bhash','nhash','blink','nlink','bat','nat','rt','slang','dlex']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-ec8afe8e9241>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'nchars'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'nwords'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'bhash'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'nhash'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'blink'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'nlink'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'bat'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'nat'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'rt'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'slang'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'dlex'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#features = df\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#df[['label']]  = df[['label']].replace(to_replace=r'on-topic', value='1', regex=True)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "                        \n",
    "### BERT\n",
    "\n",
    "                        #### Doing all the text pre processing\n",
    "        \n",
    "\n",
    "# Get the GPU device name.\n",
    "device_name = tf.test.gpu_device_name()\n",
    "# The device name should look like the following:\n",
    "if device_name == '/device:GPU:0':\n",
    "    print('Found GPU at: {}'.format(device_name))\n",
    "else:\n",
    "    raise SystemError('GPU device not found')\n",
    "    \n",
    "\n",
    "# If there's a GPU available...\n",
    "if torch.cuda.is_available():    \n",
    "    device = torch.device(\"cuda\")\n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")\n",
    "    \n",
    "\n",
    "sentences = df['sentence']\n",
    "sentences.head()\n",
    "\n",
    "### Remove URL, RT, mention(@)\n",
    "df.ProcessedText = df.sentence.str.replace(r'http(\\S)+', r'')\n",
    "df.ProcessedText = df.ProcessedText.str.replace(r'http ...', r'')\n",
    "df.ProcessedText[df.ProcessedText.str.contains(r'http')]\n",
    "df.ProcessedText = df.ProcessedText.str.replace(r'(RT|rt)[ ]*@[ ]*[\\S]+',r'')\n",
    "df.ProcessedText[df.ProcessedText.str.contains(r'RT[ ]?@')]\n",
    "df.ProcessedText = df.ProcessedText.str.replace(r'@[\\S]+',r'')\n",
    "df.ProcessedText = df.ProcessedText.str.replace(r'_[\\S]?',r'')\n",
    "\n",
    "#Remove extra space\n",
    "df.ProcessedText = df.ProcessedText.str.replace(r'[ ]{2, }',r' ')\n",
    "\n",
    "#Removing &, < and >\n",
    "df.ProcessedText = df.ProcessedText.str.replace(r'&amp;?',r'and')\n",
    "\n",
    "#Remove extra space\n",
    "df.ProcessedText = df.ProcessedText.str.replace(r'&lt;',r'<')\n",
    "df.ProcessedText = df.ProcessedText.str.replace(r'&gt;',r'>')\n",
    "\n",
    "#Insert space between words and punctuation marks\n",
    "df.ProcessedText = df.ProcessedText.str.replace(r'([\\w\\d]+)([^\\w\\d ]+)', r'\\1 \\2')\n",
    "df.ProcessedText = df.ProcessedText.str.replace(r'([^\\w\\d ]+)([\\w\\d]+)', r'\\1 \\2')\n",
    "\n",
    "#Lowercased and strip\n",
    "df.ProcessedText = df.ProcessedText.str.lower()\n",
    "df.ProcessedText = df.ProcessedText.str.strip()\n",
    "\n",
    "sentences = df.ProcessedText\n",
    "\n",
    "# Load the BERT tokenizer.\n",
    "print('Loading BERT tokenizer...')\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "\n",
    "max_len = 0\n",
    "# For every sentence...\n",
    "for sent in sentences:\n",
    "    # Tokenize the text and add `[CLS]` and `[SEP]` tokens.\n",
    "    input_ids = tokenizer.encode(sent, add_special_tokens=True)\n",
    "    # Update the maximum sentence length.\n",
    "    max_len = max(max_len, len(input_ids))\n",
    "\n",
    "print('Max sentence length: ', max_len)\n",
    "\n",
    "# Load pretrained model/tokenizer\n",
    "model_class, tokenizer_class, pretrained_weights = (ppb.BertModel, ppb.BertTokenizer, 'bert-base-uncased')\n",
    "model = model_class.from_pretrained(pretrained_weights)\n",
    "\n",
    "# Tell pytorch to run this model on the GPU.\n",
    "model.cuda()\n",
    "model.to(device)\n",
    "\n",
    "# Tokenization\n",
    "tokenized = sentences.apply((lambda x: tokenizer.encode(x,add_special_tokens=True)))\n",
    "#Padding\n",
    "padded = np.array([i + [0]*(max_len-len(i)) for i in tokenized.values])\n",
    "# Masking\n",
    "attention_mask = np.where(padded != 0, 1, 0)\n",
    "\n",
    "## Now deep learning !\n",
    "####  DEEP LEARNING\n",
    "input_ids = torch.tensor(padded).to(device)\n",
    "attention_mask = torch.tensor(attention_mask).to(device)\n",
    "labels = torch.tensor(df[\"label\"].values).to(device)\n",
    "\n",
    "dataset = TensorDataset(input_ids, attention_mask, labels)\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "# Create the DataLoaders for our training and validation sets.\n",
    "dataloader = DataLoader(dataset, sampler = SequentialSampler(dataset),batch_size = batch_size)\n",
    "\n",
    "dfLabels = pd.DataFrame()\n",
    "dfFeatures = pd.DataFrame()\n",
    "\n",
    "# For each batch of training data...\n",
    "for batch in dataloader:\n",
    "    with torch.no_grad():\n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "        last_hidden_states = model(b_input_ids,attention_mask = b_input_mask)\n",
    "        features = last_hidden_states[0][:,0,:]#Let's slice only the part of the output that we need. That is the output corresponding the first token of each sentence.  The way BERT does sentence classification, is that it adds a token called [CLS] (for classification) at the beginning of every sentence. The output corresponding to that token can be thought of as an embedding for the entire sentence.\n",
    "        features = features.cpu().detach().numpy()        \n",
    "        labels = b_labels.cpu().detach().numpy()\n",
    "        dfLabels = dfLabels.append(pd.DataFrame(labels),ignore_index = True)\n",
    "        dfFeatures = dfFeatures.append(pd.DataFrame(features),ignore_index = True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Model 2. Train and Test Split \n",
    "# The output from BERT is going to be input to SKLEARN\n",
    "\n",
    "train_features, test_features, train_labels, test_labels = train_test_split(dfFeatures, dfLabels,test_size=0.33, random_state=42)\n",
    "\n",
    "features = features.reset_index(drop=True)\n",
    "\n",
    "combine_df = pd.concat([dfFeatures, features],axis=1, ignore_index=True)\n",
    "\n",
    "train_features, test_features, train_labels, test_labels = train_test_split(combine_df, dfLabels,test_size=0.33, random_state=42)\n",
    "\n",
    "train_features, test_features, train_labels, test_labels = train_test_split(features, labels, test_size=0.33, random_state=42)\n",
    "\n",
    "# Now we are going to train Logistic Regression model\n",
    "\n",
    "lr_clf = LogisticRegression()\n",
    "lr_clf.fit(train_features, train_labels)\n",
    "\n",
    "dt_clf = DecisionTreeClassifier()\n",
    "dt_clf.fit(train_features, train_labels)\n",
    "\n",
    "svm_clf = svm.SVC(gamma=0.001, C=100.)\n",
    "svm_clf.fit(train_features, train_labels)\n",
    "\n",
    "rf_clf = RandomForestClassifier()\n",
    "rf_clf.fit(train_features, train_labels)\n",
    "\n",
    "ab_clf = AdaBoostClassifier(n_estimators=100, random_state=0)\n",
    "ab_clf.fit(train_features, train_labels)\n",
    "\n",
    "nb_clf = GaussianNB()\n",
    "nb_clf.fit(train_features, train_labels)\n",
    "\n",
    "nn_clf = MLPClassifier(random_state=1, max_iter=300)\n",
    "nn_clf.fit(train_features, train_labels)\n",
    "\n",
    "\n",
    "#Evaluating Model #2\n",
    "#So how well does our model do in classifying sentences? One way is to check the accuracy against the testing dataset:\n",
    "lr_clf.score(test_features, test_labels)\n",
    "dt_clf.score(test_features, test_labels)\n",
    "rf_clf.score(test_features, test_labels)\n",
    "ab_clf.score(test_features, test_labels)\n",
    "nb_clf.score(test_features, test_labels)\n",
    "nn_clf.score(test_features, test_labels)\n",
    "svm_clf.score(test_features, test_labels)\n",
    "\n",
    "y_pred = lr_clf.predict(test_features)\n",
    "y_pred = dt_clf.predict(test_features)\n",
    "y_pred = rf_clf.predict(test_features)\n",
    "y_pred = ab_clf.predict(test_features)\n",
    "y_pred = nb_clf.predict(test_features)\n",
    "y_pred = nn_clf.predict(test_features)\n",
    "y_pred = svm_clf.predict(test_features)\n",
    "\n",
    "print('Accuracy:', accuracy_score(test_labels, y_pred))\n",
    "print('F1 score:', f1_score(test_labels, y_pred, average='macro'))\n",
    "print('Recall:', recall_score(test_labels, y_pred, average='macro'))\n",
    "print('Precision:', precision_score(test_labels, y_pred, average='macro'))\n",
    "print('\\n clasification report:\\n', classification_report(test_labels,y_pred))\n",
    "print('\\n confussion matrix:\\n',confusion_matrix(test_labels, y_pred))\n",
    "\n",
    "\n",
    "\n",
    "##### Grid Search for Parameters\n",
    "#We can dive into Logistic regression directly with the Scikit Learn default parameters, but sometimes it's worth searching for the best value of the C parameter, which determines regularization strength.\n",
    "parameters = {'C': np.linspace(0.0001, 100, 20)}\n",
    "grid_search = GridSearchCV(LogisticRegression(), parameters)\n",
    "grid_search.fit(train_features, train_labels)\n",
    "\n",
    "print('best parameters: ', grid_search.best_params_)\n",
    "print('best scrores: ', grid_search.best_score_)\n",
    "\n",
    "target_names = ['class 0', 'class 1']\n",
    "print(classification_report(test_labels, y_pred, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "# Logistic regression get importance\n",
    "importance = lr_clf.coef_[0]\n",
    "# summarize feature importance\n",
    "for i,v in enumerate(importance):\n",
    "    print('Feature: %0d, Score: %.5f' % (i,v))\n",
    "\n",
    "importance = dt_clf.feature_importances_\n",
    "# summarize feature importance\n",
    "for i,v in enumerate(importance):\n",
    "    print('Feature: %0d, Score: %.5f' % (i,v))\n",
    "    \n",
    "# get importance\n",
    "importance = rf_clf.feature_importances_\n",
    "# summarize feature importance\n",
    "for i,v in enumerate(importance):\n",
    "    print('Feature: %0d, Score: %.5f' % (i,v))\n",
    "    \n",
    "    \n",
    "results = permutation_importance(rf_clf, test_features, test_labels, n_repeats=10,random_state=42, n_jobs=2,scoring='accuracy')\n",
    "# get importance\n",
    "importance = results.importances_mean\n",
    "# summarize feature importance\n",
    "for i,v in enumerate(importance):\n",
    "    print('Feature: %0d, Score: %.5f' % (i,v))\n",
    "\n",
    "\n",
    "    ### RF feature importance###\n",
    "importances = rf_clf.feature_importances_\n",
    "std = np.std([tree.feature_importances_ for tree in rf_clf.estimators_],axis=0)\n",
    "indices = np.argsort(importances)[::-1]\n",
    "for f in range(train_features.shape[1]):\n",
    "    print(\"%d. feature %d (%f)\" % (f + 1, indices[f], importances[indices[f]]))\n",
    "# Plot the impurity-based feature importances of the forest\n",
    "plt.figure()\n",
    "plt.title(\"Feature importances\")\n",
    "plt.bar(range(X.shape[1]), importances[indices],color=\"r\", yerr=std[indices], align=\"center\")\n",
    "plt.xticks(range(X.shape[1]), indices)\n",
    "plt.xlim([-1, X.shape[1]])\n",
    "plt.show()\n",
    "\n",
    "\n",
    "result = permutation_importance(rf, X_test, y_test, n_repeats=10,random_state=42, n_jobs=2)\n",
    "sorted_idx = result.importances_mean.argsort()\n",
    "fig, ax = plt.subplots()\n",
    "ax.boxplot(result.importances[sorted_idx].T,vert=False, labels=X_test.columns[sorted_idx])\n",
    "ax.set_title(\"Permutation Importances (test set)\")\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
