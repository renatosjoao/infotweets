{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading words: <urlopen error [SSL:\n",
      "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
      "[nltk_data]     unable to get local issuer certificate (_ssl.c:1056)>\n",
      "[nltk_data] Error loading punkt: <urlopen error [SSL:\n",
      "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
      "[nltk_data]     unable to get local issuer certificate (_ssl.c:1056)>\n"
     ]
    }
   ],
   "source": [
    "import math \n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import json\n",
    "import csv\n",
    "import re\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score, cross_validate, train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import make_moons, make_circles, make_classification\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, confusion_matrix\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "import tensorflow as tf\n",
    "import torch \n",
    "from torch.utils.data import DataLoader, Dataset, TensorDataset, random_split, RandomSampler,IterableDataset\n",
    "from torch.utils.data.sampler import SequentialSampler\n",
    "from transformers import BertTokenizer\n",
    "import transformers as ppb \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string \n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('words')\n",
    "nltk.download('punkt')\n",
    "\n",
    "stopwords.words('english')\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(level=logging.ERROR)\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "#needs to be GLOBAL\n",
    "words = set(nltk.corpus.words.words())\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== covid ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1004 19:54:08.772951 140612314093376 tokenization_utils_base.py:1254] loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /home/renato/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    3816\n",
      "1    3378\n",
      "Name: label, dtype: int64\n",
      "=== crisislext6 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1004 19:54:25.028251 140612314093376 tokenization_utils_base.py:1254] loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /home/renato/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== crisislext26 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1004 20:01:48.028802 140612314093376 tokenization_utils_base.py:1254] loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /home/renato/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== crisismmd ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1004 20:02:37.174732 140612314093376 tokenization_utils_base.py:1254] loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /home/renato/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1    9343\n",
      "0    3443\n",
      "Name: label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "### https://towardsdatascience.com/bert-text-classification-using-pytorch-723dfb8b6b5b\n",
    "    \n",
    "#!/usr/bin/env python3\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import time\n",
    "import csv\n",
    "import json\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import tensorflow as tf\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from transformers import BertModel\n",
    "from torch.utils.data import DataLoader, Dataset, TensorDataset, random_split,SequentialSampler, IterableDataset\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW, BertConfig\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from sklearn.model_selection import KFold, StratifiedKFold, StratifiedShuffleSplit\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def fetch_tweet_features(df):\n",
    "    df[\"sentence\"] = df[\"sentence\"].str.lower()\n",
    "    df['nchars'] = df['sentence'].str.len()\n",
    "    df['nwords'] = df['sentence'].str.split().str.len()\n",
    "    df['bhash'] = df[\"sentence\"].str.contains(pat = '#',flags=re.IGNORECASE, regex = True).astype(int) \n",
    "    df['nhash'] = df[\"sentence\"].str.count('#') \n",
    "    df['blink'] = df[\"sentence\"].str.contains(pat = 'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', flags=re.IGNORECASE, regex = True).astype(int)\n",
    "    df['nlink'] = df[\"sentence\"].str.count(pat = 'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', flags=re.IGNORECASE) \n",
    "    df['bat'] = df[\"sentence\"].str.contains(pat = '@',flags=re.IGNORECASE, regex = True).astype(int)\n",
    "    df['nat'] = df[\"sentence\"].str.count(pat = '@')\n",
    "    df['brt'] =  df[\"sentence\"].str.contains(pat = '@rt|rt@',flags=re.IGNORECASE, regex = True).astype(int) \n",
    "    ## List of  US slangs.\n",
    "    slangList = ['ASAP','BBIAB','BBL','BBS','BF','BFF','BFFL','BRB','CYA','DS','FAQ','FB','FITBLR','FLBP','FML','FTFY','FTW','FYI','G2G','GF','GR8','GTFO','HBIC','HML','HRU','HTH','IDK','IGHT','IMO','IMHO','IMY','IRL','ISTG','JK','JMHO','KTHX','L8R','LMAO','LMFAO','LMK','LOL','MWF','NM','NOOB','NP','NSFW','OOAK','OFC','OMG','ORLY','OTOH','RN','ROFL','RUH','SFW','SOML','SOZ','STFU','TFTI','TIL','TMI','TTFN','TTYL','TWSS','U','W/','WB','W/O','WYD','WTH','WTF','WYM','WYSIWYG','Y','YMMV','YW','YWA']\n",
    "    slangList = [x.lower() for x in slangList]\n",
    "    #Checks if the sentence contains slang\n",
    "    mask = df.loc[:,'sentence'].str.contains(r'\\b(?:{})\\b'.format('|'.join(slangList)))\n",
    "    df['bslang'] = mask.astype(int)\n",
    "    ## List of interjections\n",
    "    interjList = [\"a-ha\",\"aaaahh\",\"aaah\",\"aah\",\"adios\",\"aha\",\"ahem\",\"ahh\",\"ahhh\",\"alas\",\"alasbah\",\"allhail\",\"alleluia\",\"aloha\",\"argh\",\"augh\",\"aw\",\"aww\",\"awww\",\"ay\",\"ba-dum-tss\",\"badum\",\"bah\",\"bahaha\",\"blech\",\"bleep\",\"bleh\",\"blessyou\",\"boo\",\"boo-hoo\",\"booh\",\"boohoo\",\"brr\",\"brrr\",\"brrrr\",\"bwahaha\",\"bye\",\"cheerio\",\"ciao\",\"congrats\",\"crikey\",\"cripes\",\"d'oh\",\"doh\",\"duh\",\"eeeek\",\"eeek\",\"eek\",\"eep\",\"eh\",\"eureka\",\"eww\",\"ewww\",\"eyh\",\"fiddlesticks\",\"fuff\",\"gadzooks\",\"gah\",\"gesundheit\",\"goodgrief\",\"Good-bye\",\"goodbye\",\"great\",\"grr\",\"grrr\",\"grrrr\",\"ha-ha\",\"haha\",\"hahaha\",\"harumph\",\"hbahhambug\",\"hehe\",\"hei\",\"heigh-ho\",\"hey\",\"hm\",\"hmmm\",\"hmmmm\",\"holycow\",\"holysmokes\",\"hooray\",\"hotdiggitydog\",\"huh\",\"huh\",\"humph\",\"hurrah\",\"hurray\",\"hush\",\"hush,shush\",\"huzzah\",\"ich\",\"ich\",\"ick\",\"ick\",\"ick\",\"jeepers\",\"jeeperscreepers\",\"jeez\",\"la-dee-dah\",\"la-di-da\",\"loandbehold\",\"man\",\"meh\",\"mhmm\",\"mm\",\"mmh\",\"mmhm\",\"mmm\",\"muahaha\",\"mwahaha\",\"myword\",\"nuh-uh\",\"oh\",\"oh\",\"oh\",\"ohdear\",\"ohmy\",\"ohwell\",\"oh-lala\",\"ohh\",\"ohhoh\",\"oi\",\"ooh\",\"ooh\",\"ooh-la-la\",\"oooh\",\"oops\",\"ouch\",\"ouch\",\"ow\",\"ow\",\"ow\",\"oww\",\"oy\",\"oyh\",\"pee-yew\",\"pew\",\"pff\",\"pffh\",\"pfft\",\"phew\",\"phooey\",\"pow\",\"presto\",\"pshaw\",\"pss\",\"pssh\",\"psst\",\"right-o\",\"sheesh\",\"shh\",\"shh\",\"shoo\",\"shoo\",\"shoot\",\"solong\",\"thanks\",\"tish\",\"touch√©\",\"tsk-tsk\",\"ugh\",\"ugh\",\"ugh\",\"uh-hu\",\"uh-huh\",\"uh-huh\",\"uh-oh\",\"uh-oh\",\"uh-uh,\",\"uhh\",\"uhh\",\"umm\",\"umm\",\"ummm\",\"unh-uh\",\"unh-unh\",\"voila\",\"voila\",\"waaaaah\",\"waah\",\"wahoo\",\"wee\",\"weee\",\"well\",\"wham\",\"whee\",\"whoa\",\"whoopee\",\"whoops\",\"whoosh\",\"woohoo\",\"wow\",\"yahoo\",\"yak\",\"yay\",\"yea\",\"yeah\",\"yeah\",\"yech\",\"yee-haw\",\"yeeeeaah\",\"yeehaw\",\"yeow\",\"yikes\",\"yippee\",\"yippee\",\"yo\",\"yoo-hoo\",\"yoohoo\",\"youck\",\"youck\",\"yow\",\"yowza\",\"yu-huh\",\"yuck\",\"yuck\",\"yuh-hu\",\"yuh-uh\",\"yummy\",\"zapello\",\"zing\"]\n",
    "    interjList = [x.lower() for x in interjList]\n",
    "    #Checks if the sentence contains interjection\n",
    "    mask = df.loc[:, 'sentence'].str.contains(r'\\b(?:{})\\b'.format('|'.join(interjList)))\n",
    "    df['bintj'] = mask.astype(int)\n",
    "    df['tlex'] = df[\"sentence\"].apply(lexical_diversity)\n",
    "    tweet_features =  df[['tweet_id','sentence','nchars','nwords','bhash','nhash','blink','nlink','bat','nat','brt','bslang','bintj','tlex','label']]\n",
    "    tweet_featuresDF = pd.DataFrame(tweet_features)\n",
    "    return tweet_featuresDF\n",
    "\n",
    "def fetch_usr_features(filename):\n",
    "    usr_featuresDF = pd.DataFrame(columns=['tweet_id','sentence','usr_vrf','num_followers','num_friends','num_tweets'])\n",
    "    listOfSeries = []\n",
    "    with open(filename) as json_file:\n",
    "        for line in json_file:\n",
    "            data = json.loads(line)\n",
    "            tweet_id  = int(data['id_str'])\n",
    "            user_obj = data['user']\n",
    "            sentence =  data['full_text'].lower()\n",
    "            usr_id = user_obj['id_str']\n",
    "            usr_vrf = int(user_obj['verified'])#.astype(int)\n",
    "            num_followers = user_obj['followers_count']\n",
    "            num_followers = math.log10(1 + num_followers)\n",
    "            num_friends = user_obj['friends_count']\n",
    "            num_friends = math.log10(1 + num_friends)\n",
    "            num_tweets = user_obj['statuses_count']\n",
    "            num_tweets = math.log10(1 + num_tweets)\n",
    "            listOfSeries.append(pd.Series([int(tweet_id),sentence, usr_vrf, num_followers, num_friends, num_tweets ], index=usr_featuresDF.columns ))\n",
    "        usr_featuresDF = usr_featuresDF.append(listOfSeries , ignore_index=True)\n",
    "    return usr_featuresDF\n",
    "\n",
    "def lexical_diversity(text):\n",
    "    return len(set(text.split())) / len(text.split())\n",
    "\n",
    "class CustomCombinedDataset(Dataset):\n",
    "    def __init__(self,name):\n",
    "        #self.tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-cased')\n",
    "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "        #self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "        if name == 'crisismmd':\n",
    "            self.df = pd.DataFrame()\n",
    "            files = [\"/home/renato/Downloads/CrisisMMD_v2.0/crisismmd_datasplit_all/task_informative_text_img_train.tsv\",\"/home/renato/Downloads/CrisisMMD_v2.0/crisismmd_datasplit_all/task_informative_text_img_dev.tsv\",\"/home/renato/Downloads/CrisisMMD_v2.0/crisismmd_datasplit_all/task_informative_text_img_test.tsv\"]\n",
    "            #files = [\"/home/joao/task_informative_text_img_train.tsv\",\"/home/joao/task_informative_text_img_dev.tsv\",\"/home/joao/task_informative_text_img_test.tsv\"]\n",
    "            for filename in files:\n",
    "                tmpdf = pd.read_csv(filename,delimiter='\\t',quoting=csv.QUOTE_NONE,error_bad_lines=False,encoding='utf-8')\n",
    "                tmpdf = tmpdf[['tweet_id','tweet_text','label_text']]\n",
    "                tmpdf = tmpdf.rename(columns={'tweet_text': 'sentence'})\n",
    "                tmpdf = tmpdf.rename(columns={'label_text': 'label'})\n",
    "                tmpdf['label'] = tmpdf['label'].replace('informative', 1)\n",
    "                tmpdf['label'] = tmpdf['label'].replace('not_informative', 0)\n",
    "                self.df = self.df.append(tmpdf,ignore_index = True)\n",
    "            self.df = self.df.drop_duplicates(subset=['tweet_id','sentence','label'])\n",
    "            self.df = self.df.reset_index(drop=True)\n",
    "            twt_feat = fetch_tweet_features(self.df)\n",
    "            twt_feat.to_csv(\"./crisismmd.ORG.tsv\", sep=\"\\t\")\n",
    "            usr_features = fetch_usr_features(\"/home/renato/Downloads/CrisisMMD_v2.0/crisismmd_datasplit_all/crisismmd.json\")\n",
    "            #usr_features = fetch_usr_features(\"/home/joao/crisismmd.json\")\n",
    "            series = []\n",
    "            for index, row in usr_features.iterrows():\n",
    "                tweet_id = row['tweet_id']\n",
    "                usr_vrf, num_followers, num_friends, num_tweets = row['usr_vrf'],row['num_followers'],row['num_friends'],row['num_tweets']\n",
    "                idx = twt_feat[twt_feat['tweet_id']==tweet_id].index.values.astype(int)[0]\n",
    "                tweet_features  = twt_feat.loc[idx]\n",
    "                series.append(pd.Series([tweet_features['tweet_id'], tweet_features['sentence'],tweet_features['nchars'],tweet_features['nwords'],tweet_features['bhash'],tweet_features['nhash'],tweet_features['blink'],tweet_features['nlink'],tweet_features['bat'],tweet_features['nat'],tweet_features['brt'],tweet_features['bslang'], tweet_features['bintj'], tweet_features['tlex'], usr_vrf, num_followers, num_friends, num_tweets, tweet_features['label']],index=list(['tweet_id','sentence','nchars','nwords','bhash','nhash','blink','nlink','bat','nat','brt','bslang','bintj','tlex','usr_vrf','num_followers', 'num_friends', 'num_tweets','label'])))\n",
    "            self.df = pd.DataFrame(series)\n",
    "            self.df = self.df.drop_duplicates(subset=['tweet_id','sentence','nchars','nwords','bhash','nhash','blink','nlink','bat','nat','brt','bslang','bintj','tlex','usr_vrf', 'num_followers', 'num_friends', 'num_tweets','label'])\n",
    "            self.df =  self.df.reset_index(drop=True) \n",
    "            print((self.df['label'].value_counts()))\n",
    "            self.df.to_csv(\"./crisismmd.subset.tsv\", sep=\"\\t\")\n",
    "        if name == 'covid':\n",
    "            self.df =  pd.DataFrame()\n",
    "            #\"/home/renato/Datasets/COVID19Tweet-master/train.tsv\",\n",
    "            #files = [\"/home/joao/COVID19Tweet-master/train.tsv\",\"/home/joao/COVID19Tweet-master/valid.tsv\"]\n",
    "            files = [\"/home/renato/Datasets/COVID19Tweet-master/train.tsv\",\"/home/renato/Datasets/COVID19Tweet-master/valid.tsv\"]\n",
    "            for filename in files:\n",
    "                #tmpdf = pd.read_csv(filename,delimiter='\\t',encoding='utf-8') \n",
    "                tmpdf = pd.read_csv(filename,delimiter='\\t',quoting=csv.QUOTE_NONE,error_bad_lines=False,encoding='utf-8',lineterminator=\"\\n\")\n",
    "                tmpdf = tmpdf.rename(columns={'Id': 'tweet_id'})        \n",
    "                tmpdf = tmpdf.rename(columns={'Text': 'sentence'})\n",
    "                tmpdf = tmpdf.rename(columns={'Label': 'label'})\n",
    "                tmpdf['label'] = tmpdf['label'].replace('INFORMATIVE', 1)\n",
    "                tmpdf['label'] = tmpdf['label'].replace('UNINFORMATIVE', 0)\n",
    "                self.df = self.df.append(tmpdf,ignore_index = True)\n",
    "            self.df = self.df.drop_duplicates(subset=['tweet_id','sentence','label'])\n",
    "            self.df = self.df.reset_index(drop=True)\n",
    "            twt_feat  =  fetch_tweet_features(self.df)\n",
    "            twt_feat.to_csv(\"./covid.ORG.tsv\", sep=\"\\t\")\n",
    "            usr_features = fetch_usr_features(\"/home/renato/Datasets/COVID19Tweet-master/covid.json\")\n",
    "            #usr_features = fetch_usr_features(\"/home/joao/covid.json\")\n",
    "            series = []\n",
    "            for index, row in usr_features.iterrows():\n",
    "                tweet_id = int(row['tweet_id'])\n",
    "                usr_vrf, num_followers, num_friends, num_tweets = row['usr_vrf'],row['num_followers'],row['num_friends'],row['num_tweets']\n",
    "                #print(tweet_id)\n",
    "                idx = twt_feat[twt_feat['tweet_id']==tweet_id].index.values.astype(int)[0]\n",
    "                tweet_features  = twt_feat.loc[idx]\n",
    "                series.append(pd.Series([tweet_features['tweet_id'], tweet_features['sentence'],tweet_features['nchars'],tweet_features['nwords'],tweet_features['bhash'],tweet_features['nhash'],tweet_features['blink'],tweet_features['nlink'],tweet_features['bat'],tweet_features['nat'],tweet_features['brt'],tweet_features['bslang'], tweet_features['bintj'], tweet_features['tlex'], usr_vrf, num_followers, num_friends, num_tweets, tweet_features['label']],index=list(['tweet_id','sentence','nchars','nwords','bhash','nhash','blink','nlink','bat','nat','brt','bslang','bintj','tlex','usr_vrf','num_followers', 'num_friends', 'num_tweets','label'])))\n",
    "            self.df = pd.DataFrame(series)\n",
    "            self.df = self.df.drop_duplicates(subset=['tweet_id','sentence','nchars','nwords','bhash','nhash','blink','nlink','bat','nat','brt','bslang','bintj','tlex','usr_vrf', 'num_followers', 'num_friends', 'num_tweets','label'])\n",
    "            self.df =  self.df.reset_index(drop=True) \n",
    "            print((self.df['label'].value_counts()))\n",
    "            self.df.to_csv(\"./covid.subset.tsv\", sep=\"\\t\")\n",
    "        if name == 'crisislext26':\n",
    "            self.df = pd.read_csv(\"/home/renato/Datasets/CrisisLexT26/crisisLexT26.csv\", encoding='utf-8')\n",
    "            #self.df = pd.read_csv(\"/home/joao/crisisLexT26.csv\", encoding='utf-8')\n",
    "            self.df = self.df.drop([' Information Source', ' Information Type' ], axis=1)\n",
    "            self.df = self.df.rename(columns={'Tweet ID': 'tweet_id'})\n",
    "            self.df = self.df.rename(columns={' Tweet Text': 'sentence'})\n",
    "            self.df = self.df.rename(columns={' Informativeness': 'label'})\n",
    "            self.df = self.df.reset_index(drop=True) \n",
    "            twt_feat  = fetch_tweet_features(self.df)\n",
    "            d2 = twt_feat \n",
    "            d2 = d2[d2.label!= 'Not related']\n",
    "            d2 = d2[d2.label!= 'Not applicable']\n",
    "            d2['label'] = d2['label'].replace('Related and informative', 1)\n",
    "            d2['label'] = d2['label'].replace('Related - but not informative', 0)\n",
    "            d2.to_csv(\"./crisislext26.ORG.tsv\", sep=\"\\t\")\n",
    "            #usr_features = fetch_usr_features(\"/home/joao/crisisLexT26.json\")\n",
    "            usr_features = fetch_usr_features(\"/home/renato/Datasets/CrisisLexT26/crisisLexT26.json\")\n",
    "            series = []\n",
    "            for index, row in usr_features.iterrows():\n",
    "                tweet_id = int(row['tweet_id'])\n",
    "                usr_vrf, num_followers, num_friends, num_tweets = row['usr_vrf'],row['num_followers'],row['num_friends'],row['num_tweets']\n",
    "                idx = twt_feat[twt_feat['tweet_id'].astype(int)==int(tweet_id)].index.values.astype(int)[0]\n",
    "                tweet_features  = twt_feat.loc[idx]\n",
    "                series.append(pd.Series([tweet_features['tweet_id'], tweet_features['sentence'],tweet_features['nchars'],tweet_features['nwords'],tweet_features['bhash'],tweet_features['nhash'],tweet_features['blink'],tweet_features['nlink'],tweet_features['bat'],tweet_features['nat'],tweet_features['brt'],tweet_features['bslang'], tweet_features['bintj'], tweet_features['tlex'], usr_vrf, num_followers, num_friends, num_tweets, tweet_features['label']],index=list(['tweet_id','sentence','nchars','nwords','bhash','nhash','blink','nlink','bat','nat','brt','bslang','bintj','tlex','usr_vrf','num_followers', 'num_friends', 'num_tweets','label'])))\n",
    "            self.df = pd.DataFrame(series)\n",
    "            self.df =  self.df.reset_index(drop=True) \n",
    "            self.df = self.df[self.df.label!= 'Not related']\n",
    "            self.df = self.df[self.df.label!= 'Not applicable']\n",
    "            self.df['label'] = self.df['label'].replace('Related and informative', 1)\n",
    "            self.df['label'] = self.df['label'].replace('Related - but not informative', 0)\n",
    "            self.df = self.df.drop_duplicates(subset=['tweet_id','sentence','nchars','nwords','bhash','nhash','blink','nlink','bat','nat','brt','bslang','bintj','tlex','usr_vrf', 'num_followers', 'num_friends', 'num_tweets','label'])\n",
    "            self.df = self.df.reset_index(drop=True)\n",
    "            self.df.to_csv(\"./crisislext26.subset.tsv\", sep=\"\\t\")\n",
    "        if name == 'crisislext6':\n",
    "            self.df = pd.read_csv(\"/home/renato/Datasets/CrisisLexT6-v1.0/CrisisLexT6/crisisLexT6.csv\",encoding='utf-8') \n",
    "            #self.df = pd.read_csv(\"/home/joao/crisisLexT6.csv\", encoding='utf-8')\n",
    "            self.df = self.df.rename(columns={' tweet': 'sentence'})\n",
    "            self.df = self.df.rename(columns={' label': 'label'})\n",
    "            self.df = self.df.rename(columns={'tweet id': 'tweet_id'})\n",
    "            self.df['label'] = self.df['label'].replace('on-topic', 1)\n",
    "            self.df['label'] = self.df['label'].replace('off-topic', 0)\n",
    "            self.df = self.df.drop_duplicates(subset=['tweet_id','sentence','label'])\n",
    "            self.df['tweet_id'] = self.df['tweet_id'].apply(lambda x: x.replace(\"'\", ''))\n",
    "            self.df = self.df.drop_duplicates(subset=['tweet_id','sentence','label'])\n",
    "            self.df = self.df.reset_index(drop=True) \n",
    "            twt_feat  = fetch_tweet_features(self.df)\n",
    "            twt_feat.to_csv(\"./crisislext6.ORG.tsv\", sep=\"\\t\")\n",
    "            usr_features = fetch_usr_features(\"/home/renato/Datasets/CrisisLexT6-v1.0/CrisisLexT6/crisisLexT6.json\")\n",
    "            #usr_features = fetch_usr_features(\"/home/joao/crisisLexT6.json\")\n",
    "            series = []\n",
    "            for index, row in usr_features.iterrows():\n",
    "                tweet_id = int(row['tweet_id'])\n",
    "                usr_vrf, num_followers, num_friends, num_tweets = row['usr_vrf'],row['num_followers'],row['num_friends'],row['num_tweets']\n",
    "                idx = twt_feat[twt_feat['tweet_id'].astype(int)==int(tweet_id)].index.values.astype(int)[0]\n",
    "                tweet_features  = twt_feat.loc[idx]\n",
    "                series.append(pd.Series([tweet_features['tweet_id'], tweet_features['sentence'],tweet_features['nchars'],tweet_features['nwords'],tweet_features['bhash'],tweet_features['nhash'],tweet_features['blink'],tweet_features['nlink'],tweet_features['bat'],tweet_features['nat'],tweet_features['brt'],tweet_features['bslang'], tweet_features['bintj'], tweet_features['tlex'], usr_vrf, num_followers, num_friends, num_tweets, tweet_features['label']],index=list(['tweet_id','sentence','nchars','nwords','bhash','nhash','blink','nlink','bat','nat','brt','bslang','bintj','tlex','usr_vrf','num_followers', 'num_friends', 'num_tweets','label'])))\n",
    "            self.df = pd.DataFrame(series)\n",
    "            self.df = self.df.drop_duplicates(subset=['tweet_id','sentence','nchars','nwords','bhash','nhash','blink','nlink','bat','nat','brt','bslang','bintj','tlex','usr_vrf', 'num_followers', 'num_friends', 'num_tweets','label'])\n",
    "            self.df =  self.df.reset_index(drop=True) \n",
    "            self.df.to_csv(\"./crisislext6.subset.tsv\", sep=\"\\t\")\n",
    "        self.df[\"sentence\"] = self.df[\"sentence\"].str.lower()\n",
    "        #################\n",
    "    def lexical_diversity(self,text):\n",
    "        return len(set(text.split())) / len(text.split())\n",
    " \n",
    "def main():\n",
    "\n",
    "    datasets = ['covid', 'crisislext6', 'crisislext26', 'crisismmd']\n",
    "    for data in datasets :\n",
    "        print(\"=== {} ===\".format(data))\n",
    "        if data == 'crisismmd':\n",
    "            dataset = CustomCombinedDataset(\"crisismmd\")\n",
    "        if data == 'covid':\n",
    "            dataset =  CustomCombinedDataset(\"covid\")\n",
    "        if data ==  'crisislext6':\n",
    "            dataset =  CustomCombinedDataset(\"crisislext6\")\n",
    "            # Create a 90-10 train-validation split.\n",
    "            #train_size = int(0.9 * len(dataset))\n",
    "            #val_size = len(dataset) - train_size\n",
    "            #train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "        if data == 'crisislext26':\n",
    "            dataset =  CustomCombinedDataset(\"crisislext26\")\n",
    "            # Create a 90-10 train-validation split.\n",
    "            #train_size = int(0.9 * len(dataset))\n",
    "            #val_size = len(dataset) - train_size\n",
    "            #train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "main()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
