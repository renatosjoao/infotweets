{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "import math \n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score, cross_validate, train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import make_moons, make_circles, make_classification\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, confusion_matrix\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "import tensorflow as tf\n",
    "import torch \n",
    "from torch.utils.data import DataLoader, Dataset, TensorDataset, random_split, RandomSampler,IterableDataset\n",
    "from torch.utils.data.sampler import SequentialSampler\n",
    "from transformers import BertTokenizer\n",
    "import transformers as ppb \n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self,filename,name):\n",
    "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "        if name == 'crisismmd':\n",
    "            self.df = pd.read_csv(filename,delimiter='\\t',encoding='utf-8')\n",
    "            #df = pd.read_csv('/home/joao/task_informative_text_img_dev.tsv',delimiter='\\t',encoding='utf-8')\n",
    "            #df = pd.read_csv('/home/joao/task_informative_text_img_test.tsv',delimiter='\\t',encoding='utf-8')\n",
    "            self.df = self.df[['tweet_text','label_text']]\n",
    "            self.df = self.df.rename(columns={'tweet_text': 'sentence'})\n",
    "            self.df = self.df.rename(columns={'label_text': 'label'})\n",
    "            self.df['label'] = self.df['label'].replace('informative', 1)\n",
    "            self.df['label'] = self.df['label'].replace('not_informative', 0)\n",
    "        if name == 'covid':\n",
    "            self.df = pd.read_csv(filename,delimiter='\\t',encoding='utf-8')  \n",
    "            #self.train_df = pd.read_csv(\"/home/joao/COVID19Tweet-master/train.tsv\",delimiter='\\t',encoding='utf-8')  \n",
    "            #self.val_df = pd.read_csv(\"/home/joao/COVID19Tweet-master/valid.tsv\",delimiter='\\t',encoding='utf-8')   \n",
    "            self.df = self.df.rename(columns={'Text': 'sentence'})\n",
    "            self.df = self.df.rename(columns={'Label': 'label'})\n",
    "            self.df['label'].replace('INFORMATIVE', 1)\n",
    "            self.df['label'] = self.df['label'].replace('INFORMATIVE', 1)\n",
    "            self.df['label'].replace('UNINFORMATIVE', 0)\n",
    "            self.df['label'] = self.df['label'].replace('UNINFORMATIVE', 0)\n",
    "        if name == 'crisislext26':\n",
    "            self.df = pd.read_csv(\"/home/joao/crisisLexT26.csv\", encoding='utf-8')\n",
    "            self.df = self.df.drop(['Tweet ID', ' Information Source', ' Information Type' ], axis=1)\n",
    "            #Relabelling the columns titles to remove white spaces\n",
    "            self.df = self.df.rename(columns={' Tweet Text': 'sentence'})\n",
    "            self.df = self.df.rename(columns={' Informativeness': 'label'})\n",
    "            self.df = self.df[self.df.label!= 'Not related']\n",
    "            self.df = self.df[self.df.label!= 'Not applicable']\n",
    "            self.df['label'].replace('Related and informative', 1)\n",
    "            self.df['label'] = self.df['label'].replace('Related and informative', 1)\n",
    "            self.df['label'].replace('Related - but not informative', 0)\n",
    "            self.df['label'] = self.df['label'].replace('Related - but not informative', 0)\n",
    "            self.df = self.df.reset_index(drop=True)\n",
    "        if name == 'crisislext6':\n",
    "            #self.df = pd.read_csv(\"/home/renato/Datasets/CrisisLexT6-v1.0/CrisisLexT6/2012_Sandy_Hurricane/2012_Sandy_Hurricane-ontopic_offtopic.csv\", encoding='utf-8')\n",
    "            #self.df = pd.read_csv(\"/home/joao/2012_Sandy_Hurricane-ontopic_offtopic.csv\",encoding='utf-8')\n",
    "            self.df = pd.read_csv(\"/home/joao/crisisLexT6.csv\", encoding='utf-8')\n",
    "            self.df = self.df.rename(columns={' tweet': 'sentence'})\n",
    "            self.df = self.df.rename(columns={' label': 'label'})\n",
    "            self.df['label'].replace('on-topic', 1)\n",
    "            self.df['label'] = self.df['label'].replace('on-topic', 1)\n",
    "            self.df['label'].replace('off-topic', 0)\n",
    "            self.df['label'] = self.df['label'].replace('off-topic', 0)\n",
    "        self.df = self.df[['sentence','label']]\n",
    "        self.df['nchars'] = self.df['sentence'].str.len()\n",
    "        self.df['nwords'] = self.df['sentence'].str.split().str.len()\n",
    "        self.df['bhash'] = self.df[\"sentence\"].str.contains(pat = '#',flags=re.IGNORECASE, regex = True).astype(int) \n",
    "        self.df['nhash'] = self.df[\"sentence\"].str.count('#') \n",
    "        self.df['blink']  = self.df[\"sentence\"].str.contains(pat = 'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', flags=re.IGNORECASE, regex = True) .astype(int)\n",
    "        self.df['nlink'] = self.df[\"sentence\"].str.count(pat = 'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', flags=re.IGNORECASE) \n",
    "        self.df['bat'] = self.df[\"sentence\"].str.contains(pat = '@',flags=re.IGNORECASE, regex = True).astype(int) \n",
    "        self.df['nat'] = self.df[\"sentence\"].str.count(pat = '@') \n",
    "        self.df['rt'] = self.df[\"sentence\"].str.contains(pat = '@rt|rt@',flags=re.IGNORECASE, regex = True).astype(int) \n",
    "        #df['phone'] = df[\"sentence\"].str.contains(pat = '\\(?([0-9]{3})\\)?([ .-]?)([0-9]{3})\\2([0-9]{4})',flags=re.IGNORECASE, regex = True).astype(int) \n",
    "        self.df['dlex'] = self.df[\"sentence\"].apply(self.lexical_diversity)\n",
    "        self.df[\"sentence\"] = self.df[\"sentence\"].str.lower()\n",
    "        ## List of  US slangs.\n",
    "        slangList = ['ASAP','BBIAB','BBL','BBS','BF','BFF','BFFL','BRB','CYA','DS','FAQ','FB','FITBLR','FLBP','FML','FTFY','FTW','FYI','G2G','GF','GR8','GTFO','HBIC','HML','HRU','HTH','IDK','IGHT','IMO','IMHO','IMY','IRL','ISTG','JK','JMHO','KTHX','L8R','LMAO','LMFAO','LMK','LOL','MWF','NM','NOOB','NP','NSFW','OOAK','OFC','OMG','ORLY','OTOH','RN','ROFL','RUH','SFW','SOML','SOZ','STFU','TFTI','TIL','TMI','TTFN','TTYL','TWSS','U','W/','WB','W/O','WYD','WTH','WTF','WYM','WYSIWYG','Y','YMMV','YW','YWA']\n",
    "        slangList = [x.lower() for x in slangList]\n",
    "        #happy emojis\n",
    "        happy_emojis = [':\\)', ';\\)', '\\(:']\n",
    "        #sad emojis\n",
    "        sad_emojis = [':\\(', ';\\(', '\\):']\n",
    "        punctuation = ['.',',','...','?','!',':',';']    \n",
    "        #','-','+','*','_','=','/','','%',' &','{','}','[',']','(',')','\n",
    "        #Checks if the sentence contains slang\n",
    "        mask = self.df.iloc[:, 0].str.contains(r'\\b(?:{})\\b'.format('|'.join(slangList)))\n",
    "        df1 = self.df[~mask]\n",
    "        self.df['slang'] = mask.astype(int) \n",
    "        #Checks if the sentence contains happy emojis\n",
    "        mask = self.df.iloc[:, 0].str.contains(r'\\b(?:{})\\b'.format('|'.join(happy_emojis)), regex = True)\n",
    "        df1 = self.df[~mask]\n",
    "        self.df['hemojis'] = mask.astype(int) \n",
    "        #Checks if the sentence contains happy emojis\n",
    "        mask = self.df.iloc[:, 0].str.contains(r'\\b(?:{})\\b'.format('|'.join(sad_emojis)), regex = True)\n",
    "        df1 = self.df[~mask]\n",
    "        self.df['semojis'] = mask.astype(int) \n",
    "        self.hand_features =  self.df[['nchars', 'nwords','bhash','nhash','blink','nlink','bat','nat','rt','slang','dlex']]\n",
    "        self.hand_features_DF = pd.DataFrame(self.hand_features)\n",
    "        #################\n",
    "        self.df['sentence'] = self.df['sentence'].str.replace(r'http(\\S)+', r'')\n",
    "        self.df['sentence'] = self.df['sentence'].str.replace(r'http(\\S)+', r'')\n",
    "        self.df['sentence'] = self.df['sentence'].str.replace(r'http ...', r'')\n",
    "        self.df['sentence'] = self.df['sentence'].str.replace(r'(RT|rt)[ ]*@[ ]*[\\S]+',r'')\n",
    "        self.df['sentence'] = self.df['sentence'].str.replace(r'@[\\S]+',r'')\n",
    "        self.df['sentence'] = self.df['sentence'].str.replace(r'_[\\S]?',r'')\n",
    "        self.df['sentence'] = self.df['sentence'].str.replace(r'[ ]{2, }',r' ')\n",
    "        self.df['sentence'] = self.df['sentence'].str.replace(r'&amp;?',r'and')\n",
    "        self.df['sentence'] = self.df['sentence'].str.replace(r'&lt;',r'<')\n",
    "        self.df['sentence'] = self.df['sentence'].str.replace(r'&gt;',r'>')\n",
    "        self.df['sentence'] = self.df['sentence'].str.replace(r'([\\w\\d]+)([^\\w\\d ]+)', r'\\1 \\2')\n",
    "        self.df['sentence'] = self.df['sentence'].str.replace(r'([^\\w\\d ]+)([\\w\\d]+)', r'\\1 \\2')\n",
    "        self.df['sentence'] = self.df['sentence'].str.lower()\n",
    "        self.df['sentence'] = self.df['sentence'].str.strip()\n",
    "        self.sentences = self.df['sentence']\n",
    "        self.labels = self.df['label'].values\n",
    "        self.maxlen = 0\n",
    "        if name == 'covid':\n",
    "            self.maxlen = 256\n",
    "        if name == 'crisismmd':\n",
    "            self.maxlen = 256\n",
    "        else:\n",
    "            for sent in self.sentences:\n",
    "                input_ids = self.tokenizer.encode(sent, add_special_tokens=True)\n",
    "                self.maxlen = max(self.maxlen, len(input_ids))\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    def __getitem__(self, idx):\n",
    "        sentence = self.df.loc[idx, 'sentence']\n",
    "        label = self.df.loc[idx, 'label']\n",
    "        h_features = self.hand_features_DF.loc[idx,['nchars', 'nwords','bhash','nhash','blink','nlink','bat','nat','rt','slang','dlex']]\n",
    "        h_features_tensor = torch.tensor(h_features).to(device)\n",
    "        tokens = self.tokenizer.tokenize(sentence)\n",
    "        encoded_dict = self.tokenizer.encode_plus(tokens, add_special_tokens = True, max_length = self.maxlen, pad_to_max_length = True,return_attention_mask = True)\n",
    "        tokens_ids = encoded_dict['input_ids']\n",
    "        tokens_ids_tensor = torch.tensor(tokens_ids).to(device)\n",
    "        attn_mask = encoded_dict['attention_mask']\n",
    "        attn_mask_tensor = torch.tensor(attn_mask).to(device)\n",
    "        label_tensor = torch.tensor(label).to(device)\n",
    "        return tokens_ids_tensor, attn_mask_tensor, label_tensor,h_features_tensor\n",
    "    def lexical_diversity(self,text):\n",
    "        return len(set(text.split())) / len(text.split())\n",
    "\n",
    " \n",
    "\n",
    "def main():\n",
    "    set_seed(42)\n",
    "\n",
    "    datasets = ['covid', 'crisislext6', 'crisislext26', 'crisismmd']\n",
    "    \n",
    "    for data in datasets :\n",
    "        print(\"=== {} ===\".format(data))\n",
    "        if data == 'crisismmd':\n",
    "            train_dataset =  CustomDataset(\"/home/joao/task_informative_text_img_train.tsv\",\"crisismmd\")\n",
    "            val_dataset =  CustomDataset(\"/home/joao/task_informative_text_img_dev.tsv\",\"crisismmd\")\n",
    "            test_dataset = CustomDataset(\"/home/joao/task_informative_text_img_test.tsv\",\"crisismmd\")\n",
    "            datasets = [train_dataset,val_dataset,test_dataset]\n",
    "            dataset = torch.utils.data.ConcatDataset(datasets)\n",
    "        if data == 'covid':\n",
    "            train_dataset =  CustomDataset(\"/home/joao/COVID19Tweet-master/train.tsv\",\"covid\")\n",
    "            val_dataset =  CustomDataset(\"/home/joao/COVID19Tweet-master/valid.tsv\",\"covid\")\n",
    "            datasets = [train_dataset,val_dataset]\n",
    "            dataset = torch.utils.data.ConcatDataset(datasets)\n",
    "        if data ==  'crisislext6':\n",
    "            dataset =  CustomDataset(None,\"crisislext6\")\n",
    "            # Create a 90-10 train-validation split.\n",
    "            #train_size = int(0.9 * len(dataset))\n",
    "            #val_size = len(dataset) - train_size\n",
    "            #train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "        if data == 'crisislext26':\n",
    "            dataset =  CustomDataset(None,\"crisislext26\")\n",
    "            # Create a 90-10 train-validation split.\n",
    "            #train_size = int(0.9 * len(dataset))\n",
    "            #val_size = len(dataset) - train_size\n",
    "            #train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "        batch_size = 64\n",
    "\n",
    "        # Create the DataLoaders for our training and validation sets.\n",
    "        train_dataloader = DataLoader(dataset, sampler = RandomSampler(dataset),batch_size = batch_size)\n",
    "        val_dataloader = DataLoader(val_dataset, sampler = RandomSampler(val_dataset),batch_size = batch_size)\n",
    "\n",
    "        # Load pretrained model/tokenizer\n",
    "        model_class, tokenizer_class, pretrained_weights = (ppb.BertModel, ppb.BertTokenizer, 'bert-base-uncased')\n",
    "        model = model_class.from_pretrained(pretrained_weights)\n",
    "\n",
    "        # Tell pytorch to run this model on the GPU.\n",
    "        model.cuda()\n",
    "        model.to(device)\n",
    "\n",
    "\n",
    "        # For each batch of training data...\n",
    "        train_BertdfLabels = pd.DataFrame()    # dataframe with the Labels Features only \n",
    "        train_BertdfFeatures = pd.DataFrame()  # dataframe with the Bert features only\n",
    "        train_h_dfFeatures = pd.DataFrame()    # dataframe with the hand crafted features only\n",
    "        for batch in train_dataloader:\n",
    "            with torch.no_grad():\n",
    "                b_input_ids, b_input_mask, b_labels, b_h_features = tuple(t.to(device) for t in batch)\n",
    "                last_hidden_states = model(b_input_ids,attention_mask = b_input_mask)\n",
    "                bertfeatures = last_hidden_states[0][:,0,:]#Let's slice only the part of the output that we need. That is the output corresponding the first token of each sentence.  The way BERT does sentence classification, is that it adds a token called [CLS] (for classification) at the beginning of every sentence. The output corresponding to that token can be thought of as an embedding for the entire sentence.\n",
    "                bertfeatures = bertfeatures.cpu().detach().numpy()        \n",
    "                labels = b_labels.cpu().detach().numpy()\n",
    "                h_features = b_h_features.cpu().detach().numpy()\n",
    "                train_BertdfLabels = train_BertdfLabels.append(pd.DataFrame(labels),ignore_index = True)\n",
    "                train_BertdfFeatures = train_BertdfFeatures.append(pd.DataFrame(bertfeatures),ignore_index = True)\n",
    "                train_h_dfFeatures = train_h_dfFeatures.append(pd.DataFrame(h_features),ignore_index = True)\n",
    "      \n",
    "        # For each batch of validation data...\n",
    "        val_BertdfLabels = pd.DataFrame()\n",
    "        val_BertdfFeatures = pd.DataFrame()\n",
    "        val_h_dfFeatures = pd.DataFrame()\n",
    "        for batch in val_dataloader:\n",
    "            with torch.no_grad():\n",
    "                b_input_ids, b_input_mask, b_labels, b_h_features = tuple(t.to(device) for t in batch)\n",
    "                last_hidden_states = model(b_input_ids,attention_mask = b_input_mask)\n",
    "                bertfeatures = last_hidden_states[0][:,0,:]#Let's slice only the part of the output that we need. That is the output corresponding the first token of each sentence.  The way BERT does sentence classification, is that it adds a token called [CLS] (for classification) at the beginning of every sentence. The output corresponding to that token can be thought of as an embedding for the entire sentence.\n",
    "                bertfeatures = bertfeatures.cpu().detach().numpy()        \n",
    "                labels = b_labels.cpu().detach().numpy()\n",
    "                h_features = b_h_features.cpu().detach().numpy()\n",
    "                val_BertdfLabels = val_BertdfLabels.append(pd.DataFrame(labels),ignore_index = True)\n",
    "                val_BertdfFeatures = val_BertdfFeatures.append(pd.DataFrame(bertfeatures),ignore_index = True)\n",
    "                val_h_dfFeatures = val_h_dfFeatures.append(pd.DataFrame(h_features),ignore_index = True)\n",
    "\n",
    "        # Model 2. Train and Test Split \n",
    "        # The output from BERT is going to be input to SKLEARN\n",
    "\n",
    "        ### Hand designed Features  only\n",
    "        #features = features.reset_index(drop=True)\n",
    "        #train_features, test_features, train_labels, test_labels = train_test_split(features, labels, test_size=0.33, random_state=42)\n",
    "        X_train = train_BertdfFeatures\n",
    "        y_train = train_BertdfLabels\n",
    "        #train_h_dfFeatures = pd.DataFrame()  \n",
    "        \n",
    "        lr_clf = LogisticRegression()\n",
    "        dt_clf = DecisionTreeClassifier()\n",
    "        rf_clf = RandomForestClassifier()\n",
    "        ab_clf = AdaBoostClassifier(n_estimators=100, random_state=0)\n",
    "        nb_clf = GaussianNB()\n",
    "        nn_clf = MLPClassifier(random_state=1, max_iter=300)\n",
    "        svm_clf = svm.SVC(gamma=0.001, C=100.)\n",
    "\n",
    "        scoring = ['accuracy','precision_macro', 'recall_macro', 'f1_macro']\n",
    "        \n",
    "        print(\"   Firstly evaluate for BERT  ONLY encoded Features \")\n",
    "        scores_lr_clf = cross_validate( lr_clf, X_train, y_train, cv=10, scoring=scoring, return_train_score=False)\n",
    "        scores_dt_clf = cross_validate( dt_clf, X_train, y_train, cv=10, scoring=scoring, return_train_score=False)\n",
    "        scores_rf_clf = cross_validate( rf_clf, X_train, y_train, cv=10, scoring=scoring, return_train_score=False)\n",
    "        scores_ab_clf = cross_validate( ab_clf, X_train, y_train, cv=10, scoring=scoring, return_train_score=False)\n",
    "        scores_nb_clf = cross_validate( nb_clf, X_train, y_train, cv=10, scoring=scoring, return_train_score=False)\n",
    "        scores_nn_clf = cross_validate( nn_clf, X_train, y_train, cv=10, scoring=scoring, return_train_score=False)\n",
    "        scores_svm_clf = cross_validate( svm_clf, X_train, y_train, cv=10, scoring=scoring, return_train_score=False)\n",
    "          \n",
    "        print(\"++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\")     \n",
    "        print(\"||    Classifiers     |   Accuracy  |  Precision  |  Recall  |  F-score  |\")\n",
    "        print(\"||+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++|\")           \n",
    "        print(\"||Logistic Regression |   {0:.3f}   |   {0:.3f}   |  {0:.3f} |   {0:.3f} |\".format(scores_lr_clf['test_accuracy'].mean()*100.0,scores_lr_clf['test_precision_macro'].mean()*100.0,scores_lr_clf['test_recall_macro'].mean()*100.0,scores_lr_clf['test_f1_macro'].mean()*100.0))\n",
    "        print(\"||Decision Tree       |   {0:.3f}   |   {0:.3f}   |  {0:.3f} |   {0:.3f} |\".format(scores_dt_clf['test_accuracy'].mean()*100.0,scores_dt_clf['test_precision_macro'].mean()*100.0,scores_dt_clf['test_recall_macro'].mean()*100.0,scores_dt_clf['test_f1_macro'].mean()*100.0))\n",
    "        print(\"||Random Forest       |   {0:.3f}   |   {0:.3f}   |  {0:.3f} |   {0:.3f} |\".format(scores_rf_clf['test_accuracy'].mean()*100.0,scores_rf_clf['test_precision_macro'].mean()*100.0,scores_rf_clf['test_recall_macro'].mean()*100.0,scores_rf_clf['test_f1_macro'].mean()*100.0))\n",
    "        print(\"||Adaboost            |   {0:.3f}   |   {0:.3f}   |  {0:.3f} |   {0:.3f} |\".format(scores_ab_clf['test_accuracy'].mean()*100.0,scores_ab_clf['test_precision_macro'].mean()*100.0,scores_ab_clf['test_recall_macro'].mean()*100.0,scores_ab_clf['test_f1_macro'].mean()*100.0))\n",
    "        print(\"||NaiveBayes          |   {0:.3f}   |   {0:.3f}   |  {0:.3f} |   {0:.3f} |\".format(scores_nb_clf['test_accuracy'].mean()*100.0,scores_nb_clf['test_precision_macro'].mean()*100.0,scores_nb_clf['test_recall_macro'].mean()*100.0,scores_nb_clf['test_f1_macro'].mean()*100.0))\n",
    "        print(\"||MLP                 |   {0:.3f}   |   {0:.3f}   |  {0:.3f} |   {0:.3f} |\".format(scores_nn_clf['test_accuracy'].mean()*100.0,scores_nn_clf['test_precision_macro'].mean()*100.0,scores_nn_clf['test_recall_macro'].mean()*100.0,scores_nn_clf['test_f1_macro'].mean()*100.0))\n",
    "        print(\"||SVM                 |   {0:.3f}   |   {0:.3f}   |  {0:.3f} |   {0:.3f} |\".format(scores_svm_clf['test_accuracy'].mean()*100.0,scores_svm_clf['test_precision_macro'].mean()*100.0,scores_svm_clf['test_recall_macro'].mean()*100.0,scores_svm_clf['test_f1_macro'].mean()*100.0))\n",
    "        print(\"++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\")     \n",
    "        print()\n",
    "\n",
    "        X_train =  train_h_dfFeatures\n",
    "        lr_clf = LogisticRegression()\n",
    "        dt_clf = DecisionTreeClassifier()\n",
    "        rf_clf = RandomForestClassifier()\n",
    "        ab_clf = AdaBoostClassifier(n_estimators=100, random_state=0)\n",
    "        nb_clf = GaussianNB()\n",
    "        nn_clf = MLPClassifier(random_state=1, max_iter=300)\n",
    "        svm_clf = svm.SVC(gamma=0.001, C=100.)\n",
    "    \n",
    "        print(\" Secondly evaluate for HAND CRAFTED  ONLY encoded Features   \") \n",
    "        scores_lr_clf = cross_validate( lr_clf, X_train, y_train, cv=10, scoring=scoring, return_train_score=False)\n",
    "        scores_dt_clf = cross_validate( dt_clf, X_train, y_train, cv=10, scoring=scoring, return_train_score=False)\n",
    "        scores_rf_clf = cross_validate( rf_clf, X_train, y_train, cv=10, scoring=scoring, return_train_score=False)\n",
    "        scores_ab_clf = cross_validate( ab_clf, X_train, y_train, cv=10, scoring=scoring, return_train_score=False)\n",
    "        scores_nb_clf = cross_validate( nb_clf, X_train, y_train, cv=10, scoring=scoring, return_train_score=False)\n",
    "        scores_nn_clf = cross_validate( nn_clf, X_train, y_train, cv=10, scoring=scoring, return_train_score=False)\n",
    "        scores_svm_clf = cross_validate( svm_clf, X_train, y_train, cv=10, scoring=scoring, return_train_score=False)\n",
    "   \n",
    "        print(\"++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\")     \n",
    "        print(\"||    Classifiers     |   Accuracy  |  Precision  |  Recall  |  F-score  |\")\n",
    "        print(\"||+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++|\")           \n",
    "        print(\"||Logistic Regression |   {0:.3f}   |   {0:.3f}   |  {0:.3f} |   {0:.3f} |\".format(scores_lr_clf['test_accuracy'].mean()*100.0,scores_lr_clf['test_precision_macro'].mean()*100.0,scores_lr_clf['test_recall_macro'].mean()*100.0,scores_lr_clf['test_f1_macro'].mean()*100.0))\n",
    "        print(\"||Decision Tree       |   {0:.3f}   |   {0:.3f}   |  {0:.3f} |   {0:.3f} |\".format(scores_dt_clf['test_accuracy'].mean()*100.0,scores_dt_clf['test_precision_macro'].mean()*100.0,scores_dt_clf['test_recall_macro'].mean()*100.0,scores_dt_clf['test_f1_macro'].mean()*100.0))\n",
    "        print(\"||Random Forest       |   {0:.3f}   |   {0:.3f}   |  {0:.3f} |   {0:.3f} |\".format(scores_rf_clf['test_accuracy'].mean()*100.0,scores_rf_clf['test_precision_macro'].mean()*100.0,scores_rf_clf['test_recall_macro'].mean()*100.0,scores_rf_clf['test_f1_macro'].mean()*100.0))\n",
    "        print(\"||Adaboost            |   {0:.3f}   |   {0:.3f}   |  {0:.3f} |   {0:.3f} |\".format(scores_ab_clf['test_accuracy'].mean()*100.0,scores_ab_clf['test_precision_macro'].mean()*100.0,scores_ab_clf['test_recall_macro'].mean()*100.0,scores_ab_clf['test_f1_macro'].mean()*100.0))\n",
    "        print(\"||NaiveBayes          |   {0:.3f}   |   {0:.3f}   |  {0:.3f} |   {0:.3f} |\".format(scores_nb_clf['test_accuracy'].mean()*100.0,scores_nb_clf['test_precision_macro'].mean()*100.0,scores_nb_clf['test_recall_macro'].mean()*100.0,scores_nb_clf['test_f1_macro'].mean()*100.0))\n",
    "        print(\"||MLP                 |   {0:.3f}   |   {0:.3f}   |  {0:.3f} |   {0:.3f} |\".format(scores_nn_clf['test_accuracy'].mean()*100.0,scores_nn_clf['test_precision_macro'].mean()*100.0,scores_nn_clf['test_recall_macro'].mean()*100.0,scores_nn_clf['test_f1_macro'].mean()*100.0))\n",
    "        print(\"||SVM                 |   {0:.3f}   |   {0:.3f}   |  {0:.3f} |   {0:.3f} |\".format(scores_svm_clf['test_accuracy'].mean()*100.0,scores_svm_clf['test_precision_macro'].mean()*100.0,scores_svm_clf['test_recall_macro'].mean()*100.0,scores_svm_clf['test_f1_macro'].mean()*100.0))\n",
    "        print(\"++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\")     \n",
    "        print()\n",
    "              \n",
    "           \n",
    "        X_train = pd.concat([train_BertdfFeatures, train_h_dfFeatures],axis=1, ignore_index=True)\n",
    "              \n",
    "        lr_clf = LogisticRegression()\n",
    "        dt_clf = DecisionTreeClassifier()\n",
    "        rf_clf = RandomForestClassifier()\n",
    "        ab_clf = AdaBoostClassifier(n_estimators=100, random_state=0)\n",
    "        nb_clf = GaussianNB()\n",
    "        nn_clf = MLPClassifier(random_state=1, max_iter=300)\n",
    "        svm_clf = svm.SVC(gamma=0.001, C=100.)\n",
    "    \n",
    "        print(\" Thirdly evaluate for BERT + HAND CRAFTED   encoded Features \") \n",
    "        scores_lr_clf = cross_validate( lr_clf, X_train, y_train, cv=10, scoring=scoring, return_train_score=False)\n",
    "        scores_dt_clf = cross_validate( dt_clf, X_train, y_train, cv=10, scoring=scoring, return_train_score=False)\n",
    "        scores_svm_clf = cross_validate( svm_clf, X_train, y_train, cv=10, scoring=scoring, return_train_score=False)\n",
    "        scores_rf_clf = cross_validate( rf_clf, X_train, y_train, cv=10, scoring=scoring, return_train_score=False)\n",
    "        scores_ab_clf = cross_validate( ab_clf, X_train, y_train, cv=10, scoring=scoring, return_train_score=False)\n",
    "        scores_nb_clf = cross_validate( nb_clf, X_train, y_train, cv=10, scoring=scoring, return_train_score=False)\n",
    "        scores_nn_clf = cross_validate( nn_clf, X_train, y_train, cv=10, scoring=scoring, return_train_score=False)\n",
    "       \n",
    "        print(\"++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\")     \n",
    "        print(\"||    Classifiers     |   Accuracy  |  Precision  |  Recall  |  F-score  |\")\n",
    "        print(\"||+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++|\")           \n",
    "        print(\"||Logistic Regression |   {0:.3f}   |   {0:.3f}   |  {0:.3f} |   {0:.3f} |\".format(scores_lr_clf['test_accuracy'].mean()*100.0,scores_lr_clf['test_precision_macro'].mean()*100.0,scores_lr_clf['test_recall_macro'].mean()*100.0,scores_lr_clf['test_f1_macro'].mean()*100.0))\n",
    "        print(\"||Decision Tree       |   {0:.3f}   |   {0:.3f}   |  {0:.3f} |   {0:.3f} |\".format(scores_dt_clf['test_accuracy'].mean()*100.0,scores_dt_clf['test_precision_macro'].mean()*100.0,scores_dt_clf['test_recall_macro'].mean()*100.0,scores_dt_clf['test_f1_macro'].mean()*100.0))\n",
    "        print(\"||Random Forest       |   {0:.3f}   |   {0:.3f}   |  {0:.3f} |   {0:.3f} |\".format(scores_rf_clf['test_accuracy'].mean()*100.0,scores_rf_clf['test_precision_macro'].mean()*100.0,scores_rf_clf['test_recall_macro'].mean()*100.0,scores_rf_clf['test_f1_macro'].mean()*100.0))\n",
    "        print(\"||Adaboost            |   {0:.3f}   |   {0:.3f}   |  {0:.3f} |   {0:.3f} |\".format(scores_ab_clf['test_accuracy'].mean()*100.0,scores_ab_clf['test_precision_macro'].mean()*100.0,scores_ab_clf['test_recall_macro'].mean()*100.0,scores_ab_clf['test_f1_macro'].mean()*100.0))\n",
    "        print(\"||NaiveBayes          |   {0:.3f}   |   {0:.3f}   |  {0:.3f} |   {0:.3f} |\".format(scores_nb_clf['test_accuracy'].mean()*100.0,scores_nb_clf['test_precision_macro'].mean()*100.0,scores_nb_clf['test_recall_macro'].mean()*100.0,scores_nb_clf['test_f1_macro'].mean()*100.0))\n",
    "        print(\"||MLP                 |   {0:.3f}   |   {0:.3f}   |  {0:.3f} |   {0:.3f} |\".format(scores_nn_clf['test_accuracy'].mean()*100.0,scores_nn_clf['test_precision_macro'].mean()*100.0,scores_nn_clf['test_recall_macro'].mean()*100.0,scores_nn_clf['test_f1_macro'].mean()*100.0))\n",
    "        print(\"||SVM                 |   {0:.3f}   |   {0:.3f}   |  {0:.3f} |   {0:.3f} |\".format(scores_svm_clf['test_accuracy'].mean()*100.0,scores_svm_clf['test_precision_macro'].mean()*100.0,scores_svm_clf['test_recall_macro'].mean()*100.0,scores_svm_clf['test_f1_macro'].mean()*100.0))\n",
    "        print(\"++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\")     \n",
    "        print(\"\")\n",
    "    \n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "### Bert features only ( BertModel [CLS] token )\n",
    "train_features, test_features, train_labels, test_labels = train_test_split(dfFeatures, dfLabels,test_size=0.33, random_state=42)\n",
    "\n",
    "\n",
    "### Combined featues  BertModel [CLS] token  + Hand designed Features\n",
    "combine_df = pd.concat([dfFeatures, features],axis=1, ignore_index=True)\n",
    "train_features, test_features, train_labels, test_labels = train_test_split(combine_df, dfLabels,test_size=0.33, random_state=42)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Evaluating Model #2\n",
    "#So how well does our model do in classifying sentences? One way is to check the accuracy against the testing dataset:\n",
    "\n",
    "print(\"++++++++++++++ SCORES +++++++++++++++++++++++++++++++++++++++++++++++++++\"     \n",
    "print(\"|| {0:.3f} | {0:.3f} | {0:.3f} | {0:.3f} | {0:.3f} | {0:.3f} | {0:.3f} ||\".format(lr_clf.score(test_features, test_labels),dt_clf.score(test_features, test_labels),rf_clf.score(test_features, test_labels),ab_clf.score(test_features, test_labels),nb_clf.score(test_features, test_labels),nn_clf.score(test_features, test_labels),svm_clf.score(test_features, test_labels))\n",
    "print(\"+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\"     \n",
    "\n",
    "y_pred_lr = lr_clf.predict(test_features)\n",
    "y_pred_dt = dt_clf.predict(test_features)\n",
    "y_pred_rf = rf_clf.predict(test_features)\n",
    "y_pred_ab = ab_clf.predict(test_features)\n",
    "y_pred_nb = nb_clf.predict(test_features)\n",
    "y_pred_nn = nn_clf.predict(test_features)\n",
    "y_pred_svm = svm_clf.predict(test_features)\n",
    "\n",
    "      \n",
    "print(\"++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\")     \n",
    "print(\"||    Classifiers     |   Accuracy  |  Precision  |  Recall  |  F-score  |\")\n",
    "print(\"||+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++|\")           \n",
    "print(\"||Logistic Regression |   {0:.3f}   |   {0:.3f}   |  {0:.3f} |   {0:.3f} |\".format(accuracy_score(test_labels, y_pred_lr),precision_score(test_labels, y_pred_lr, average='macro'),recall_score(test_labels, y_pred_lr, average='macro'),f1_score(test_labels, y_pred_lr, average='macro')))\n",
    "print(\"||Decision Tree       |   {0:.3f}   |   {0:.3f}   |  {0:.3f} |   {0:.3f} |\".format(accuracy_score(test_labels, y_pred_dt),precision_score(test_labels, y_pred_dt, average='macro'),recall_score(test_labels, y_pred_dt, average='macro'),f1_score(test_labels, y_pred_dt, average='macro')))\n",
    "print(\"||Random Forest       |   {0:.3f}   |   {0:.3f}   |  {0:.3f} |   {0:.3f} |\".format(accuracy_score(test_labels, y_pred_rf),precision_score(test_labels, y_pred_rf, average='macro'),recall_score(test_labels, y_pred_rf, average='macro'),f1_score(test_labels, y_pred_rf, average='macro')))\n",
    "print(\"||Adaboost            |   {0:.3f}   |   {0:.3f}   |  {0:.3f} |   {0:.3f} |\".format(accuracy_score(test_labels, y_pred_ab),precision_score(test_labels, y_pred_ab, average='macro'),recall_score(test_labels, y_pred_ab, average='macro'),f1_score(test_labels, y_pred_ab, average='macro')))\n",
    "print(\"||NaiveBayes          |   {0:.3f}   |   {0:.3f}   |  {0:.3f} |   {0:.3f} |\".format(accuracy_score(test_labels, y_pred_nb),precision_score(test_labels, y_pred_nb, average='macro'),recall_score(test_labels, y_pred_nb, average='macro'),f1_score(test_labels, y_pred_nb, average='macro')))\n",
    "print(\"||MLP                 |   {0:.3f}   |   {0:.3f}   |  {0:.3f} |   {0:.3f} |\".format(accuracy_score(test_labels, y_pred_nn),precision_score(test_labels, y_pred_nn, average='macro'),recall_score(test_labels, y_pred_nn, average='macro'),f1_score(test_labels, y_pred_nn, average='macro')))\n",
    "print(\"||SVM                 |   {0:.3f}   |   {0:.3f}   |  {0:.3f} |   {0:.3f} |\".format(accuracy_score(test_labels, y_pred_svm),precision_score(test_labels, y_pred_svm, average='macro'),recall_score(test_labels, y_pred_svm, average='macro'),f1_score(test_labels, y_pred_svm, average='macro'))) \n",
    "print(\"++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\"     \n",
    "      \n",
    "   \n",
    "#print('\\n clasification report:\\n', classification_report(test_labels,y_pred))\n",
    "#print('\\n confussion matrix:\\n',confusion_matrix(test_labels, y_pred))\n",
    "\n",
    "\n",
    "\n",
    "##### Grid Search for Parameters\n",
    "#We can dive into Logistic regression directly with the Scikit Learn default parameters, but sometimes it's worth searching for the best value of the C parameter, which determines regularization strength.\n",
    "#parameters = {'penalty': ['l1', 'l2'],'C':[0.001,0.009,0.01,0.09,1,3,5,10,25,50,100,200]}\n",
    "\n",
    "#grid_search = GridSearchCV(LogisticRegression(),param_grid = parameters,scoring = )\n",
    "#grid_search.fit(train_features, train_labels)\n",
    "#print('best parameters: ', grid_search.best_params_)\n",
    "#print('best scrores: ', grid_search.best_score_)\n",
    "#grid_clf_acc = GridSearchCV(clf, param_grid = grid_values,scoring = 'recall')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#target_names = ['class 0', 'class 1']\n",
    "#print(classification_report(test_labels, y_pred, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import svm\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "digits = datasets.load_digits()\n",
    "n_samples = len(digits.images)\n",
    "X = digits.images.reshape((n_samples, -1))\n",
    "y = digits.target\n",
    "\n",
    "# Split the dataset in two equal parts\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n",
    "\n",
    "# Set the parameters by cross-validation\n",
    "tuned_parameters = [{'kernel': ['rbf'], 'gamma': [0.0009,0.001, 0.002,  0.004, 0.008, 0.01,0.09, 1, 10, 100, 1000]},\n",
    "                    {'kernel': ['linear'], 'C': [0.0009,0.001, 0.002,  0.004, 0.008, 0.01,0.09, 1, 10, 100, 1000]}]\n",
    "scores = ['precision', 'recall', 'f1']\n",
    "\n",
    "\n",
    "lr_clf = LogisticRegression()\n",
    "\n",
    "for score in scores:\n",
    "    print(\"# Tuning hyper-parameters for %s\" % score)\n",
    "    clf = GridSearchCV(SVC(),tuned_parameters, scoring='%s_macro' % score)\n",
    "    clf.fit(X_train, y_train)\n",
    "    print(\"Best parameters set found on development set:\")\n",
    "    print(clf.best_params_)\n",
    "    #print(\"Grid scores on development set:\")\n",
    "    #means = clf.cv_results_['mean_test_score']\n",
    "    #stds = clf.cv_results_['std_test_score']\n",
    "    #for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n",
    "    #    print(\"%0.3f (+/-%0.03f) for %r\" % (mean, std * 2, params))\n",
    "    #print()\n",
    "    #print(\"Detailed classification report:\")\n",
    "    #print()\n",
    "    #print(\"The model is trained on the full development set.\")\n",
    "    #print(\"The scores are computed on the full evaluation set.\")\n",
    "    #print()\n",
    "    #y_true, y_pred = y_test, clf.predict(X_test)\n",
    "    #print(classification_report(y_true, y_pred))\n",
    "    \n",
    "#print()\n",
    "\n",
    "#tuned_parameters = {'penalty': ['l1', 'l2'],'C':[0.0009,0.001, 0.002,  0.004, 0.008, 0.01,0.09, 1, 10, 100, 1000]}\n",
    "#for score in scores:\n",
    "#    print(\"# Tuning hyper-parameters for %s\" % score)\n",
    "#    clf  = GridSearchCV(LogisticRegression(), tuned_parameters, scoring='%s_macro' % score)\n",
    "#    clf.fit(X_train, y_train)\n",
    "#    print(\"Best parameters set found on development set:\")\n",
    "#    print(clf.best_params_)\n",
    "    #grid_values = {'penalty': ['l1', 'l2'],'C':[0.001,.009,0.01,.09,1,5,10,25]}\n",
    "    #grid_clf_acc = GridSearchCV(clf, param_grid = grid_values,scoring = 'recall')\n",
    "    #grid_clf_acc.fit(X_train, y_train)\n",
    "\n",
    "y_pred=clf.predict(X_test)\n",
    "\n",
    "from sklearn import metrics\n",
    "\n",
    "cnf_matrix = metrics.confusion_matrix(y_test, y_pred)\n",
    "cnf_matrix\n",
    "\n",
    "class_names=[0,1] # name  of classes\n",
    "fig, ax = plt.subplots()\n",
    "tick_marks = np.arange(len(class_names))\n",
    "plt.xticks(tick_marks, class_names)\n",
    "plt.yticks(tick_marks, class_names)\n",
    "# create heatmap\n",
    "sns.heatmap(pd.DataFrame(cnf_matrix), annot=True, cmap=\"YlGnBu\" ,fmt='g')\n",
    "ax.xaxis.set_label_position(\"top\")\n",
    "plt.tight_layout()\n",
    "plt.title('Confusion matrix', y=1.1)\n",
    "plt.ylabel('Actual label')\n",
    "plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n",
    "\n",
    "from sklearn.model_selection import cross_val_score, train_test_split, GridSearchCV\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "lr_clf = LogisticRegression()\n",
    "lr_clf.fit(X_train, y_train)\n",
    "\n",
    "scoring = ['accuracy','precision_macro', 'recall_macro', 'f1_macro']\n",
    "\n",
    "scores = cross_validate( lr_clf, X_train, y_train, cv=10, scoring=scoring, return_train_score=False)\n",
    "#scores['train_precision_macro'].mean()\n",
    "\n",
    "scores['test_accuracy'].mean()*100.0\n",
    "scores['test_precision_macro'].mean()*100.0\n",
    "scores['test_recall_macro'].mean()*100.0\n",
    "scores['test_f1_macro'].mean()*100.0\n",
    "\n",
    "print()\n",
    "print(scores)\n",
    "print(\": %0.2f (+/- %0.2f)\" % (scores['test_accuracy'].mean()*100.0, scores['test_accuracy'].std() * 2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_pred_proba = clf.predict_proba(X_test)[::,1]\n",
    "#fpr, tpr, _ = metrics.roc_curve(y_test,  y_pred_proba)\n",
    "#auc = metrics.roc_auc_score(y_test, y_pred_proba)\n",
    "#plt.plot(fpr,tpr,label=\"data 1, auc=\"+str(auc))\n",
    "#plt.legend(loc=4)\n",
    "#plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
