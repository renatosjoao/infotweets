{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0805 02:54:42.931121 140348828829504 file_utils.py:39] PyTorch version 1.5.0 available.\n",
      "I0805 02:54:45.441259 140348828829504 tokenization_utils_base.py:1254] loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /home/renato/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "File b'/home/joao/COVID19Tweet-master/train.tsv' does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-1c6dd6c9e0e2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    272\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 274\u001b[0;31m \u001b[0mtrain_dataset\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mCustomDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/home/joao/COVID19Tweet-master/train.tsv\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"covid\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    275\u001b[0m \u001b[0mval_dataset\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mCustomDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/home/joao/COVID19Tweet-master/valid.tsv\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"covid\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-1c6dd6c9e0e2>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, filename, name)\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBertTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'bert-base-uncased'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdo_lower_case\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'covid'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdelimiter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'\\t'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m             \u001b[0;31m#self.train_df = pd.read_csv(\"/home/joao/COVID19Tweet-master/train.tsv\",delimiter='\\t',encoding='utf-8')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m             \u001b[0;31m#self.val_df = pd.read_csv(\"/home/joao/COVID19Tweet-master/valid.tsv\",delimiter='\\t',encoding='utf-8')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, doublequote, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    676\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 678\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    679\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    680\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    438\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    439\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 440\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    785\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    786\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 787\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    788\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    789\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1012\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1013\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'c'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1014\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1015\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1016\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'python'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1706\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'usecols'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1707\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1708\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1709\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1710\u001b[0m         \u001b[0mpassed_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnames\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: File b'/home/joao/COVID19Tweet-master/train.tsv' does not exist"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import time\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import tensorflow as tf\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset, TensorDataset, random_split, RandomSampler, SequentialSampler, IterableDataset\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW, BertConfig\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import KFold, StratifiedKFold, StratifiedShuffleSplit\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "torch.cuda.set_device(2)\n",
    "torch.cuda.current_device()\n",
    "\n",
    "\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
    "\n",
    "def format_time(elapsed):\n",
    "    # Round to the nearest second.\n",
    "    elapsed_rounded = int(round((elapsed)))    \n",
    "    # Format as hh:mm:ss\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='binary')\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall\n",
    "    }\n",
    "\n",
    "def report_average(report_list):\n",
    "    r_list = list()\n",
    "    for report in report_list:\n",
    "        splited = [' '.join(x.split()) for x in report.split('\\n\\n')]\n",
    "        header = [x for x in splited[0].split(' ')]\n",
    "        data = np.array(splited[1].split(' ')).reshape(-1, len(header) + 1)\n",
    "        data = np.delete(data, 0, 1).astype(float)\n",
    "        df = pd.DataFrame(data, columns=header)\n",
    "        r_list.append(df)\n",
    "    tmp = pd.DataFrame()\n",
    "    for df in r_list:\n",
    "        tmp = tmp.add(df, fill_value=0)           \n",
    "    report_ave =  tmp/len(r_list)\n",
    "    return(report_ave)\n",
    "\n",
    "def flat_fscore(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return f1_score(labels_flat, pred_flat, average='macro')\n",
    "\n",
    "#In [1]: import torch\n",
    "#In [2]: import torch.nn.functional as F \n",
    "#In [3]: probs =  F.softmax(logits_tensor)\n",
    "#probs = probs.detach().cpu().numpy()\n",
    "#predictions  = np.argmax(probs, axis=1).flatten()\n",
    "\n",
    "###matthews_corrcoef(labels, preds)\n",
    "\n",
    "# Add a new row at index k with values provided in list\n",
    "### dfObj.loc['k'] = ['Smriti', 26, 'Bangalore', 'India']\n",
    "### dfObj.iloc[2] = ['Smriti', 26, 'Bangalore', 'India']\n",
    "# get length of all the messages in the train set\n",
    "seq_len = [len(i.split()) for i in train_text]\n",
    "pd.Series(seq_len).hist(bins = 30)\n",
    "\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self,filename,name):\n",
    "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "        if name == 'covid':\n",
    "            self.df = pd.read_csv(filename,delimiter='\\t',encoding='utf-8')  \n",
    "            #self.train_df = pd.read_csv(\"/home/joao/COVID19Tweet-master/train.tsv\",delimiter='\\t',encoding='utf-8')  \n",
    "            #self.val_df = pd.read_csv(\"/home/joao/COVID19Tweet-master/valid.tsv\",delimiter='\\t',encoding='utf-8')   \n",
    "            self.df = self.df.rename(columns={'Text': 'sentence'})\n",
    "            self.df = self.df.rename(columns={'Label': 'label'})\n",
    "            self.df['label'].replace('INFORMATIVE', 1)\n",
    "            self.df['label'] = self.df['label'].replace('INFORMATIVE', 1)\n",
    "            self.df['label'].replace('UNINFORMATIVE', 0)\n",
    "            self.df['label'] = self.df['label'].replace('UNINFORMATIVE', 0)\n",
    "        if name == 'crisislext26':\n",
    "            self.df = pd.read_csv(\"/home/joao/crisisLexT26.csv\", encoding='utf-8')\n",
    "            self.df = self.df.drop(['Tweet ID', ' Information Source', ' Information Type' ], axis=1)\n",
    "            #Relabelling the columns titles to remove white spaces\n",
    "            self.df = self.df.rename(columns={' Tweet Text': 'sentence'})\n",
    "            self.df = self.df.rename(columns={' Informativeness': 'label'})\n",
    "            self.df = self.df[self.df.label!= 'Not related']\n",
    "            self.df = self.df[self.df.label!= 'Not applicable']\n",
    "            self.df['label'].replace('Related and informative', 1)\n",
    "            self.df['label'] = self.df['label'].replace('Related and informative', 1)\n",
    "            self.df['label'].replace('Related - but not informative', 0)\n",
    "            self.df['label'] = self.df['label'].replace('Related - but not informative', 0)\n",
    "            self.df = self.df.reset_index(drop=True)\n",
    "        if name == 'crisislext6':\n",
    "            #self.df = pd.read_csv(\"/home/renato/Datasets/CrisisLexT6-v1.0/CrisisLexT6/2012_Sandy_Hurricane/2012_Sandy_Hurricane-ontopic_offtopic.csv\", encoding='utf-8')\n",
    "            #self.df = pd.read_csv(\"/home/joao/2012_Sandy_Hurricane-ontopic_offtopic.csv\",encoding='utf-8')\n",
    "            self.df = pd.read_csv(\"/home/joao/crisisLexT6.csv\", encoding='utf-8')\n",
    "            self.df = self.df.rename(columns={' tweet': 'sentence'})\n",
    "            self.df = self.df.rename(columns={' label': 'label'})\n",
    "            self.df['label'].replace('on-topic', 1)\n",
    "            self.df['label'] = self.df['label'].replace('on-topic', 1)\n",
    "            self.df['label'].replace('off-topic', 0)\n",
    "            self.df['label'] = self.df['label'].replace('off-topic', 0)\n",
    "        self.df = self.df[['sentence','label']]\n",
    "        self.df['nchars'] = self.df['sentence'].str.len()\n",
    "        self.df['nwords'] = self.df['sentence'].str.split().str.len()\n",
    "        self.df['bhash'] = self.df[\"sentence\"].str.contains(pat = '#',flags=re.IGNORECASE, regex = True).astype(int) \n",
    "        self.df['nhash'] = self.df[\"sentence\"].str.count('#') \n",
    "        self.df['blink']  = self.df[\"sentence\"].str.contains(pat = 'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', flags=re.IGNORECASE, regex = True) .astype(int)\n",
    "        self.df['nlink'] = self.df[\"sentence\"].str.count(pat = 'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', flags=re.IGNORECASE) \n",
    "        self.df['bat'] = self.df[\"sentence\"].str.contains(pat = '@',flags=re.IGNORECASE, regex = True).astype(int) \n",
    "        self.df['nat'] = self.df[\"sentence\"].str.count(pat = '@') \n",
    "        self.df['rt'] = self.df[\"sentence\"].str.contains(pat = '@rt|rt@',flags=re.IGNORECASE, regex = True).astype(int) \n",
    "        #df['phone'] = df[\"sentence\"].str.contains(pat = '\\(?([0-9]{3})\\)?([ .-]?)([0-9]{3})\\2([0-9]{4})',flags=re.IGNORECASE, regex = True).astype(int) \n",
    "        self.df['dlex'] = self.df[\"sentence\"].apply(self.lexical_diversity)\n",
    "        self.df[\"sentence\"] = self.df[\"sentence\"].str.lower()\n",
    "        ## List of  US slangs.\n",
    "        slangList = ['ASAP','BBIAB','BBL','BBS','BF','BFF','BFFL','BRB','CYA','DS','FAQ','FB','FITBLR','FLBP','FML','FTFY','FTW','FYI','G2G','GF','GR8','GTFO','HBIC','HML','HRU','HTH','IDK','IGHT','IMO','IMHO','IMY','IRL','ISTG','JK','JMHO','KTHX','L8R','LMAO','LMFAO','LMK','LOL','MWF','NM','NOOB','NP','NSFW','OOAK','OFC','OMG','ORLY','OTOH','RN','ROFL','RUH','SFW','SOML','SOZ','STFU','TFTI','TIL','TMI','TTFN','TTYL','TWSS','U','W/','WB','W/O','WYD','WTH','WTF','WYM','WYSIWYG','Y','YMMV','YW','YWA']\n",
    "        slangList = [x.lower() for x in slangList]\n",
    "        #happy emojis\n",
    "        happy_emojis = [':\\)', ';\\)', '\\(:']\n",
    "        #sad emojis\n",
    "        sad_emojis = [':\\(', ';\\(', '\\):']\n",
    "        punctuation = ['.',',','...','?','!',':',';']    \n",
    "        #','-','+','*','_','=','/','','%',' &','{','}','[',']','(',')','\n",
    "        #Checks if the sentence contains slang\n",
    "        mask = self.df.iloc[:, 0].str.contains(r'\\b(?:{})\\b'.format('|'.join(slangList)))\n",
    "        df1 = self.df[~mask]\n",
    "        self.df['slang'] = mask.astype(int) \n",
    "        #Checks if the sentence contains happy emojis\n",
    "        mask = self.df.iloc[:, 0].str.contains(r'\\b(?:{})\\b'.format('|'.join(happy_emojis)), regex = True)\n",
    "        df1 = self.df[~mask]\n",
    "        self.df['hemojis'] = mask.astype(int) \n",
    "        #Checks if the sentence contains happy emojis\n",
    "        mask = self.df.iloc[:, 0].str.contains(r'\\b(?:{})\\b'.format('|'.join(sad_emojis)), regex = True)\n",
    "        df1 = self.df[~mask]\n",
    "        self.df['semojis'] = mask.astype(int) \n",
    "        self.hand_features =  self.df[['nchars', 'nwords','bhash','nhash','blink','nlink','bat','nat','rt','slang','dlex']]\n",
    "        self.hand_features_DF = pd.DataFrame(self.hand_features)\n",
    "        #################\n",
    "        self.df['sentence'] = self.df['sentence'].str.replace(r'http(\\S)+', r'')\n",
    "        self.df['sentence'] = self.df['sentence'].str.replace(r'http(\\S)+', r'')\n",
    "        self.df['sentence'] = self.df['sentence'].str.replace(r'http ...', r'')\n",
    "        self.df['sentence'] = self.df['sentence'].str.replace(r'(RT|rt)[ ]*@[ ]*[\\S]+',r'')\n",
    "        self.df['sentence'] = self.df['sentence'].str.replace(r'@[\\S]+',r'')\n",
    "        self.df['sentence'] = self.df['sentence'].str.replace(r'_[\\S]?',r'')\n",
    "        self.df['sentence'] = self.df['sentence'].str.replace(r'[ ]{2, }',r' ')\n",
    "        self.df['sentence'] = self.df['sentence'].str.replace(r'&amp;?',r'and')\n",
    "        self.df['sentence'] = self.df['sentence'].str.replace(r'&lt;',r'<')\n",
    "        self.df['sentence'] = self.df['sentence'].str.replace(r'&gt;',r'>')\n",
    "        self.df['sentence'] = self.df['sentence'].str.replace(r'([\\w\\d]+)([^\\w\\d ]+)', r'\\1 \\2')\n",
    "        self.df['sentence'] = self.df['sentence'].str.replace(r'([^\\w\\d ]+)([\\w\\d]+)', r'\\1 \\2')\n",
    "        self.df['sentence'] = self.df['sentence'].str.lower()\n",
    "        self.df['sentence'] = self.df['sentence'].str.strip()\n",
    "        self.sentences = self.df['sentence']\n",
    "        self.labels = self.df['label'].values\n",
    "        self.maxlen = 0\n",
    "        if name == 'covid':\n",
    "            self.maxlen = 256\n",
    "        else:\n",
    "            for sent in self.sentences:\n",
    "                input_ids = self.tokenizer.encode(sent, add_special_tokens=True)\n",
    "                self.maxlen = max(self.maxlen, len(input_ids))\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    def __getitem__(self, idx):\n",
    "        sentence = self.df.loc[idx, 'sentence']\n",
    "        label = self.df.loc[idx, 'label']\n",
    "        h_features = self.hand_features_DF.loc[idx,['nchars', 'nwords','bhash','nhash','blink','nlink','bat','nat','rt','slang','dlex']]\n",
    "        h_features_tensor = torch.tensor(h_features).to(device)\n",
    "        tokens = self.tokenizer.tokenize(sentence)\n",
    "        encoded_dict = self.tokenizer.encode_plus(tokens, add_special_tokens = True, max_length = self.maxlen, pad_to_max_length = True,return_attention_mask = True)\n",
    "        tokens_ids = encoded_dict['input_ids']\n",
    "        tokens_ids_tensor = torch.tensor(tokens_ids).to(device) #Converting the list to a pytorch tensor\n",
    "        attn_mask = encoded_dict['attention_mask']\n",
    "        attn_mask_tensor = torch.tensor(attn_mask).to(device)\n",
    "        label_tensor = torch.tensor(label).to(device)\n",
    "        return tokens_ids_tensor, attn_mask_tensor, label_tensor,h_features_tensor\n",
    "    def lexical_diversity(self,text):\n",
    "        return len(set(text.split())) / len(text.split())\n",
    "\n",
    "    \n",
    "def train(model, train_dataloader, validation_dataloader=None, epochs=10, evaluation=False):\n",
    "    training_stats = []\n",
    "    total_t0 = time.time()\n",
    "    for epoch_i in range(0, epochs):  # For each epoch...\n",
    "        t0 = time.time() ###Measure how long the training epoch takes.\n",
    "        total_train_loss = 0 ### Reset the total loss for this epoch.\n",
    "        model.train()   ### Put the model into training mode.\n",
    "        for step, batch in enumerate(train_dataloader):   ### For each batch of training data...\n",
    "            b_input_ids = batch[0].to(device)             ### `batch` contains three pytorch tensors:    #   [0]: input ids      #   [1]: attention masks    #   [2]: labels\n",
    "            b_input_mask = batch[1].to(device)\n",
    "            b_labels = batch[2].to(device)\n",
    "            model.zero_grad()  ### Clear any previously calculated gradients before performing a backward pass.\n",
    "            loss, logits, hidden_states = model(b_input_ids,token_type_ids=None,attention_mask=b_input_mask,labels=b_labels)   #### Perform a forward pass\n",
    "            total_train_loss += loss.item() ### Accumulate the training loss over all of the batches\n",
    "            loss.backward() ### Perform a backward pass to calculate the gradients\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0) ### Clip the norm of the gradients to 1.0 to help prevent the \"exploding gradients\" problem.\n",
    "            optimizer.step()  ### Update parameters and take a step using the computed gradient.\n",
    "            scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "        avg_train_loss = total_train_loss / len(train_dataloader) ### Calculate the average loss over all of the batches.\n",
    "        training_time = format_time(time.time() - t0)\n",
    "        #print(\"\")\n",
    "        #print(\"  Average training loss: {0:.3f}\".format(avg_train_loss))\n",
    "        #print(\"  Training epoch took: {:}\".format(training_time))\n",
    "        print(\"{},{},{}\".format(epoch_i + 1,avg_train_loss,training_time))\n",
    "        #print(\"\")\n",
    "        # =======================================\n",
    "        #               Evaluation\n",
    "        # =======================================\n",
    "        if evaluation == True:\n",
    "            print(\"Running Validation...\")\n",
    "            model.eval() ### Put the model in evaluation mode--the dropout layers behave differently during evaluation.\n",
    "            total_eval_accuracy = 0\n",
    "            total_eval_fscore = 0\n",
    "            total_eval_loss = 0\n",
    "            nb_eval_steps = 0\n",
    "            report_list  = []   \n",
    "            predictions , true_labels = [], []\n",
    "            for batch in val_dataloader:  # Unpack this training batch from our dataloader.\n",
    "                b_input_ids = batch[0].to(device)\n",
    "                b_input_mask = batch[1].to(device)\n",
    "                b_labels = batch[2].to(device)\n",
    "                with torch.no_grad():  ### Tell pytorch not to bother with constructing the compute graph during  the forward pass, since this is only needed for backprop (training).\n",
    "                    (loss, logits, hidden_states) = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask,labels=b_labels)\n",
    "                total_eval_loss += loss.item() ### Accumulate the validation loss.\n",
    "                logits = logits.detach().cpu().numpy()\n",
    "                label_ids = b_labels.to('cpu').numpy()\n",
    "                #total_eval_accuracy += flat_accuracy(logits, label_ids) ###  Calculate the accuracy for this batch of test sentences, and accumulate it over all batches.\n",
    "                pred_flat = np.argmax(logits, axis=1).flatten()\n",
    "                labels_flat = label_ids.flatten()\n",
    "                true_labels = np.concatenate((true_labels,labels_flat))\n",
    "                predictions = np.concatenate((predictions,pred_flat))\n",
    "            report = classification_report(true_labels, predictions)\n",
    "            #avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n",
    "            #print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n",
    "            avg_val_loss = total_eval_loss / len(val_dataloader)\n",
    "            validation_time = format_time(time.time() - t0)\n",
    "            print(\"  Validation Loss: {0:.3f}\".format(avg_val_loss))\n",
    "            print(\"  Validation took: {:}\".format(validation_time))\n",
    "            #print()\n",
    "            print(report)\n",
    "            print()\n",
    "            training_stats.append(\n",
    "                {\n",
    "                    'epoch': epoch_i + 1,\n",
    "                    'Training Loss': avg_train_loss,\n",
    "                    'Valid. Loss': avg_val_loss,\n",
    "                    'Training Time': training_time,\n",
    "                    'Validation Time': validation_time\n",
    "                }\n",
    "            )\n",
    "    print(\"Training complete!\")\n",
    "    print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0))) \n",
    "    df_stats = pd.DataFrame(data=training_stats)\n",
    "    df_stats = df_stats.set_index('epoch')\n",
    "    #plot_loss_curve(df_stats, epochs, \"/home/joao/loss_covid.png\")\n",
    "   \n",
    "\n",
    "\n",
    "train_dataset =  CustomDataset(\"/home/joao/COVID19Tweet-master/train.tsv\",\"covid\")\n",
    "val_dataset =  CustomDataset(\"/home/joao/COVID19Tweet-master/valid.tsv\",\"covid\")\n",
    "datasets = [train_dataset,val_dataset]\n",
    "dataset = torch.utils.data.ConcatDataset(datasets)\n",
    "\n",
    "dataset =  CustomDataset(None,\"crisislext6\")\n",
    "\n",
    "dataset =  CustomDataset(None,\"crisislext26\")\n",
    "\n",
    "\n",
    "# Create a 90-10 train-validation split.\n",
    "train_size = int(.9 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "# Split dataset randomly according to the training and validation sizes\n",
    "dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "print('{:>5,} training samples'.format(train_size))\n",
    "print('{:>5,} validation samples'.format(val_size))\n",
    "\n",
    "\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "train_dataloader = DataLoader(dataset,sampler = SequentialSampler(dataset), batch_size = batch_size )\n",
    "val_dataloader = DataLoader(val_dataset,sampler = SequentialSampler(val_dataset), batch_size = batch_size )\n",
    "\n",
    "#complete data set\n",
    "dataloader = DataLoader(dataset,sampler = SequentialSampler(dataset), batch_size = batch_size )\n",
    "\n",
    "\n",
    "def initialize_model(dataloader,epochs=100):\n",
    "    model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\",num_labels = 2,output_attentions = False, output_hidden_states = True)\n",
    "    model.cuda()\n",
    "    #optimizer = optim.Adam(model.parameters(), lr=2e-5)\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "    #optimizer = AdamW(model.parameters(),lr=5e-5,eps=1e-8)\n",
    "    #optimizer = optim.Adagrad(params, lr=0.01, lr_decay=0, weight_decay=0, initial_accumulator_value=0, eps=1e-10)\n",
    "    #optimizer = optim.Adadelta(model.parameters(), lr=1.0, rho=0.9, eps=1e-06, weight_decay=0)\n",
    "    total_steps = len(dataloader) * epochs\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer,num_warmup_steps=0,num_training_steps=total_steps)\n",
    "    return model, optimizer, scheduler\n",
    "\n",
    "\n",
    "set_seed(42)    # Set seed for reproducibility\n",
    "model, optimizer, scheduler = initialize_model(dataloader,epochs=100)\n",
    "val_dataloader = None\n",
    "train(model, dataloader, val_dataloader, epochs=100, evaluation=False)\n",
    "\n",
    "\n",
    "def train_and_evalCV(model, dataset, cv=10, epochs=10, batch_size = 32):\n",
    "    training_stats = []\n",
    "    stats = []\n",
    "    total_t0 = time.time()\n",
    "    train_dataloader = DataLoader(dataset,sampler = SequentialSampler(dataset), batch_size = batch_size )\n",
    "    kf = KFold(n_splits=cv)\n",
    "    for train_index, test_index in kf.split(dataset): # For each fold...\n",
    "        train_dataset = torch.utils.data.Subset(dataset,train_index)\n",
    "        val_dataset  = torch.utils.data.Subset(dataset, test_index)\n",
    "        train_dataloader = DataLoader(train_dataset, sampler = SequentialSampler(train_dataset),batch_size = batch_size)\n",
    "        val_dataloader = DataLoader(val_dataset, sampler = SequentialSampler(val_dataset),batch_size = batch_size)    \n",
    "        model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\",num_labels = 2,output_attentions = False, output_hidden_states = True)\n",
    "        model.cuda()\n",
    "        #optimizer = AdamW(model.parameters(),lr=5e-5,eps=1e-8)\n",
    "        optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "        total_steps = len(train_dataloader) * epochs\n",
    "        scheduler = get_linear_schedule_with_warmup(optimizer,num_warmup_steps=0,num_training_steps=total_steps)\n",
    "        for epoch_i in range(0, epochs):  # For each epoch...\n",
    "            #print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "            t0 = time.time()\n",
    "            total_train_loss = 0\n",
    "            model.train()\n",
    "            for step, batch in enumerate(train_dataloader):\n",
    "                b_input_ids = batch[0].to(device)             ### `batch` contains three pytorch tensors:    #   [0]: input ids      #   [1]: attention masks    #   [2]: labels\n",
    "                b_input_mask = batch[1].to(device)\n",
    "                b_labels = batch[2].to(device)\n",
    "                model.zero_grad()\n",
    "                loss, logits, hidden_states = model(b_input_ids,token_type_ids=None,attention_mask=b_input_mask,labels=b_labels)\n",
    "                total_train_loss += loss.item()\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "            avg_train_loss = total_train_loss / len(train_dataloader)\n",
    "            training_time = format_time(time.time() - t0)\n",
    "            training_stats.append({'epoch': epoch_i + 1,'Training Loss': avg_train_loss,'Training Time': training_time})\n",
    "            print({'epoch': epoch_i + 1,'Training Loss': avg_train_loss,'Training Time': training_time})\n",
    "#    sss = StratifiedShuffleSplit(n_splits=cv, test_size=0.5, random_state=0)\n",
    "#    skf = StratifiedKFold(n_splits=cv)\n",
    "        predictions , true_labels = [], []\n",
    "        model.eval()\n",
    "        for step, batch in enumerate(val_dataloader):\n",
    "            b_input_ids = batch[0].to(device)\n",
    "            b_input_mask = batch[1].to(device)\n",
    "            b_labels = batch[2].to(device)\n",
    "            with torch.no_grad():\n",
    "                (loss, logits, hidden_states) = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask,labels=b_labels)\n",
    "            logits = logits.detach().cpu().numpy()\n",
    "            label_ids = b_labels.to('cpu').numpy()\n",
    "            pred_flat = np.argmax(logits, axis=1).flatten()\n",
    "            labels_flat = label_ids.flatten()\n",
    "            true_labels = np.concatenate((true_labels,labels_flat))\n",
    "            predictions = np.concatenate((predictions,pred_flat))\n",
    "        stats.append(precision_recall_fscore_support(true_labels, predictions, average='macro'))\n",
    "        print(precision_recall_fscore_support(true_labels, predictions, average='macro'))\n",
    "        print(classification_report(true_labels,predictions))\n",
    "    #print(stats)\n",
    "    aggP,aggR,aggF = 0.0,0.0, 0.0\n",
    "    for scores in stats:\n",
    "        aggP+=scores[0]\n",
    "        aggR+=scores[1]\n",
    "        aggF+=scores[2]\n",
    "    avgP = aggP/len(stats)\n",
    "    avgR = aggR/len(stats)\n",
    "    avgF = aggF/len(stats)\n",
    "    print(\"P: {0:.3f}, R: {0:.3f}, F: {0:.3f} \".format(avgP, avgR, avgF))\n",
    "\n",
    "\n",
    "    \n",
    "set_seed(42)    # Set seed for reproducibility\n",
    "model, optimizer, scheduler = initialize_model(dataloader,epochs=10)\n",
    "train_and_evalCV(model, dataset, cv=10, epochs=10, batch_size = 8)\n",
    "\n",
    "# Saving and Loading Functions\n",
    "def save_model(path, model, optimizer, valid_loss):\n",
    "    if save_path == None:\n",
    "        return\n",
    "    state_dict = {'model_state_dict': model.state_dict(),\n",
    "                  'optimizer_state_dict': optimizer.state_dict(),\n",
    "                  'valid_loss': valid_loss}\n",
    "    torch.save(state_dict, save_path)\n",
    "    print(f'Model saved to ==> {save_path}')\n",
    "\n",
    "\n",
    "def load_model(path, model, optimizer):\n",
    "    if load_path==None:\n",
    "        return\n",
    "    state_dict = torch.load(load_path, map_location=device)\n",
    "    print(f'Model loaded from <== {load_path}')\n",
    "    model.load_state_dict(state_dict['model_state_dict'])\n",
    "    optimizer.load_state_dict(state_dict['optimizer_state_dict'])\n",
    "    return state_dict['valid_loss']\n",
    "\n",
    "\n",
    "\n",
    "                 \n",
    "## Method to plot the loss curve\n",
    "def plot_loss_curve(df_stats, nepochs, filename):\n",
    "    plt.rcParams[\"figure.figsize\"] = (12,6)# Increase the plot size and font size.\n",
    "    plt.grid()\n",
    "    plt.plot(df_stats['Training Loss'], 'b-o', label=\"Training Loss\")\n",
    "    plt.title(\"Training Loss\")\n",
    "    plt.xlabel(\"# Epochs\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    plt.xticks(np.array(range(nepochs)))\n",
    "    plt.savefig(filename)\n",
    "    \n",
    "set_seed(42)    # Set seed for reproducibility\n",
    "model, optimizer, scheduler = initialize_model(dataloader,epochs=100)\n",
    "model, df_stats = train(model, dataloader, epochs=100)\n",
    "\n",
    "\n",
    "load_checkpoint(destination_folder + '/model.pt', best_model)\n",
    "evaluate(best_model, test_iter)                 \n",
    "                                           \n",
    "    \n",
    "    \n",
    "def main():\n",
    "    print(\"Hello World!\")\n",
    "\n",
    "    \n",
    "# Display floats with two decimal places.\n",
    "#df_stats.set_option('Training Loss', 3)\n",
    "\n",
    "# Create a DataFrame from our training statistics.\n",
    "#df_stats = pd.DataFrame(data=stats)\n",
    "\n",
    "# Use the 'epoch' as the row index.\n",
    "#df_stats = df_stats.set_index('epoch')\n",
    "\n",
    "# A hack to force the column headers to wrap.\n",
    "#df_stats = df_stats.style.set_table_styles([dict(selector=\"th\",props=[('max-width', '70px')])])\n",
    "\n",
    "# Display the table.\n",
    "#df_stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import matthews_corrcoef\n",
    "\n",
    "matthews_set = []\n",
    "\n",
    "# Evaluate each test batch using Matthew's correlation coefficient\n",
    "print('Calculating Matthews Corr. Coef. for each batch...')\n",
    "\n",
    "# For each input batch...\n",
    "for i in range(len(true_labels)):\n",
    "  \n",
    "  # The predictions for this batch are a 2-column ndarray (one column for \"0\" \n",
    "  # and one column for \"1\"). Pick the label with the highest value and turn this\n",
    "  # in to a list of 0s and 1s.\n",
    "  pred_labels_i = np.argmax(predictions[i], axis=1).flatten()\n",
    "  \n",
    "  # Calculate and store the coef for this batch.  \n",
    "  matthews = matthews_corrcoef(true_labels[i], pred_labels_i)                \n",
    "  matthews_set.append(matthews)\n",
    "\n",
    "# Create a barplot showing the MCC score for each batch of test samples.\n",
    "ax = sns.barplot(x=list(range(len(matthews_set))), y=matthews_set, ci=None)\n",
    "\n",
    "plt.title('MCC Score per Batch')\n",
    "plt.ylabel('MCC Score (-1 to +1)')\n",
    "plt.xlabel('Batch #')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
