{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import tensorflow as tf\n",
    "import torch \n",
    "from torch.utils.data import DataLoader, Dataset, TensorDataset\n",
    "from torch.utils.data.sampler import SequentialSampler\n",
    "import transformers as ppb \n",
    "from transformers import BertTokenizer\n",
    "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
    "\n",
    "\n",
    "def format_time(elapsed):\n",
    "    '''\n",
    "    Takes a time in seconds and returns a string hh:mm:ss\n",
    "    '''\n",
    "    # Round to the nearest second.\n",
    "    elapsed_rounded = int(round((elapsed)))    \n",
    "    # Format as hh:mm:ss\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))\n",
    "\n",
    "# Function to calculate the accuracy of our predictions vs labels\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
    "\n",
    "# Function to calculate the f1 of our predictions vs labels\n",
    "def flat_fscore(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return f1_score(labels_flat, pred_flat, average='macro')\n",
    "   \n",
    "\n",
    "df = pd.read_csv(\"/home/joao/crisisLexT6.csv\", encoding='utf-8')\n",
    "\n",
    "print()\n",
    "print('Number of sentences in the original dataset: {:,}\\n'.format(df.shape[0]))\n",
    "\n",
    "#Relabelling the columns titles to remove white spaces\n",
    "df = df.rename(columns={' tweet': 'sentence'})\n",
    "df = df.rename(columns={' label': 'label'})\n",
    "\n",
    "df['label'].replace('on-topic', 1)\n",
    "df['label'] = df['label'].replace('on-topic', 1)\n",
    "\n",
    "df['label'].replace('off-topic', 0)\n",
    "df['label'] = df['label'].replace('off-topic', 0)\n",
    "\n",
    "\n",
    "labels = df['label'].values\n",
    "sentences = df['sentence']\n",
    "\n",
    "\n",
    "#Dropping useless columns as I will only be using the tweet text and the corresponding label\n",
    "df = df[['sentence','label']]\n",
    "print(df.keys())\n",
    "print(df['label'].value_counts())\n",
    "\n",
    "                        \n",
    "### BERT\n",
    "\n",
    "                        #### Doing all the text pre processing\n",
    "        \n",
    "\n",
    "\n",
    "# Get the GPU device name.\n",
    "device_name = tf.test.gpu_device_name()\n",
    "# The device name should look like the following:\n",
    "if device_name == '/device:GPU:0':\n",
    "    print('Found GPU at: {}'.format(device_name))\n",
    "else:\n",
    "    raise SystemError('GPU device not found')\n",
    "    \n",
    "\n",
    "# If there's a GPU available...\n",
    "if torch.cuda.is_available():    \n",
    "    device = torch.device(\"cuda\")\n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")   \n",
    "\n",
    "#labels = df['label']\n",
    "sentences = df['sentence']\n",
    "sentences.head()\n",
    "\n",
    "\n",
    "### Remove URL, RT, mention(@)\n",
    "df.ProcessedText = df.sentence.str.replace(r'http(\\S)+', r'')\n",
    "df.ProcessedText = df.ProcessedText.str.replace(r'http ...', r'')\n",
    "df.ProcessedText[df.ProcessedText.str.contains(r'http')]\n",
    "df.ProcessedText = df.ProcessedText.str.replace(r'(RT|rt)[ ]*@[ ]*[\\S]+',r'')\n",
    "df.ProcessedText[df.ProcessedText.str.contains(r'RT[ ]?@')]\n",
    "df.ProcessedText = df.ProcessedText.str.replace(r'@[\\S]+',r'')\n",
    "df.ProcessedText = df.ProcessedText.str.replace(r'_[\\S]?',r'')\n",
    "\n",
    "#Remove extra space\n",
    "df.ProcessedText = df.ProcessedText.str.replace(r'[ ]{2, }',r' ')\n",
    "\n",
    "#Removing &, < and >\n",
    "df.ProcessedText = df.ProcessedText.str.replace(r'&amp;?',r'and')\n",
    "\n",
    "#Remove extra space\n",
    "df.ProcessedText = df.ProcessedText.str.replace(r'&lt;',r'<')\n",
    "df.ProcessedText = df.ProcessedText.str.replace(r'&gt;',r'>')\n",
    "\n",
    "#Insert space between words and punctuation marks\n",
    "df.ProcessedText = df.ProcessedText.str.replace(r'([\\w\\d]+)([^\\w\\d ]+)', r'\\1 \\2')\n",
    "df.ProcessedText = df.ProcessedText.str.replace(r'([^\\w\\d ]+)([\\w\\d]+)', r'\\1 \\2')\n",
    "\n",
    "#Lowercased and strip\n",
    "df.ProcessedText = df.ProcessedText.str.lower()\n",
    "df.ProcessedText = df.ProcessedText.str.strip()\n",
    "\n",
    "sentences = df.ProcessedText\n",
    "\n",
    "# Load the BERT tokenizer.\n",
    "print('Loading BERT tokenizer...')\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "\n",
    "max_len = 0\n",
    "# For every sentence...\n",
    "for sent in sentences:\n",
    "    # Tokenize the text and add `[CLS]` and `[SEP]` tokens.\n",
    "    input_ids = tokenizer.encode(sent, add_special_tokens=True)\n",
    "    # Update the maximum sentence length.\n",
    "    max_len = max(max_len, len(input_ids))    \n",
    "print('Max sentence length: ', max_len)\n",
    "\n",
    "\n",
    "#BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "  \n",
    "# Tokenization\n",
    "tokenized = sentences.apply((lambda x: tokenizer.encode(x,add_special_tokens=True)))\n",
    "#Padding\n",
    "padded = np.array([i + [0]*(max_len-len(i)) for i in tokenized.values])\n",
    "# Masking\n",
    "attention_mask = np.where(padded != 0, 1, 0)\n",
    "\n",
    "\n",
    "\n",
    "input_ids = torch.tensor(padded).to(device)\n",
    "attention_mask = torch.tensor(attention_mask).to(device)\n",
    "labels = torch.tensor(df[\"label\"].values).to(device)\n",
    "\n",
    "dataset = TensorDataset(input_ids, attention_mask, labels)\n",
    "\n",
    "# Create a 90-10 train-validation split. Calculate the number of samples to include in each set.\n",
    "train_size = int(0.9 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "\n",
    "# Divide the dataset by randomly selecting samples.\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "print('{:>5,} training samples'.format(train_size))\n",
    "print('{:>5,} validation samples'.format(val_size))\n",
    "\n",
    "\n",
    "seed_val = 42\n",
    "\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    " \n",
    "batch_size = 32\n",
    "train_dataloader = DataLoader(train_dataset,sampler = RandomSampler(train_dataset), batch_size = batch_size )\n",
    "validation_dataloader = DataLoader(val_dataset,sampler = SequentialSampler(val_dataset),batch_size = batch_size)\n",
    "\n",
    "training_stats = []\n",
    "\n",
    "total_t0 = time.time()\n",
    "\n",
    "epochs = 4\n",
    "\n",
    "optimizer = AdamW(model.parameters(),lr = 2e-5,eps = 1e-8 )\n",
    "\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\",num_labels = 2,output_attentions = False, output_hidden_states = True)\n",
    "model.to(device)\n",
    "\n",
    "for epoch_i in range(0, epochs):  # For each epoch...\n",
    "    print(\"\")\n",
    "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "    print('Training...')\n",
    "    t0 = time.time() ###Measure how long the training epoch takes.\n",
    "    total_train_loss = 0 ### Reset the total loss for this epoch.    \n",
    "    model.train()   ### Put the model into training mode.      \n",
    "    for step, batch in enumerate(train_dataloader):   ### For each batch of training data...\n",
    "        b_input_ids = batch[0].to(device)             ### `batch` contains three pytorch tensors:    #   [0]: input ids      #   [1]: attention masks    #   [2]: labels \n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "        if step % 40 == 0 and not step == 0: ### Progress update every 40 batches.\n",
    "            elapsed = format_time(time.time() - t0)                       \n",
    "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
    "        model.zero_grad()  ### Clear any previously calculated gradients before performing a backward pass. \n",
    "        loss, logits, hidden_states = model(b_input_ids,token_type_ids=None,attention_mask=b_input_mask,labels=b_labels)   #### Perform a forward pass \n",
    "        total_train_loss += loss.item() ### Accumulate the training loss over all of the batches\n",
    "        loss.backward() ### Perform a backward pass to calculate the gradients\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0) ### Clip the norm of the gradients to 1.0 to help prevent the \"exploding gradients\" problem.                        \n",
    "        optimizer.step()  ### Update parameters and take a step using the computed gradient.\n",
    "    avg_train_loss = total_train_loss / len(train_dataloader) ### Calculate the average loss over all of the batches.\n",
    "    training_time = format_time(time.time() - t0)\n",
    "    print(\"\")\n",
    "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
    "    print(\"  Training epcoh took: {:}\".format(training_time))\n",
    "    print(\"\")\n",
    "    print(\"Running Validation...\")\n",
    "    model.eval() ### Put the model in evaluation mode--the dropout layers behave differently during evaluation.\n",
    "    total_eval_accuracy = 0\n",
    "    total_eval_fscore = 0\n",
    "    total_eval_loss = 0\n",
    "    nb_eval_steps = 0\n",
    "    for batch in validation_dataloader:  # Unpack this training batch from our dataloader.         \n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "        with torch.no_grad():  ### Tell pytorch not to bother with constructing the compute graph during  the forward pass, since this is only needed for backprop (training).\n",
    "            loss, logits, hidden_states = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask,labels=b_labels)      \n",
    "        total_eval_loss += loss.item() ### Accumulate the validation loss.\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "        total_eval_accuracy += flat_accuracy(logits, label_ids) ###  Calculate the accuracy for this batch of test sentences, and accumulate it over all batches.                 \n",
    "        total_eval_fscore += flat_fscore(logits, label_ids)\n",
    "    avg_val_accuracy = total_eval_accuracy / len(validation_dataloader) ### Report the final accuracy for this validation run.\n",
    "    avg_val_fscore = total_eval_fscore / len(validation_dataloader)\n",
    "    print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))    \n",
    "    print(\"  F1-Score: {0:.2f}\".format(avg_val_fscore))    \n",
    "    avg_val_loss = total_eval_loss / len(validation_dataloader) ### Calculate the average loss over all of the batches.\n",
    "    print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
    "    validation_time = format_time(time.time() - t0) ### Measure how long the validation run took.\n",
    "    print(\"  Validation took: {:}\".format(validation_time))    \n",
    "print(\"\")\n",
    "print(\"Training complete!\")\n",
    "print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
