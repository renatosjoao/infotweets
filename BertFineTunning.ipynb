{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "\nFound no NVIDIA driver on your system. Please check that you\nhave an NVIDIA GPU and installed a driver from\nhttp://www.nvidia.com/Download/index.aspx",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-d699771c24aa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'cuda'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m'cpu'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36mset_device\u001b[0;34m(device)\u001b[0m\n\u001b[1;32m    243\u001b[0m     \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_device_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 245\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cuda_setDevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    147\u001b[0m             raise RuntimeError(\n\u001b[1;32m    148\u001b[0m                 \"Cannot re-initialize CUDA in forked subprocess. \" + msg)\n\u001b[0;32m--> 149\u001b[0;31m         \u001b[0m_check_driver\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    150\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_cudart\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m             raise AssertionError(\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36m_check_driver\u001b[0;34m()\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0mFound\u001b[0m \u001b[0mno\u001b[0m \u001b[0mNVIDIA\u001b[0m \u001b[0mdriver\u001b[0m \u001b[0mon\u001b[0m \u001b[0myour\u001b[0m \u001b[0msystem\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mPlease\u001b[0m \u001b[0mcheck\u001b[0m \u001b[0mthat\u001b[0m \u001b[0myou\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0mhave\u001b[0m \u001b[0man\u001b[0m \u001b[0mNVIDIA\u001b[0m \u001b[0mGPU\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0minstalled\u001b[0m \u001b[0ma\u001b[0m \u001b[0mdriver\u001b[0m \u001b[0;32mfrom\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m http://www.nvidia.com/Download/index.aspx\"\"\")\n\u001b[0m\u001b[1;32m     55\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m             \u001b[0;31m# TODO: directly link to the alternative bin that needs install\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: \nFound no NVIDIA driver on your system. Please check that you\nhave an NVIDIA GPU and installed a driver from\nhttp://www.nvidia.com/Download/index.aspx"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import time\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import tensorflow as tf\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from transformers import BertModel\n",
    "from torch.utils.data import DataLoader, Dataset, TensorDataset, random_split, RandomSampler, SequentialSampler, IterableDataset\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW, BertConfig\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import KFold, StratifiedKFold, StratifiedShuffleSplit\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "torch.cuda.set_device(2)\n",
    "torch.cuda.current_device()\n",
    "\n",
    "\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
    "\n",
    "def format_time(elapsed):\n",
    "    # Round to the nearest second.\n",
    "    elapsed_rounded = int(round((elapsed)))    \n",
    "    # Format as hh:mm:ss\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='binary')\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall\n",
    "    }\n",
    "\n",
    "def report_average(report_list):\n",
    "    r_list = list()\n",
    "    for report in report_list:\n",
    "        splited = [' '.join(x.split()) for x in report.split('\\n\\n')]\n",
    "        header = [x for x in splited[0].split(' ')]\n",
    "        data = np.array(splited[1].split(' ')).reshape(-1, len(header) + 1)\n",
    "        data = np.delete(data, 0, 1).astype(float)\n",
    "        df = pd.DataFrame(data, columns=header)\n",
    "        r_list.append(df)\n",
    "    tmp = pd.DataFrame()\n",
    "    for df in r_list:\n",
    "        tmp = tmp.add(df, fill_value=0)           \n",
    "    report_ave =  tmp/len(r_list)\n",
    "    return(report_ave)\n",
    "\n",
    "def flat_fscore(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return f1_score(labels_flat, pred_flat, average='macro')\n",
    "\n",
    "#In [1]: import torch\n",
    "#In [2]: import torch.nn.functional as F \n",
    "#In [3]: probs =  F.softmax(logits_tensor)\n",
    "#probs = probs.detach().cpu().numpy()\n",
    "#predictions  = np.argmax(probs, axis=1).flatten()\n",
    "\n",
    "###matthews_corrcoef(labels, preds)\n",
    "\n",
    "# Add a new row at index k with values provided in list\n",
    "### dfObj.loc['k'] = ['Smriti', 26, 'Bangalore', 'India']\n",
    "### dfObj.iloc[2] = ['Smriti', 26, 'Bangalore', 'India']\n",
    "# get length of all the messages in the train set\n",
    "#seq_len = [len(i.split()) for i in train_text]\n",
    "#pd.Series(seq_len).hist(bins = 30)\n",
    "\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self,filename,name):\n",
    "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "        if name == 'crisismmd':\n",
    "            df = pd.read_csv(filename,delimiter='\\t',encoding='utf-8')\n",
    "            #df = pd.read_csv('/home/joao/task_informative_text_img_dev.tsv',delimiter='\\t',encoding='utf-8')\n",
    "            #df = pd.read_csv('/home/joao/task_informative_text_img_test.tsv',delimiter='\\t',encoding='utf-8')\n",
    "            df = df[['tweet_text','label_text']]\n",
    "            df = df.rename(columns={'tweet_text': 'sentence'})\n",
    "            df = df.rename(columns={'label_text': 'label'})\n",
    "            df['label'] = df['label'].replace('informative', 1).astype(int)\n",
    "            df['label'] = df['label'].replace('not_informative', 0).astype(int)\n",
    "        if name == 'covid':\n",
    "            self.df = pd.read_csv(filename,delimiter='\\t',encoding='utf-8')  \n",
    "            #self.train_df = pd.read_csv(\"/home/joao/COVID19Tweet-master/train.tsv\",delimiter='\\t',encoding='utf-8')  \n",
    "            #self.val_df = pd.read_csv(\"/home/joao/COVID19Tweet-master/valid.tsv\",delimiter='\\t',encoding='utf-8')   \n",
    "            self.df = self.df.rename(columns={'Text': 'sentence'})\n",
    "            self.df = self.df.rename(columns={'Label': 'label'})\n",
    "            self.df['label'].replace('INFORMATIVE', 1)\n",
    "            self.df['label'] = self.df['label'].replace('INFORMATIVE', 1)\n",
    "            self.df['label'].replace('UNINFORMATIVE', 0)\n",
    "            self.df['label'] = self.df['label'].replace('UNINFORMATIVE', 0)\n",
    "        if name == 'crisislext26':\n",
    "            self.df = pd.read_csv(\"/home/joao/crisisLexT26.csv\", encoding='utf-8')\n",
    "            self.df = self.df.drop(['Tweet ID', ' Information Source', ' Information Type' ], axis=1)\n",
    "            #Relabelling the columns titles to remove white spaces\n",
    "            self.df = self.df.rename(columns={' Tweet Text': 'sentence'})\n",
    "            self.df = self.df.rename(columns={' Informativeness': 'label'})\n",
    "            self.df = self.df[self.df.label!= 'Not related']\n",
    "            self.df = self.df[self.df.label!= 'Not applicable']\n",
    "            self.df['label'].replace('Related and informative', 1)\n",
    "            self.df['label'] = self.df['label'].replace('Related and informative', 1)\n",
    "            self.df['label'].replace('Related - but not informative', 0)\n",
    "            self.df['label'] = self.df['label'].replace('Related - but not informative', 0)\n",
    "            self.df = self.df.reset_index(drop=True)\n",
    "        if name == 'crisislext6':\n",
    "            #self.df = pd.read_csv(\"/home/renato/Datasets/CrisisLexT6-v1.0/CrisisLexT6/2012_Sandy_Hurricane/2012_Sandy_Hurricane-ontopic_offtopic.csv\", encoding='utf-8')\n",
    "            #self.df = pd.read_csv(\"/home/joao/2012_Sandy_Hurricane-ontopic_offtopic.csv\",encoding='utf-8')\n",
    "            self.df = pd.read_csv(\"/home/joao/crisisLexT6.csv\", encoding='utf-8')\n",
    "            self.df = self.df.rename(columns={' tweet': 'sentence'})\n",
    "            self.df = self.df.rename(columns={' label': 'label'})\n",
    "            self.df['label'].replace('on-topic', 1)\n",
    "            self.df['label'] = self.df['label'].replace('on-topic', 1)\n",
    "            self.df['label'].replace('off-topic', 0)\n",
    "            self.df['label'] = self.df['label'].replace('off-topic', 0)\n",
    "        self.df = self.df[['sentence','label']]\n",
    "        self.df['nchars'] = self.df['sentence'].str.len()\n",
    "        self.df['nwords'] = self.df['sentence'].str.split().str.len()\n",
    "        self.df['bhash'] = self.df[\"sentence\"].str.contains(pat = '#',flags=re.IGNORECASE, regex = True).astype(int) \n",
    "        self.df['nhash'] = self.df[\"sentence\"].str.count('#') \n",
    "        self.df['blink']  = self.df[\"sentence\"].str.contains(pat = 'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', flags=re.IGNORECASE, regex = True) .astype(int)\n",
    "        self.df['nlink'] = self.df[\"sentence\"].str.count(pat = 'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', flags=re.IGNORECASE) \n",
    "        self.df['bat'] = self.df[\"sentence\"].str.contains(pat = '@',flags=re.IGNORECASE, regex = True).astype(int) \n",
    "        self.df['nat'] = self.df[\"sentence\"].str.count(pat = '@') \n",
    "        self.df['rt'] = self.df[\"sentence\"].str.contains(pat = '@rt|rt@',flags=re.IGNORECASE, regex = True).astype(int) \n",
    "        #df['phone'] = df[\"sentence\"].str.contains(pat = '\\(?([0-9]{3})\\)?([ .-]?)([0-9]{3})\\2([0-9]{4})',flags=re.IGNORECASE, regex = True).astype(int) \n",
    "        self.df['dlex'] = self.df[\"sentence\"].apply(self.lexical_diversity)\n",
    "        self.df[\"sentence\"] = self.df[\"sentence\"].str.lower()\n",
    "        ## List of  US slangs.\n",
    "        slangList = ['ASAP','BBIAB','BBL','BBS','BF','BFF','BFFL','BRB','CYA','DS','FAQ','FB','FITBLR','FLBP','FML','FTFY','FTW','FYI','G2G','GF','GR8','GTFO','HBIC','HML','HRU','HTH','IDK','IGHT','IMO','IMHO','IMY','IRL','ISTG','JK','JMHO','KTHX','L8R','LMAO','LMFAO','LMK','LOL','MWF','NM','NOOB','NP','NSFW','OOAK','OFC','OMG','ORLY','OTOH','RN','ROFL','RUH','SFW','SOML','SOZ','STFU','TFTI','TIL','TMI','TTFN','TTYL','TWSS','U','W/','WB','W/O','WYD','WTH','WTF','WYM','WYSIWYG','Y','YMMV','YW','YWA']\n",
    "        slangList = [x.lower() for x in slangList]\n",
    "        #happy emojis\n",
    "        happy_emojis = [':\\)', ';\\)', '\\(:']\n",
    "        #sad emojis\n",
    "        sad_emojis = [':\\(', ';\\(', '\\):']\n",
    "        punctuation = ['.',',','...','?','!',':',';']    \n",
    "        #','-','+','*','_','=','/','','%',' &','{','}','[',']','(',')','\n",
    "        #Checks if the sentence contains slang\n",
    "        mask = self.df.iloc[:, 0].str.contains(r'\\b(?:{})\\b'.format('|'.join(slangList)))\n",
    "        df1 = self.df[~mask]\n",
    "        self.df['slang'] = mask.astype(int) \n",
    "        #Checks if the sentence contains happy emojis\n",
    "        mask = self.df.iloc[:, 0].str.contains(r'\\b(?:{})\\b'.format('|'.join(happy_emojis)), regex = True)\n",
    "        df1 = self.df[~mask]\n",
    "        self.df['hemojis'] = mask.astype(int) \n",
    "        #Checks if the sentence contains happy emojis\n",
    "        mask = self.df.iloc[:, 0].str.contains(r'\\b(?:{})\\b'.format('|'.join(sad_emojis)), regex = True)\n",
    "        df1 = self.df[~mask]\n",
    "        self.df['semojis'] = mask.astype(int) \n",
    "        self.hand_features =  self.df[['nchars', 'nwords','bhash','nhash','blink','nlink','bat','nat','rt','slang','dlex']]\n",
    "        self.hand_features_DF = pd.DataFrame(self.hand_features)\n",
    "        #################\n",
    "        self.df['sentence'] = self.df['sentence'].str.replace(r'http(\\S)+', r'')\n",
    "        self.df['sentence'] = self.df['sentence'].str.replace(r'http(\\S)+', r'')\n",
    "        self.df['sentence'] = self.df['sentence'].str.replace(r'http ...', r'')\n",
    "        self.df['sentence'] = self.df['sentence'].str.replace(r'(RT|rt)[ ]*@[ ]*[\\S]+',r'')\n",
    "        self.df['sentence'] = self.df['sentence'].str.replace(r'@[\\S]+',r'')\n",
    "        self.df['sentence'] = self.df['sentence'].str.replace(r'_[\\S]?',r'')\n",
    "        self.df['sentence'] = self.df['sentence'].str.replace(r'[ ]{2, }',r' ')\n",
    "        self.df['sentence'] = self.df['sentence'].str.replace(r'&amp;?',r'and')\n",
    "        self.df['sentence'] = self.df['sentence'].str.replace(r'&lt;',r'<')\n",
    "        self.df['sentence'] = self.df['sentence'].str.replace(r'&gt;',r'>')\n",
    "        self.df['sentence'] = self.df['sentence'].str.replace(r'([\\w\\d]+)([^\\w\\d ]+)', r'\\1 \\2')\n",
    "        self.df['sentence'] = self.df['sentence'].str.replace(r'([^\\w\\d ]+)([\\w\\d]+)', r'\\1 \\2')\n",
    "        self.df['sentence'] = self.df['sentence'].str.lower()\n",
    "        self.df['sentence'] = self.df['sentence'].str.strip()\n",
    "        self.sentences = self.df['sentence']\n",
    "        self.labels = self.df['label'].values\n",
    "        self.maxlen = 0\n",
    "        if name == 'covid':\n",
    "            self.maxlen = 256\n",
    "        else:\n",
    "            for sent in self.sentences:\n",
    "                input_ids = self.tokenizer.encode(sent, add_special_tokens=True)\n",
    "                self.maxlen = max(self.maxlen, len(input_ids))\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    def __getitem__(self, idx):\n",
    "        sentence = self.df.loc[idx, 'sentence']\n",
    "        label = self.df.loc[idx, 'label']\n",
    "        h_features = self.hand_features_DF.loc[idx,['nchars', 'nwords','bhash','nhash','blink','nlink','bat','nat','rt','slang','dlex']]\n",
    "        h_features_tensor = torch.tensor(h_features).to(device)\n",
    "        tokens = self.tokenizer.tokenize(sentence)\n",
    "        encoded_dict = self.tokenizer.encode_plus(tokens, add_special_tokens = True, max_length = self.maxlen, pad_to_max_length = True,return_attention_mask = True)\n",
    "        tokens_ids = encoded_dict['input_ids']\n",
    "        tokens_ids_tensor = torch.tensor(tokens_ids).to(device) #Converting the list to a pytorch tensor\n",
    "        attn_mask = encoded_dict['attention_mask']\n",
    "        attn_mask_tensor = torch.tensor(attn_mask).to(device)\n",
    "        label_tensor = torch.tensor(label).to(device)\n",
    "        return tokens_ids_tensor, attn_mask_tensor, label_tensor,h_features_tensor\n",
    "    def lexical_diversity(self,text):\n",
    "        return len(set(text.split())) / len(text.split())\n",
    "\n",
    "\n",
    "def initialize_model(dataloader,epochs=10):\n",
    "    model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\",num_labels = 2,output_attentions = False, output_hidden_states = True)\n",
    "    model.cuda()\n",
    "    #optimizer = optim.Adam(model.parameters(), lr=2e-5)\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "    #optimizer = AdamW(model.parameters(),lr=5e-5,eps=1e-8)\n",
    "    #optimizer = optim.Adagrad(params, lr=0.01, lr_decay=0, weight_decay=0, initial_accumulator_value=0, eps=1e-10)\n",
    "    #optimizer = optim.Adadelta(model.parameters(), lr=1.0, rho=0.9, eps=1e-06, weight_decay=0)\n",
    "    total_steps = len(dataloader) * epochs\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer,num_warmup_steps=10000,num_training_steps=total_steps)\n",
    "    return model, optimizer, scheduler\n",
    "\n",
    "\n",
    "def train(model, train_dataloader, validation_dataloader=None, epochs=10, evaluation=False):\n",
    "    training_stats = []\n",
    "    stats = []\n",
    "    total_t0 = time.time()\n",
    "    best_valid_loss = float(\"Inf\")\n",
    "    for epoch_i in range(0, epochs):  # For each epoch...\n",
    "        t0 = time.time() ###Measure how long the training epoch takes.\n",
    "        total_train_loss = 0 ### Reset the total loss for this epoch.\n",
    "        model.train()   ### Put the model into training mode.\n",
    "        for step, batch in enumerate(train_dataloader):   ### For each batch of training data...\n",
    "            b_input_ids, b_attn_mask, b_labels, h_features = tuple(t.to(device) for t in batch)\n",
    "            model.zero_grad()  ### Clear any previously calculated gradients before performing a backward pass.\n",
    "            loss, logits, hidden_states = model(b_input_ids,token_type_ids=None,attention_mask=b_input_mask,labels=b_labels)   #### Perform a forward pass\n",
    "            total_train_loss += loss.item() ### Accumulate the training loss over all of the batches\n",
    "            loss.backward() ### Perform a backward pass to calculate the gradients\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0) ### Clip the norm of the gradients to 1.0 to help prevent the \"exploding gradients\" problem.\n",
    "            optimizer.step()  ### Update parameters and take a step using the computed gradient.\n",
    "            scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "        avg_train_loss = total_train_loss / len(train_dataloader) ### Calculate the average loss over all of the batches.\n",
    "        training_time = format_time(time.time() - t0)\n",
    "        print({'epoch': epoch_i + 1,'Training Loss': avg_train_loss,'Training Time': training_time})\n",
    "        # =======================================\n",
    "        #               Evaluation\n",
    "        # =======================================\n",
    "        if evaluation == True:\n",
    "            model.eval() ### Put the model in evaluation mode--the dropout layers behave differently during evaluation.\n",
    "            total_eval_accuracy = 0\n",
    "            total_eval_fscore = 0\n",
    "            total_eval_loss = 0\n",
    "            nb_eval_steps = 0\n",
    "            report_list  = []   \n",
    "            predictions , true_labels = [], []\n",
    "            for batch in validation_dataloader:  # Unpack this training batch from our dataloader.\n",
    "                b_input_ids, b_attn_mask, b_labels, h_features = tuple(t.to(device) for t in batch)\n",
    "                with torch.no_grad():  ### Tell pytorch not to bother with constructing the compute graph during  the forward pass, since this is only needed for backprop (training).\n",
    "                    (loss, logits, hidden_states) = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask,labels=b_labels)\n",
    "                total_eval_loss += loss.item() ### Accumulate the validation loss.\n",
    "                logits = logits.detach().cpu().numpy()\n",
    "                label_ids = b_labels.to('cpu').numpy()\n",
    "                pred_flat = np.argmax(logits, axis=1).flatten()\n",
    "                predictions = np.concatenate((predictions,pred_flat), axis=None)\n",
    "                labels_flat = label_ids.flatten()\n",
    "                true_labels = np.concatenate((true_labels,labels_flat), axis=None)\n",
    "            avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
    "            validation_time = format_time(time.time() - t0)\n",
    "            print({'epoch': epoch_i + 1,'Validation Loss': avg_val_loss,'Validation Time': validation_time})\n",
    "            print(classification_report(true_labels,predictions))\n",
    "            if best_valid_loss >  avg_val_loss:\n",
    "                best_valid_loss = avg_val_loss\n",
    "                state_dict = {'model_state_dict': model.state_dict(),'optimizer_state_dict': optimizer.state_dict(),'valid_loss':avg_val_loss}\n",
    "                torch.save(state_dict, \"/home/joao/bert_for_sequence_classification_model.pt\")\n",
    "                print(f'Model saved to ==> /home/joao/bert_for_sequence_classification_model.pt')\n",
    "    print(\"Training complete!\")\n",
    "   \n",
    "\n",
    "def train_and_evalCV(model, dataset, cv=10, epochs=10, batch_size = 32):\n",
    "    training_stats = []\n",
    "    stats = []\n",
    "    total_t0 = time.time()\n",
    "    train_dataloader = DataLoader(dataset,sampler = RandomSampler(dataset), batch_size = 32 )\n",
    "    kf = KFold(n_splits=cv)\n",
    "    for train_index, test_index in kf.split(dataset): # For each fold...\n",
    "        train_dataset = torch.utils.data.Subset(dataset,train_index)\n",
    "        val_dataset  = torch.utils.data.Subset(dataset, test_index)\n",
    "        train_dataloader = DataLoader(train_dataset, sampler = RandomSampler(train_dataset),batch_size = batch_size)\n",
    "        val_dataloader = DataLoader(val_dataset, sampler = RandomSampler(val_dataset),batch_size = batch_size)    \n",
    "        model, optimizer, scheduler = initialize_model(train_dataloader,epochs)\n",
    "        model.cuda()\n",
    "        for epoch_i in range(0, epochs):  # For each epoch...\n",
    "            #print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "            t0 = time.time()\n",
    "            total_train_loss = 0\n",
    "            model.train()\n",
    "            for step, batch in enumerate(train_dataloader):\n",
    "                b_input_ids, b_attn_mask, b_labels, h_features = tuple(t.to(device) for t in batch)\n",
    "                model.zero_grad()\n",
    "                loss, logits, hidden_states = model(b_input_ids,token_type_ids=None,attention_mask=b_attn_mask,labels=b_labels)\n",
    "                total_train_loss += loss.item()\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "            avg_train_loss = total_train_loss / len(train_dataloader)\n",
    "            training_time = format_time(time.time() - t0)\n",
    "            training_stats.append({'epoch': epoch_i + 1,'Training Loss': avg_train_loss,'Training Time': training_time})\n",
    "            print({'epoch': epoch_i + 1,'Training Loss': avg_train_loss,'Training Time': training_time})\n",
    "#    sss = StratifiedShuffleSplit(n_splits=cv, test_size=0.5, random_state=0)\n",
    "#    skf = StratifiedKFold(n_splits=cv)\n",
    "        predictions , true_labels = [], []\n",
    "        model.eval()\n",
    "        for step, batch in enumerate(val_dataloader):\n",
    "            b_input_ids, b_attn_mask, b_labels, h_features = tuple(t.to(device) for t in batch)\n",
    "            with torch.no_grad():\n",
    "                (loss, logits, hidden_states) = model(b_input_ids, token_type_ids=None, attention_mask=b_attn_mask,labels=b_labels)\n",
    "            logits = logits.detach().cpu().numpy()\n",
    "            label_ids = b_labels.to('cpu').numpy()\n",
    "            pred_flat = np.argmax(logits, axis=1).flatten()\n",
    "            labels_flat = label_ids.flatten()\n",
    "            true_labels = np.concatenate((true_labels,labels_flat), axis=None)\n",
    "            predictions = np.concatenate((predictions,pred_flat), axis=None)\n",
    "        stats.append(precision_recall_fscore_support(true_labels, predictions, average='macro'))\n",
    "        print(precision_recall_fscore_support(true_labels, predictions, average='macro'))\n",
    "        print(classification_report(true_labels,predictions))\n",
    "    print()\n",
    "    print(stats)\n",
    "    print()\n",
    "    aggP,aggR,aggF = 0.0,0.0, 0.0\n",
    "    for scores in stats:\n",
    "        aggP+=scores[0]\n",
    "        aggR+=scores[1]\n",
    "        aggF+=scores[2]\n",
    "    avgP = aggP/len(stats)\n",
    "    avgR = aggR/len(stats)\n",
    "    avgF = aggF/len(stats)\n",
    "    print(\"P: {0:.3f}, R: {0:.3f}, F: {0:.3f} \".format(avgP, avgR, avgF))\n",
    "\n",
    "# Saving and Loading Functions\n",
    "def save_model(path, model, optimizer, valid_loss):\n",
    "    if save_path == None:\n",
    "        return\n",
    "    state_dict = {'model_state_dict': model.state_dict(),\n",
    "                  'optimizer_state_dict': optimizer.state_dict(),\n",
    "                  'valid_loss': valid_loss}\n",
    "    torch.save(state_dict, save_path)\n",
    "    print(f'Model saved to ==> {save_path}')\n",
    "\n",
    "def load_model(path, model, optimizer):\n",
    "    if load_path==None:\n",
    "        return\n",
    "    state_dict = torch.load(load_path, map_location=device)\n",
    "    print(f'Model loaded from <== {load_path}')\n",
    "    model.load_state_dict(state_dict['model_state_dict'])\n",
    "    optimizer.load_state_dict(state_dict['optimizer_state_dict'])\n",
    "    return state_dict['valid_loss']\n",
    "                 \n",
    "## Method to plot the loss curve\n",
    "def plot_loss_curve(df_stats, nepochs, filename):\n",
    "    plt.rcParams[\"figure.figsize\"] = (12,6)# Increase the plot size and font size.\n",
    "    plt.grid()\n",
    "    plt.plot(df_stats['Training Loss'], 'b-o', label=\"Training Loss\")\n",
    "    plt.title(\"Training Loss\")\n",
    "    plt.xlabel(\"# Epochs\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    plt.xticks(np.array(range(nepochs)))\n",
    "    plt.savefig(filename)\n",
    "    \n",
    "\n",
    "    \n",
    "def main():\n",
    "    \n",
    "    datasets = ['covid', 'crisislext6', 'crisislext26', 'crisismmd']\n",
    "    \n",
    "    for data in datasets :\n",
    "        print(\"=== {} ===\".format(data))\n",
    "        if data == 'crisismmd':\n",
    "            train_dataset =  CustomDataset(\"/home/joao/task_informative_text_img_train.tsv\",\"crisismmd\")\n",
    "            val_dataset =  CustomDataset(\"/home/joao/task_informative_text_img_dev.tsv\",\"crisismmd\")\n",
    "            test_dataset = CustomDataset(\"/home/joao/task_informative_text_img_test.tsv\",\"crisismmd\")\n",
    "            datasets = [train_dataset,val_dataset,test_dataset]\n",
    "            dataset = torch.utils.data.ConcatDataset(datasets)\n",
    "        if data == 'covid':\n",
    "            train_dataset =  CustomDataset(\"/home/joao/COVID19Tweet-master/train.tsv\",\"covid\")\n",
    "            val_dataset =  CustomDataset(\"/home/joao/COVID19Tweet-master/valid.tsv\",\"covid\")\n",
    "            datasets = [train_dataset,val_dataset]\n",
    "            dataset = torch.utils.data.ConcatDataset(datasets)\n",
    "        if data ==  'crisislext6':\n",
    "            dataset =  CustomDataset(None,\"crisislext6\")\n",
    "            # Create a 90-10 train-validation split.\n",
    "            #train_size = int(0.9 * len(dataset))\n",
    "            #val_size = len(dataset) - train_size\n",
    "            #train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "        if data == 'crisislext26':\n",
    "            dataset =  CustomDataset(None,\"crisislext26\")\n",
    "            # Create a 90-10 train-validation split.\n",
    "            #train_size = int(0.9 * len(dataset))\n",
    "            #val_size = len(dataset) - train_size\n",
    "            #train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "        batch_size = 32\n",
    "        \n",
    "        # Create the DataLoaders for our training and validation sets.\n",
    "        #train_dataloader = DataLoader(train_dataset,sampler = RandomSampler(train_dataset), batch_size = batch_size )\n",
    "        #val_dataloader = DataLoader(val_dataset,sampler = RandomSampler(val_dataset), batch_size = batch_size )\n",
    "\n",
    "        #complete data set\n",
    "        dataloader = DataLoader(dataset,sampler = RandomSampler(dataset), batch_size = batch_size )\n",
    "\n",
    "        set_seed(42)    # Set seed for reproducibility\n",
    "        model, optimizer, scheduler = initialize_model(dataloader,epochs=10)\n",
    "        #val_dataloader = None\n",
    "        #train(model, train_dataloader, val_dataloader, epochs=10, evaluation=False)\n",
    "\n",
    "        train_and_evalCV(model, dataset, cv=10, epochs=10, batch_size = 32)\n",
    "\n",
    "        print(\"===  ===\")\n",
    "        \n",
    "    #parser = argparse.ArgumentParser()\n",
    "    #parser.add_argument('--data_dir', type=str, default='toy_data/names',help='data_directory')\n",
    "    #parser.add_argument('--hidden_dim', type=int, default=32,help='LSTM hidden dimensions')\n",
    "    #parser.add_argument('--batch_size', type=int, default=32,help='size for each minibatch')\n",
    "    #parser.add_argument('--num_epochs', type=int, default=5,help='maximum number of epochs')\n",
    "    #parser.add_argument('--char_dim', type=int, default=128,help='character embedding dimensions')\n",
    "    #parser.add_argument('--learning_rate', type=float, default=0.01,help='initial learning rate')\n",
    "    #parser.add_argument('--weight_decay', type=float, default=1e-4,help='weight_decay rate')\n",
    "    #parser.add_argument('--seed', type=int, default=123,help='seed for random initialisation')\n",
    "    #args = parser.parse_args()\n",
    "    #train(args)   \n",
    "    \n",
    "# Display floats with two decimal places.\n",
    "#df_stats.set_option('Training Loss', 3)\n",
    "\n",
    "# Create a DataFrame from our training statistics.\n",
    "#df_stats = pd.DataFrame(data=stats)\n",
    "\n",
    "# Use the 'epoch' as the row index.\n",
    "#df_stats = df_stats.set_index('epoch')\n",
    "\n",
    "# A hack to force the column headers to wrap.\n",
    "#df_stats = df_stats.style.set_table_styles([dict(selector=\"th\",props=[('max-width', '70px')])])\n",
    "\n",
    "# Display the table.\n",
    "#df_stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import matthews_corrcoef\n",
    "\n",
    "matthews_set = []\n",
    "\n",
    "# Evaluate each test batch using Matthew's correlation coefficient\n",
    "print('Calculating Matthews Corr. Coef. for each batch...')\n",
    "\n",
    "# For each input batch...\n",
    "for i in range(len(true_labels)):\n",
    "  # The predictions for this batch are a 2-column ndarray (one column for \"0\" \n",
    "  # and one column for \"1\"). Pick the label with the highest value and turn this\n",
    "  # in to a list of 0s and 1s.\n",
    "  pred_labels_i = np.argmax(predictions[i], axis=1).flatten()\n",
    "  # Calculate and store the coef for this batch.  \n",
    "  matthews = matthews_corrcoef(true_labels[i], pred_labels_i)                \n",
    "  matthews_set.append(matthews)\n",
    "# Create a barplot showing the MCC score for each batch of test samples.\n",
    "ax = sns.barplot(x=list(range(len(matthews_set))), y=matthews_set, ci=None)\n",
    "plt.title('MCC Score per Batch')\n",
    "plt.ylabel('MCC Score (-1 to +1)')\n",
    "plt.xlabel('Batch #')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
