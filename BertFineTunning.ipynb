{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import time\n",
    "import datetime\n",
    "import re\n",
    "import tensorflow as tf\n",
    "import torch \n",
    "from torch.utils.data import DataLoader, Dataset, TensorDataset,random_split, RandomSampler, SequentialSampler, IterableDataset\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW, BertConfig\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
    "\n",
    "def format_time(elapsed):\n",
    "    # Round to the nearest second.\n",
    "    elapsed_rounded = int(round((elapsed)))    \n",
    "    # Format as hh:mm:ss\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='binary')\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall\n",
    "    }\n",
    "\n",
    "def report_average(report_list):\n",
    "    r_list = list()\n",
    "    for report in report_list:\n",
    "        splited = [' '.join(x.split()) for x in report.split('\\n\\n')]\n",
    "        header = [x for x in splited[0].split(' ')]\n",
    "        data = np.array(splited[1].split(' ')).reshape(-1, len(header) + 1)\n",
    "        data = np.delete(data, 0, 1).astype(float)\n",
    "        df = pd.DataFrame(data, columns=header)\n",
    "        r_list.append(df)\n",
    "    tmp = pd.DataFrame()\n",
    "    for df in r_list:\n",
    "        tmp = tmp.add(df, fill_value=0)           \n",
    "    report_ave =  tmp/len(r_list)\n",
    "    return(report_ave)\n",
    "\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
    "\n",
    "def flat_fscore(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return f1_score(labels_flat, pred_flat, average='macro')\n",
    "\n",
    "#In [1]: import torch\n",
    "#In [2]: import torch.nn.functional as F \n",
    "#In [3]: probs =  F.softmax(logits_tensor)\n",
    "#probs = probs.detach().cpu().numpy()\n",
    "#predictions  = np.argmax(probs, axis=1).flatten()\n",
    "\n",
    "###matthews_corrcoef(labels, preds)\n",
    "\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self,filename,name):\n",
    "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "        if name == 'covid':\n",
    "            self.df = pd.read_csv(filename,delimiter='\\t',encoding='utf-8')  \n",
    "            #self.train_df = pd.read_csv(\"/home/joao/COVID19Tweet-master/train.tsv\",delimiter='\\t',encoding='utf-8')  \n",
    "            #self.val_df = pd.read_csv(\"/home/joao/COVID19Tweet-master/valid.tsv\",delimiter='\\t',encoding='utf-8')   \n",
    "            self.df = self.df.rename(columns={'Text': 'sentence'})\n",
    "            self.df = self.df.rename(columns={'Label': 'label'})\n",
    "            self.df['label'].replace('INFORMATIVE', 1)\n",
    "            self.df['label'] = self.df['label'].replace('INFORMATIVE', 1)\n",
    "            self.df['label'].replace('UNINFORMATIVE', 0)\n",
    "            self.df['label'] = self.df['label'].replace('UNINFORMATIVE', 0)\n",
    "        if name == 'crisislext26':\n",
    "            self.df = pd.read_csv(\"/home/joao/crisisLexT26.csv\", encoding='utf-8')\n",
    "            self.df = self.df.drop(['Tweet ID', ' Information Source', ' Information Type' ], axis=1)\n",
    "            #Relabelling the columns titles to remove white spaces\n",
    "            self.df = self.df.rename(columns={' Tweet Text': 'sentence'})\n",
    "            self.df = self.df.rename(columns={' Informativeness': 'label'})\n",
    "            self.df = self.df[self.df.label!= 'Not related']\n",
    "            self.df = self.df[self.df.label!= 'Not applicable']\n",
    "            self.df['label'].replace('Related and informative', 1)\n",
    "            self.df['label'] = self.df['label'].replace('Related and informative', 1)\n",
    "            self.df['label'].replace('Related - but not informative', 0)\n",
    "            self.df['label'] = self.df['label'].replace('Related - but not informative', 0)\n",
    "            self.df = self.df.reset_index(drop=True)\n",
    "        if name == 'crisislext6':\n",
    "            #self.df = pd.read_csv(\"/home/renato/Datasets/CrisisLexT6-v1.0/CrisisLexT6/2012_Sandy_Hurricane/2012_Sandy_Hurricane-ontopic_offtopic.csv\", encoding='utf-8')\n",
    "            #self.df = pd.read_csv(\"/home/joao/2012_Sandy_Hurricane-ontopic_offtopic.csv\",encoding='utf-8')\n",
    "            self.df = pd.read_csv(\"/home/joao/crisisLexT6.csv\", encoding='utf-8')\n",
    "            self.df = self.df.rename(columns={' tweet': 'sentence'})\n",
    "            self.df = self.df.rename(columns={' label': 'label'})\n",
    "            self.df['label'].replace('on-topic', 1)\n",
    "            self.df['label'] = self.df['label'].replace('on-topic', 1)\n",
    "            self.df['label'].replace('off-topic', 0)\n",
    "            self.df['label'] = self.df['label'].replace('off-topic', 0)\n",
    "        self.df = self.df[['sentence','label']]\n",
    "        self.df['nchars'] = self.df['sentence'].str.len()\n",
    "        self.df['nwords'] = self.df['sentence'].str.split().str.len()\n",
    "        self.df['bhash'] = self.df[\"sentence\"].str.contains(pat = '#',flags=re.IGNORECASE, regex = True).astype(int) \n",
    "        self.df['nhash'] = self.df[\"sentence\"].str.count('#') \n",
    "        self.df['blink']  = self.df[\"sentence\"].str.contains(pat = 'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', flags=re.IGNORECASE, regex = True) .astype(int)\n",
    "        self.df['nlink'] = self.df[\"sentence\"].str.count(pat = 'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', flags=re.IGNORECASE) \n",
    "        self.df['bat'] = self.df[\"sentence\"].str.contains(pat = '@',flags=re.IGNORECASE, regex = True).astype(int) \n",
    "        self.df['nat'] = self.df[\"sentence\"].str.count(pat = '@') \n",
    "        self.df['rt'] = self.df[\"sentence\"].str.contains(pat = '@rt|rt@',flags=re.IGNORECASE, regex = True).astype(int) \n",
    "        #df['phone'] = df[\"sentence\"].str.contains(pat = '\\(?([0-9]{3})\\)?([ .-]?)([0-9]{3})\\2([0-9]{4})',flags=re.IGNORECASE, regex = True).astype(int) \n",
    "        self.df['dlex'] = self.df[\"sentence\"].apply(self.lexical_diversity)\n",
    "        self.df[\"sentence\"] = self.df[\"sentence\"].str.lower()\n",
    "        ## List of  US slangs.\n",
    "        slangList = ['ASAP','BBIAB','BBL','BBS','BF','BFF','BFFL','BRB','CYA','DS','FAQ','FB','FITBLR','FLBP','FML','FTFY','FTW','FYI','G2G','GF','GR8','GTFO','HBIC','HML','HRU','HTH','IDK','IGHT','IMO','IMHO','IMY','IRL','ISTG','JK','JMHO','KTHX','L8R','LMAO','LMFAO','LMK','LOL','MWF','NM','NOOB','NP','NSFW','OOAK','OFC','OMG','ORLY','OTOH','RN','ROFL','RUH','SFW','SOML','SOZ','STFU','TFTI','TIL','TMI','TTFN','TTYL','TWSS','U','W/','WB','W/O','WYD','WTH','WTF','WYM','WYSIWYG','Y','YMMV','YW','YWA']\n",
    "        slangList = [x.lower() for x in slangList]\n",
    "        #happy emojis\n",
    "        happy_emojis = [':\\)', ';\\)', '\\(:']\n",
    "        #sad emojis\n",
    "        sad_emojis = [':\\(', ';\\(', '\\):']\n",
    "        punctuation = ['.',',','...','?','!',':',';']    \n",
    "        #','-','+','*','_','=','/','','%',' &','{','}','[',']','(',')','\n",
    "        #Checks if the sentence contains slang\n",
    "        mask = self.df.iloc[:, 0].str.contains(r'\\b(?:{})\\b'.format('|'.join(slangList)))\n",
    "        df1 = self.df[~mask]\n",
    "        self.df['slang'] = mask.astype(int) \n",
    "        #Checks if the sentence contains happy emojis\n",
    "        mask = self.df.iloc[:, 0].str.contains(r'\\b(?:{})\\b'.format('|'.join(happy_emojis)), regex = True)\n",
    "        df1 = self.df[~mask]\n",
    "        self.df['hemojis'] = mask.astype(int) \n",
    "        #Checks if the sentence contains happy emojis\n",
    "        mask = self.df.iloc[:, 0].str.contains(r'\\b(?:{})\\b'.format('|'.join(sad_emojis)), regex = True)\n",
    "        df1 = self.df[~mask]\n",
    "        self.df['semojis'] = mask.astype(int) \n",
    "        self.hand_features =  self.df[['nchars', 'nwords','bhash','nhash','blink','nlink','bat','nat','rt','slang','dlex']]\n",
    "        self.hand_features_DF = pd.DataFrame(self.hand_features)\n",
    "        #################\n",
    "        self.df['sentence'] = self.df['sentence'].str.replace(r'http(\\S)+', r'')\n",
    "        self.df['sentence'] = self.df['sentence'].str.replace(r'http(\\S)+', r'')\n",
    "        self.df['sentence'] = self.df['sentence'].str.replace(r'http ...', r'')\n",
    "        self.df['sentence'] = self.df['sentence'].str.replace(r'(RT|rt)[ ]*@[ ]*[\\S]+',r'')\n",
    "        self.df['sentence'] = self.df['sentence'].str.replace(r'@[\\S]+',r'')\n",
    "        self.df['sentence'] = self.df['sentence'].str.replace(r'_[\\S]?',r'')\n",
    "        self.df['sentence'] = self.df['sentence'].str.replace(r'[ ]{2, }',r' ')\n",
    "        self.df['sentence'] = self.df['sentence'].str.replace(r'&amp;?',r'and')\n",
    "        self.df['sentence'] = self.df['sentence'].str.replace(r'&lt;',r'<')\n",
    "        self.df['sentence'] = self.df['sentence'].str.replace(r'&gt;',r'>')\n",
    "        self.df['sentence'] = self.df['sentence'].str.replace(r'([\\w\\d]+)([^\\w\\d ]+)', r'\\1 \\2')\n",
    "        self.df['sentence'] = self.df['sentence'].str.replace(r'([^\\w\\d ]+)([\\w\\d]+)', r'\\1 \\2')\n",
    "        self.df['sentence'] = self.df['sentence'].str.lower()\n",
    "        self.df['sentence'] = self.df['sentence'].str.strip()\n",
    "        self.sentences = self.df['sentence']\n",
    "        self.labels = self.df['label'].values\n",
    "        self.maxlen = 0\n",
    "        if name == 'covid':\n",
    "            self.maxlen = 512\n",
    "        else:\n",
    "            for sent in self.sentences:\n",
    "                input_ids = self.tokenizer.encode(sent, add_special_tokens=True)\n",
    "                self.maxlen = max(self.maxlen, len(input_ids))\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    def __getitem__(self, idx):\n",
    "        sentence = self.df.loc[idx, 'sentence']\n",
    "        label = self.df.loc[idx, 'label']\n",
    "        h_features = self.hand_features_DF.loc[idx,['nchars', 'nwords','bhash','nhash','blink','nlink','bat','nat','rt','slang','dlex']]\n",
    "        h_features_tensor = torch.tensor(h_features).to(device)\n",
    "        tokens = self.tokenizer.tokenize(sentence)\n",
    "        encoded_dict = self.tokenizer.encode_plus(tokens, add_special_tokens = True, max_length = self.maxlen, pad_to_max_length = True,return_attention_mask = True)\n",
    "        tokens_ids = encoded_dict['input_ids']\n",
    "        tokens_ids_tensor = torch.tensor(tokens_ids).to(device) #Converting the list to a pytorch tensor\n",
    "        attn_mask = encoded_dict['attention_mask']\n",
    "        attn_mask_tensor = torch.tensor(attn_mask).to(device)\n",
    "        label_tensor = torch.tensor(label).to(device)\n",
    "        return tokens_ids_tensor, attn_mask_tensor, label_tensor,h_features_tensor\n",
    "    def lexical_diversity(self,text):\n",
    "        return len(set(text.split())) / len(text.split())\n",
    "\n",
    "def train(model, train_dataloader, validation_dataloader=None, epochs=4, evaluation=False):\n",
    "    training_stats = []\n",
    "    total_t0 = time.time()\n",
    "    for epoch_i in range(0, epochs):  # For each epoch...\n",
    "        print(\"\")\n",
    "        print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "        print('Training...')\n",
    "        t0 = time.time() ###Measure how long the training epoch takes.\n",
    "        total_train_loss = 0 ### Reset the total loss for this epoch.\n",
    "        model.train()   ### Put the model into training mode.\n",
    "        for step, batch in enumerate(train_dataloader):   ### For each batch of training data...\n",
    "            if step % 40 == 0 and not step == 0: ### Progress update every 40 batches.\n",
    "                elapsed = format_time(time.time() - t0)\n",
    "                print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
    "            b_input_ids = batch[0].to(device)             ### `batch` contains three pytorch tensors:    #   [0]: input ids      #   [1]: attention masks    #   [2]: labels\n",
    "            b_input_mask = batch[1].to(device)\n",
    "            b_labels = batch[2].to(device)\n",
    "            model.zero_grad()  ### Clear any previously calculated gradients before performing a backward pass.\n",
    "            loss, logits, hidden_states = model(b_input_ids,token_type_ids=None,attention_mask=b_input_mask,labels=b_labels)   #### Perform a forward pass\n",
    "            total_train_loss += loss.item() ### Accumulate the training loss over all of the batches\n",
    "            loss.backward() ### Perform a backward pass to calculate the gradients\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0) ### Clip the norm of the gradients to 1.0 to help prevent the \"exploding gradients\" problem.\n",
    "            optimizer.step()  ### Update parameters and take a step using the computed gradient.\n",
    "            scheduler.step()\n",
    "        avg_train_loss = total_train_loss / len(train_dataloader) ### Calculate the average loss over all of the batches.\n",
    "        training_time = format_time(time.time() - t0)\n",
    "        print(\"\")\n",
    "        print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
    "        print(\"  Training epcoh took: {:}\".format(training_time))\n",
    "        print(\"\")\n",
    "        # =======================================\n",
    "        #               Evaluation\n",
    "        # =======================================\n",
    "        if evaluation == True:\n",
    "            print(\"Running Validation...\")\n",
    "            model.eval() ### Put the model in evaluation mode--the dropout layers behave differently during evaluation.\n",
    "            total_eval_accuracy = 0\n",
    "            total_eval_fscore = 0\n",
    "            total_eval_loss = 0\n",
    "            nb_eval_steps = 0\n",
    "            report_list  = []    \n",
    "            for batch in val_dataloader:  # Unpack this training batch from our dataloader.\n",
    "                b_input_ids = batch[0].to(device)\n",
    "                b_input_mask = batch[1].to(device)\n",
    "                b_labels = batch[2].to(device)\n",
    "                with torch.no_grad():  ### Tell pytorch not to bother with constructing the compute graph during  the forward pass, since this is only needed for backprop (training).\n",
    "                    (loss, logits, hidden_states) = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask,labels=b_labels)\n",
    "                total_eval_loss += loss.item() ### Accumulate the validation loss.\n",
    "                logits = logits.detach().cpu().numpy()\n",
    "                label_ids = b_labels.to('cpu').numpy()\n",
    "                #total_eval_accuracy += flat_accuracy(logits, label_ids) ###  Calculate the accuracy for this batch of test sentences, and accumulate it over all batches.\n",
    "                pred_flat = np.argmax(logits, axis=1).flatten()\n",
    "                labels_flat = label_ids.flatten()\n",
    "                report = classification_report(labels_flat, pred_flat)\n",
    "                report_list.append(report)\n",
    "            #avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n",
    "            #print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n",
    "            avg_val_loss = total_eval_loss / len(val_dataloader)\n",
    "            validation_time = format_time(time.time() - t0) ### Measure how long the validation run took.\n",
    "            print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
    "            print(\"  Validation took: {:}\".format(validation_time))\n",
    "            print()\n",
    "            print(report_average(report_list))\n",
    "            print()\n",
    "            training_stats.append(\n",
    "                {\n",
    "                    'epoch': epoch_i + 1,\n",
    "                    'Training Loss': avg_train_loss,\n",
    "                    'Valid. Loss': avg_val_loss,\n",
    "                    'Training Time': training_time,\n",
    "                    'Validation Time': validation_time\n",
    "                }\n",
    "            )\n",
    "    print(\"Training complete!\")\n",
    "    print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))  \n",
    "\n",
    "\n",
    "train_dataset =  CustomDataset(\"/home/joao/COVID19Tweet-master/train.tsv\",\"covid\")\n",
    "val_dataset =  CustomDataset(\"/home/joao/COVID19Tweet-master/valid.tsv\",\"covid\")\n",
    "\n",
    "\n",
    "dataset =  CustomDataset(None,\"crisislext6\")\n",
    "\n",
    "dataset =  CustomDataset(None,\"crisislext26\")\n",
    "\n",
    "\n",
    "# Create a 90-10 train-validation split.\n",
    "train_size = int(0.9 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "print('{:>5,} training samples'.format(train_size))\n",
    "print('{:>5,} validation samples'.format(val_size))\n",
    "\n",
    "\n",
    "\n",
    "batch_size = 8\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset,sampler = SequentialSampler(train_dataset), batch_size = batch_size )\n",
    "val_dataloader = DataLoader(val_dataset,sampler = SequentialSampler(val_dataset), batch_size = batch_size )\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\",num_labels = 2,output_attentions = False, output_hidden_states = True)\n",
    "model.to(device)\n",
    "\n",
    "optimizer = AdamW(model.parameters(),lr = 2e-5,eps = 1e-8 )\n",
    "\n",
    "epochs = 2\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps = 0,num_training_steps = total_steps)\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "train(model, train_dataloader, val_dataloader, epochs=4, evaluation=True)\n",
    "\n",
    "\n",
    "# Display floats with two decimal places.\n",
    "pd.set_option('precision', 3)\n",
    "\n",
    "# Create a DataFrame from our training statistics.\n",
    "df_stats = pd.DataFrame(data=training_stats)\n",
    "\n",
    "# Use the 'epoch' as the row index.\n",
    "df_stats = df_stats.set_index('epoch')\n",
    "\n",
    "# A hack to force the column headers to wrap.\n",
    "df_stats = df_stats.style.set_table_styles([dict(selector=\"th\",props=[('max-width', '70px')])])\n",
    "\n",
    "# Display the table.\n",
    "df_stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "% matplotlib inline\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "# Use plot styling from seaborn.\n",
    "sns.set(style='darkgrid')\n",
    "\n",
    "# Increase the plot size and font size.\n",
    "sns.set(font_scale=1.5)\n",
    "plt.rcParams[\"figure.figsize\"] = (12,6)\n",
    "\n",
    "# Plot the learning curve.\n",
    "plt.plot(df_stats['Training Loss'], 'b-o', label=\"Training\")\n",
    "plt.plot(df_stats['Valid. Loss'], 'g-o', label=\"Validation\")\n",
    "\n",
    "# Label the plot.\n",
    "plt.title(\"Training & Validation Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.xticks([1, 2, 3, 4])\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "\n",
    "matthews_set = []\n",
    "\n",
    "# Evaluate each test batch using Matthew's correlation coefficient\n",
    "print('Calculating Matthews Corr. Coef. for each batch...')\n",
    "\n",
    "# For each input batch...\n",
    "for i in range(len(true_labels)):\n",
    "  \n",
    "  # The predictions for this batch are a 2-column ndarray (one column for \"0\" \n",
    "  # and one column for \"1\"). Pick the label with the highest value and turn this\n",
    "  # in to a list of 0s and 1s.\n",
    "  pred_labels_i = np.argmax(predictions[i], axis=1).flatten()\n",
    "  \n",
    "  # Calculate and store the coef for this batch.  \n",
    "  matthews = matthews_corrcoef(true_labels[i], pred_labels_i)                \n",
    "  matthews_set.append(matthews)\n",
    "\n",
    "# Create a barplot showing the MCC score for each batch of test samples.\n",
    "ax = sns.barplot(x=list(range(len(matthews_set))), y=matthews_set, ci=None)\n",
    "\n",
    "plt.title('MCC Score per Batch')\n",
    "plt.ylabel('MCC Score (-1 to +1)')\n",
    "plt.xlabel('Batch #')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
