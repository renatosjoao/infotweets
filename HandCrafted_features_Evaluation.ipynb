{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "import fasttext\n",
    "import math \n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import csv\n",
    "import re\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score, cross_validate\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, confusion_matrix\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "import torch \n",
    "from torch.utils.data import DataLoader, Dataset, TensorDataset, random_split, IterableDataset\n",
    "from torch.utils.data.sampler import SequentialSampler\n",
    "from transformers import BertTokenizer\n",
    "import transformers as ppb \n",
    "import logging\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from gensim import corpora\n",
    "from gensim.models import TfidfModel\n",
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "from gensim.utils import simple_preprocess\n",
    "import transformers as ppb \n",
    "\n",
    "nltk.download('words')\n",
    "nltk.download('punkt')\n",
    "stopwords.words('english')\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(level=logging.ERROR)\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "#needs to be GLOBAL\n",
    "words = set(nltk.corpus.words.words())\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def remove_digit(text):\n",
    "    return re.sub(r'\\d+', '', text)\n",
    "\n",
    "def remove_non_english(text):\n",
    "    text = [w for w in nltk.wordpunct_tokenize(text) if w in text or not w.isalpha()]\n",
    "    return ' '.join(text)\n",
    "\n",
    "def remove_special_chars(text):\n",
    "    return re.sub(\"(\\\\d|\\\\W)+\",\" \",text)    \n",
    "\n",
    "def remove_shortwords(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    text = [i for i in tokens if len(i) > 2]\n",
    "    return ' '.join(text)\n",
    "\n",
    "def remove_nonUTF8(data):\n",
    "    return bytes(data, 'utf-8').decode('utf-8', 'ignore')\n",
    "\n",
    "def preprocess(df):\n",
    "    df['sentence'] = df['sentence'].str.replace(r'http(\\S)+', r'')\n",
    "    df['sentence'] = df['sentence'].str.replace(r'http(\\S)+', r'')\n",
    "    df['sentence'] = df['sentence'].str.replace(r'http ...', r'')\n",
    "    df['sentence'] = df['sentence'].str.replace(r'(RT|rt)[ ]*@[ ]*[\\S]+',r'')\n",
    "    df['sentence'] = df['sentence'].str.replace(r'@[\\S]+',r'')\n",
    "    df['sentence'] = df['sentence'].str.replace(r'_[\\S]?',r'')\n",
    "    df['sentence'] = df['sentence'].str.replace(r'[ ]{2, }',r' ')\n",
    "    df['sentence'] = df['sentence'].str.replace(r'&amp;?',r'and')\n",
    "    df['sentence'] = df['sentence'].str.replace(r'&lt;',r'<')\n",
    "    df['sentence'] = df['sentence'].str.replace(r'&gt;',r'>')\n",
    "    df['sentence'] = df['sentence'].str.replace(r'([\\w\\d]+)([^\\w\\d ]+)', r'\\1 \\2')\n",
    "    df['sentence'] = df['sentence'].str.replace(r'([^\\w\\d ]+)([\\w\\d]+)', r'\\1 \\2')\n",
    "    df['sentence'] = df['sentence'].str.lower()\n",
    "    df['sentence'] = df['sentence'].str.strip()\n",
    "    df['sentence'] = df['sentence'].apply(remove_stopwords)\n",
    "    df['sentence'] = df['sentence'].apply(remove_digit)\n",
    "    df['sentence'] = df['sentence'].apply(remove_non_english)\n",
    "    df['sentence'] = df['sentence'].apply(remove_special_chars)\n",
    "    df['sentence'] = df['sentence'].apply(remove_nonUTF8)\n",
    "    df['sentence'] = df['sentence'].str.replace(\"\\'\", \"\")\n",
    "    df['sentence'] = df['sentence'].str.replace(\"\\\"\", \"\")\n",
    "    df['sentence'] = df['sentence'].apply(remove_shortwords)\n",
    "    return df\n",
    "\n",
    "#Bag of Words model ONLY\n",
    "def bow_evaluate(fullsetdf,subsetdf):\n",
    "    df = preprocess(fullsetdf)\n",
    "    df.drop(df.columns[[0]], axis=1, inplace=True)\n",
    "    # Tokenize the text column to get the new column 'tokenized_text'\n",
    "    df['tokenized_text'] = [simple_preprocess(line, deacc=True) for line in df['sentence']]\n",
    "    vectorizer = TfidfVectorizer(analyzer = 'word', strip_accents= 'ascii',smooth_idf = True, use_idf=True,max_df = 10000, min_df = 5,  stop_words = 'english')\n",
    "    X = vectorizer.fit_transform(df['sentence'])\n",
    "    tfidf_df = pd.DataFrame(X.toarray(),columns=vectorizer.get_feature_names()) \n",
    "    y_train = pd.DataFrame(fullsetdf['label'])\n",
    "    #dp = pd.concat([dp,y_label],axis=1)\n",
    "    X_train = pd.DataFrame(tfidf_df)\n",
    "    lr_clf = LogisticRegression()\n",
    "    dt_clf = DecisionTreeClassifier()\n",
    "    rf_clf = RandomForestClassifier()\n",
    "    ab_clf = AdaBoostClassifier(n_estimators=100, random_state=0)\n",
    "    nb_clf = GaussianNB()\n",
    "    nn_clf = MLPClassifier(random_state=1, max_iter=300)\n",
    "    svm_clf = svm.SVC(gamma=0.001, C=100.)\n",
    "    scoring = ['accuracy','precision_macro', 'recall_macro', 'f1_macro']\n",
    "    print(\" *** Full dataset TFxIDF features *** \")\n",
    "    scores_lr_clf = cross_validate( lr_clf, X_train, y_train, cv=10, scoring=scoring, return_train_score=False)\n",
    "    scores_dt_clf = cross_validate( dt_clf, X_train, y_train, cv=10, scoring=scoring, return_train_score=False)\n",
    "    scores_rf_clf = cross_validate( rf_clf, X_train, y_train, cv=10, scoring=scoring, return_train_score=False)\n",
    "    scores_ab_clf = cross_validate( ab_clf, X_train, y_train, cv=10, scoring=scoring, return_train_score=False)\n",
    "    scores_nb_clf = cross_validate( nb_clf, X_train, y_train, cv=10, scoring=scoring, return_train_score=False)\n",
    "    scores_nn_clf = cross_validate( nn_clf, X_train, y_train, cv=10, scoring=scoring, return_train_score=False)\n",
    "    scores_svm_clf = cross_validate( svm_clf, X_train, y_train, cv=10, scoring=scoring, return_train_score=False)\n",
    "    print(\"MODELS              ,Accuracy , Precision  ,  Recall   ,  F-score          \", flush=True)\n",
    "    print(\"Logistic Regression ,{:.2f}   ,   {:.2f}   ,  {:.2f}   , {:.2f}(+/- {:.2f})\".format(scores_lr_clf['test_accuracy'].mean()*100.0,scores_lr_clf['test_precision_macro'].mean()*100.0,scores_lr_clf['test_recall_macro'].mean()*100.0,scores_lr_clf['test_f1_macro'].mean()*100.0, scores_lr_clf['test_f1_macro'].std() * 2), flush=True)\n",
    "    print(\"Decision Tree       ,{:.2f}   ,   {:.2f}   ,  {:.2f}   , {:.2f}(+/- {:.2f})\".format(scores_dt_clf['test_accuracy'].mean()*100.0,scores_dt_clf['test_precision_macro'].mean()*100.0,scores_dt_clf['test_recall_macro'].mean()*100.0,scores_dt_clf['test_f1_macro'].mean()*100.0, scores_lr_clf['test_f1_macro'].std() * 2), flush=True)\n",
    "    print(\"Random Forest       ,{:.2f}   ,   {:.2f}   ,  {:.2f}   , {:.2f}(+/- {:.2f})\".format(scores_rf_clf['test_accuracy'].mean()*100.0,scores_rf_clf['test_precision_macro'].mean()*100.0,scores_rf_clf['test_recall_macro'].mean()*100.0,scores_rf_clf['test_f1_macro'].mean()*100.0, scores_lr_clf['test_f1_macro'].std() * 2), flush=True)\n",
    "    print(\"Adaboost            ,{:.2f}   ,   {:.2f}   ,  {:.2f}   , {:.2f}(+/- {:.2f})\".format(scores_ab_clf['test_accuracy'].mean()*100.0,scores_ab_clf['test_precision_macro'].mean()*100.0,scores_ab_clf['test_recall_macro'].mean()*100.0,scores_ab_clf['test_f1_macro'].mean()*100.0, scores_lr_clf['test_f1_macro'].std() * 2), flush=True)\n",
    "    print(\"NaiveBayes          ,{:.2f}   ,   {:.2f}   ,  {:.2f}   , {:.2f}(+/- {:.2f})\".format(scores_nb_clf['test_accuracy'].mean()*100.0,scores_nb_clf['test_precision_macro'].mean()*100.0,scores_nb_clf['test_recall_macro'].mean()*100.0,scores_nb_clf['test_f1_macro'].mean()*100.0, scores_lr_clf['test_f1_macro'].std() * 2), flush=True)\n",
    "    print(\"MLP                 ,{:.2f}   ,   {:.2f}   ,  {:.2f}   , {:.2f}(+/- {:.2f})\".format(scores_nn_clf['test_accuracy'].mean()*100.0,scores_nn_clf['test_precision_macro'].mean()*100.0,scores_nn_clf['test_recall_macro'].mean()*100.0,scores_nn_clf['test_f1_macro'].mean()*100.0, scores_lr_clf['test_f1_macro'].std() * 2), flush=True)\n",
    "    print(\"SVM                 ,{:.2f}   ,   {:.2f}   ,  {:.2f}   , {:.2f}(+/- {:.2f})\".format(scores_svm_clf['test_accuracy'].mean()*100.0,scores_svm_clf['test_precision_macro'].mean()*100.0,scores_svm_clf['test_recall_macro'].mean()*100.0,scores_svm_clf['test_f1_macro'].mean()*100.0, scores_lr_clf['test_f1_macro'].std() * 2), flush=True)\n",
    "    print()\n",
    "    #@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@#\n",
    "    ### SUBSET ###\n",
    "    #@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@#\n",
    "    df = preprocess(subsetdf)\n",
    "    df.drop(df.columns[[0]], axis=1, inplace=True)\n",
    "    # Tokenize the text column to get the new column 'tokenized_text'\n",
    "    df['tokenized_text'] = [simple_preprocess(line, deacc=True) for line in df['sentence']]\n",
    "    vectorizer = TfidfVectorizer(analyzer = 'word', strip_accents= 'ascii',smooth_idf = True, use_idf=True,max_df = 10000, min_df = 5,  stop_words = 'english')\n",
    "    X = vectorizer.fit_transform(df['sentence'])\n",
    "    tfidf_df = pd.DataFrame(X.toarray(),columns=vectorizer.get_feature_names()) \n",
    "    #print(dp.head)\n",
    "    y_train = pd.DataFrame(subsetdf['label'])\n",
    "    #dp = pd.concat([dp,y_label],axis=1)\n",
    "    X_train = pd.DataFrame(tfidf_df)\n",
    "    lr_clf = LogisticRegression()\n",
    "    dt_clf = DecisionTreeClassifier()\n",
    "    rf_clf = RandomForestClassifier()\n",
    "    ab_clf = AdaBoostClassifier(n_estimators=100, random_state=0)\n",
    "    nb_clf = GaussianNB()\n",
    "    nn_clf = MLPClassifier(random_state=1, max_iter=300)\n",
    "    svm_clf = svm.SVC(gamma=0.001, C=100.)\n",
    "    scoring = ['accuracy','precision_macro', 'recall_macro', 'f1_macro']\n",
    "    print(\" *** Subset TFxIDF features *** \")\n",
    "    scores_lr_clf = cross_validate( lr_clf, X_train, y_train, cv=10, scoring=scoring, return_train_score=False)\n",
    "    scores_dt_clf = cross_validate( dt_clf, X_train, y_train, cv=10, scoring=scoring, return_train_score=False)\n",
    "    scores_rf_clf = cross_validate( rf_clf, X_train, y_train, cv=10, scoring=scoring, return_train_score=False)\n",
    "    scores_ab_clf = cross_validate( ab_clf, X_train, y_train, cv=10, scoring=scoring, return_train_score=False)\n",
    "    scores_nb_clf = cross_validate( nb_clf, X_train, y_train, cv=10, scoring=scoring, return_train_score=False)\n",
    "    scores_nn_clf = cross_validate( nn_clf, X_train, y_train, cv=10, scoring=scoring, return_train_score=False)\n",
    "    scores_svm_clf = cross_validate( svm_clf, X_train, y_train, cv=10, scoring=scoring, return_train_score=False)\n",
    "    print(\"MODELS              ,Accuracy , Precision  ,  Recall   ,  F-score          \", flush=True)\n",
    "    print(\"Logistic Regression ,{:.2f}   ,   {:.2f}   ,  {:.2f}   , {:.2f}(+/- {:.2f})\".format(scores_lr_clf['test_accuracy'].mean()*100.0,scores_lr_clf['test_precision_macro'].mean()*100.0,scores_lr_clf['test_recall_macro'].mean()*100.0,scores_lr_clf['test_f1_macro'].mean()*100.0, scores_lr_clf['test_f1_macro'].std() * 2), flush=True)\n",
    "    print(\"Decision Tree       ,{:.2f}   ,   {:.2f}   ,  {:.2f}   , {:.2f}(+/- {:.2f})\".format(scores_dt_clf['test_accuracy'].mean()*100.0,scores_dt_clf['test_precision_macro'].mean()*100.0,scores_dt_clf['test_recall_macro'].mean()*100.0,scores_dt_clf['test_f1_macro'].mean()*100.0, scores_lr_clf['test_f1_macro'].std() * 2), flush=True)\n",
    "    print(\"Random Forest       ,{:.2f}   ,   {:.2f}   ,  {:.2f}   , {:.2f}(+/- {:.2f})\".format(scores_rf_clf['test_accuracy'].mean()*100.0,scores_rf_clf['test_precision_macro'].mean()*100.0,scores_rf_clf['test_recall_macro'].mean()*100.0,scores_rf_clf['test_f1_macro'].mean()*100.0, scores_lr_clf['test_f1_macro'].std() * 2), flush=True)\n",
    "    print(\"Adaboost            ,{:.2f}   ,   {:.2f}   ,  {:.2f}   , {:.2f}(+/- {:.2f})\".format(scores_ab_clf['test_accuracy'].mean()*100.0,scores_ab_clf['test_precision_macro'].mean()*100.0,scores_ab_clf['test_recall_macro'].mean()*100.0,scores_ab_clf['test_f1_macro'].mean()*100.0, scores_lr_clf['test_f1_macro'].std() * 2), flush=True)\n",
    "    print(\"NaiveBayes          ,{:.2f}   ,   {:.2f}   ,  {:.2f}   , {:.2f}(+/- {:.2f})\".format(scores_nb_clf['test_accuracy'].mean()*100.0,scores_nb_clf['test_precision_macro'].mean()*100.0,scores_nb_clf['test_recall_macro'].mean()*100.0,scores_nb_clf['test_f1_macro'].mean()*100.0, scores_lr_clf['test_f1_macro'].std() * 2), flush=True)\n",
    "    print(\"MLP                 ,{:.2f}   ,   {:.2f}   ,  {:.2f}   , {:.2f}(+/- {:.2f})\".format(scores_nn_clf['test_accuracy'].mean()*100.0,scores_nn_clf['test_precision_macro'].mean()*100.0,scores_nn_clf['test_recall_macro'].mean()*100.0,scores_nn_clf['test_f1_macro'].mean()*100.0, scores_lr_clf['test_f1_macro'].std() * 2), flush=True)\n",
    "    print(\"SVM                 ,{:.2f}   ,   {:.2f}   ,  {:.2f}   , {:.2f}(+/- {:.2f})\".format(scores_svm_clf['test_accuracy'].mean()*100.0,scores_svm_clf['test_precision_macro'].mean()*100.0,scores_svm_clf['test_recall_macro'].mean()*100.0,scores_svm_clf['test_f1_macro'].mean()*100.0, scores_lr_clf['test_f1_macro'].std() * 2), flush=True)\n",
    "    print()  \n",
    "\n",
    "        \n",
    "def bert_based_features_evaluate(data):\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    torch.cuda.set_device(3)\n",
    "    batch_size = 16\n",
    "    if data == 'covid':\n",
    "        dataset = FullDataset('/home/joao/covid.ORG.tsv','covid')\n",
    "    if data ==  'crisislext6':\n",
    "        dataset = FullDataset('/home/joao/crisislext6.ORG.tsv','crisislext6')\n",
    "    if data == 'crisislext26':\n",
    "        dataset = FullDataset('/home/joao/crisislext26.ORG.tsv','crisislext26')\n",
    "    if data == 'crisismmd':\n",
    "        dataset = FullDataset('/home/joao/crisismmd.ORG.tsv','crisismmd')\n",
    "    dataloader = DataLoader(dataset,sampler = SequentialSampler(dataset), batch_size = batch_size )\n",
    "    # Model 1.  \n",
    "    # Load pretrained model/tokenizer\n",
    "    model_class, tokenizer_class, pretrained_weights = (ppb.BertModel, ppb.BertTokenizer, 'bert-base-uncased')\n",
    "    model = model_class.from_pretrained(pretrained_weights)\n",
    "    model.to(device)\n",
    "     # For each batch of training data...\n",
    "    _bertLabels = pd.DataFrame()    # dataframe with the Labels Features only \n",
    "    _bertFeatures = pd.DataFrame()  # dataframe with the Bert features only\n",
    "    for batch in dataloader:\n",
    "        with torch.no_grad():\n",
    "            b_input_ids, b_input_mask, hand_features, b_labels = tuple(t.to(device) for t in batch)\n",
    "            last_hidden_states = model(b_input_ids,attention_mask = b_input_mask)\n",
    "            bertfeatures = last_hidden_states[0][:,0,:]#Let's slice only the part of the output that we need. That is the output corresponding the first token of each sentence.  The way BERT does sentence classification, is that it adds a token called [CLS] (for classification) at the beginning of every sentence. The output corresponding to that token can be thought of as an embedding for the entire sentence.\n",
    "            bertfeatures = bertfeatures.cpu().detach().numpy()        \n",
    "            labels = b_labels.cpu().detach().numpy()\n",
    "            _bertLabels = _bertLabels.append(pd.DataFrame(labels),ignore_index = True)\n",
    "            _bertFeatures = _bertFeatures.append(pd.DataFrame(bertfeatures),ignore_index = True)\n",
    "    # Model 2.  \n",
    "    # The output from BERT is going to be input to SKLEARN models\n",
    "    X_train = _bertFeatures\n",
    "    y_train = _bertLabels\n",
    "    lr_clf = LogisticRegression()\n",
    "    dt_clf = DecisionTreeClassifier()\n",
    "    rf_clf = RandomForestClassifier()\n",
    "    ab_clf = AdaBoostClassifier(n_estimators=100, random_state=0)\n",
    "    nb_clf = GaussianNB()\n",
    "    nn_clf = MLPClassifier(random_state=1, max_iter=300)\n",
    "    svm_clf = svm.SVC(gamma=0.001, C=100.)\n",
    "    scoring = ['accuracy','precision_macro', 'recall_macro', 'f1_macro']\n",
    "    print(\" *** Full dataset BERT encoded features *** \")\n",
    "    scores_lr_clf = cross_validate( lr_clf, X_train, y_train, cv=10, scoring=scoring, return_train_score=False)\n",
    "    scores_dt_clf = cross_validate( dt_clf, X_train, y_train, cv=10, scoring=scoring, return_train_score=False)\n",
    "    scores_rf_clf = cross_validate( rf_clf, X_train, y_train, cv=10, scoring=scoring, return_train_score=False)\n",
    "    scores_ab_clf = cross_validate( ab_clf, X_train, y_train, cv=10, scoring=scoring, return_train_score=False)\n",
    "    scores_nb_clf = cross_validate( nb_clf, X_train, y_train, cv=10, scoring=scoring, return_train_score=False)\n",
    "    scores_nn_clf = cross_validate( nn_clf, X_train, y_train, cv=10, scoring=scoring, return_train_score=False)\n",
    "    scores_svm_clf = cross_validate( svm_clf, X_train, y_train, cv=10, scoring=scoring, return_train_score=False)\n",
    "    print(\"MODELS              ,Accuracy , Precision  ,  Recall   ,  F-score          \", flush=True)\n",
    "    print(\"Logistic Regression ,{:.2f}   ,   {:.2f}   ,  {:.2f}   , {:.2f}(+/- {:.2f})\".format(scores_lr_clf['test_accuracy'].mean()*100.0,scores_lr_clf['test_precision_macro'].mean()*100.0,scores_lr_clf['test_recall_macro'].mean()*100.0,scores_lr_clf['test_f1_macro'].mean()*100.0, scores_lr_clf['test_f1_macro'].std() * 2), flush=True)\n",
    "    print(\"Decision Tree       ,{:.2f}   ,   {:.2f}   ,  {:.2f}   , {:.2f}(+/- {:.2f})\".format(scores_dt_clf['test_accuracy'].mean()*100.0,scores_dt_clf['test_precision_macro'].mean()*100.0,scores_dt_clf['test_recall_macro'].mean()*100.0,scores_dt_clf['test_f1_macro'].mean()*100.0, scores_lr_clf['test_f1_macro'].std() * 2), flush=True)\n",
    "    print(\"Random Forest       ,{:.2f}   ,   {:.2f}   ,  {:.2f}   , {:.2f}(+/- {:.2f})\".format(scores_rf_clf['test_accuracy'].mean()*100.0,scores_rf_clf['test_precision_macro'].mean()*100.0,scores_rf_clf['test_recall_macro'].mean()*100.0,scores_rf_clf['test_f1_macro'].mean()*100.0, scores_lr_clf['test_f1_macro'].std() * 2), flush=True)\n",
    "    print(\"Adaboost            ,{:.2f}   ,   {:.2f}   ,  {:.2f}   , {:.2f}(+/- {:.2f})\".format(scores_ab_clf['test_accuracy'].mean()*100.0,scores_ab_clf['test_precision_macro'].mean()*100.0,scores_ab_clf['test_recall_macro'].mean()*100.0,scores_ab_clf['test_f1_macro'].mean()*100.0, scores_lr_clf['test_f1_macro'].std() * 2), flush=True)\n",
    "    print(\"NaiveBayes          ,{:.2f}   ,   {:.2f}   ,  {:.2f}   , {:.2f}(+/- {:.2f})\".format(scores_nb_clf['test_accuracy'].mean()*100.0,scores_nb_clf['test_precision_macro'].mean()*100.0,scores_nb_clf['test_recall_macro'].mean()*100.0,scores_nb_clf['test_f1_macro'].mean()*100.0, scores_lr_clf['test_f1_macro'].std() * 2), flush=True)\n",
    "    print(\"MLP                 ,{:.2f}   ,   {:.2f}   ,  {:.2f}   , {:.2f}(+/- {:.2f})\".format(scores_nn_clf['test_accuracy'].mean()*100.0,scores_nn_clf['test_precision_macro'].mean()*100.0,scores_nn_clf['test_recall_macro'].mean()*100.0,scores_nn_clf['test_f1_macro'].mean()*100.0, scores_lr_clf['test_f1_macro'].std() * 2), flush=True)\n",
    "    print(\"SVM                 ,{:.2f}   ,   {:.2f}   ,  {:.2f}   , {:.2f}(+/- {:.2f})\".format(scores_svm_clf['test_accuracy'].mean()*100.0,scores_svm_clf['test_precision_macro'].mean()*100.0,scores_svm_clf['test_recall_macro'].mean()*100.0,scores_svm_clf['test_f1_macro'].mean()*100.0, scores_lr_clf['test_f1_macro'].std() * 2), flush=True)\n",
    "    print()\n",
    "    #@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@#\n",
    "    ### SUBSET ###\n",
    "    #@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@#\n",
    "    if data == 'covid':\n",
    "        dataset = FullDataset('/home/joao/covid.subset.tsv','covid')\n",
    "    if data ==  'crisislext6':\n",
    "        dataset = FullDataset('/home/joao/crisislext6.subset.tsv','crisislext6')\n",
    "    if data == 'crisislext26':\n",
    "        dataset = FullDataset('/home/joao/crisislext26.subset.tsv','crisislext26')\n",
    "    if data == 'crisismmd':\n",
    "        dataset = FullDataset('/home/joao/crisismmd.subset.tsv','crisismmd')\n",
    "    dataloader = DataLoader(dataset,sampler = SequentialSampler(dataset), batch_size = batch_size )\n",
    "    # Model 1.  \n",
    "    # Load pretrained model/tokenizer\n",
    "    model_class, tokenizer_class, pretrained_weights = (ppb.BertModel, ppb.BertTokenizer, 'bert-base-uncased')\n",
    "    model = model_class.from_pretrained(pretrained_weights)\n",
    "    model.to(device)\n",
    "     # For each batch of training data...\n",
    "    _bertLabels = pd.DataFrame()    # dataframe with the Labels Features only \n",
    "    _bertFeatures = pd.DataFrame()  # dataframe with the Bert features only\n",
    "    for batch in dataloader:\n",
    "        with torch.no_grad():\n",
    "            b_input_ids, b_input_mask, hand_features, b_labels = tuple(t.to(device) for t in batch)\n",
    "            last_hidden_states = model(b_input_ids,attention_mask = b_input_mask)\n",
    "            bertfeatures = last_hidden_states[0][:,0,:]#Let's slice only the part of the output that we need. That is the output corresponding the first token of each sentence.  The way BERT does sentence classification, is that it adds a token called [CLS] (for classification) at the beginning of every sentence. The output corresponding to that token can be thought of as an embedding for the entire sentence.\n",
    "            bertfeatures = bertfeatures.cpu().detach().numpy()        \n",
    "            labels = b_labels.cpu().detach().numpy()\n",
    "            _bertLabels = _bertLabels.append(pd.DataFrame(labels),ignore_index = True)\n",
    "            _bertFeatures = _bertFeatures.append(pd.DataFrame(bertfeatures),ignore_index = True)\n",
    "    # Model 2.  \n",
    "    # The output from BERT is going to be input to SKLEARN models\n",
    "    X_train = _bertFeatures\n",
    "    y_train = _bertLabels\n",
    "    lr_clf = LogisticRegression()\n",
    "    dt_clf = DecisionTreeClassifier()\n",
    "    rf_clf = RandomForestClassifier()\n",
    "    ab_clf = AdaBoostClassifier(n_estimators=100, random_state=0)\n",
    "    nb_clf = GaussianNB()\n",
    "    nn_clf = MLPClassifier(random_state=1, max_iter=300)\n",
    "    svm_clf = svm.SVC(gamma=0.001, C=100.)\n",
    "    scoring = ['accuracy','precision_macro', 'recall_macro', 'f1_macro']\n",
    "    print(\" *** Subset dataset BERT encoded features *** \")\n",
    "    scores_lr_clf = cross_validate( lr_clf, X_train, y_train, cv=10, scoring=scoring, return_train_score=False)\n",
    "    scores_dt_clf = cross_validate( dt_clf, X_train, y_train, cv=10, scoring=scoring, return_train_score=False)\n",
    "    scores_rf_clf = cross_validate( rf_clf, X_train, y_train, cv=10, scoring=scoring, return_train_score=False)\n",
    "    scores_ab_clf = cross_validate( ab_clf, X_train, y_train, cv=10, scoring=scoring, return_train_score=False)\n",
    "    scores_nb_clf = cross_validate( nb_clf, X_train, y_train, cv=10, scoring=scoring, return_train_score=False)\n",
    "    scores_nn_clf = cross_validate( nn_clf, X_train, y_train, cv=10, scoring=scoring, return_train_score=False)\n",
    "    scores_svm_clf = cross_validate( svm_clf, X_train, y_train, cv=10, scoring=scoring, return_train_score=False)\n",
    "    print(\"MODELS              ,Accuracy , Precision  ,  Recall   ,  F-score          \", flush=True)\n",
    "    print(\"Logistic Regression ,{:.2f}   ,   {:.2f}   ,  {:.2f}   , {:.2f}(+/- {:.2f})\".format(scores_lr_clf['test_accuracy'].mean()*100.0,scores_lr_clf['test_precision_macro'].mean()*100.0,scores_lr_clf['test_recall_macro'].mean()*100.0,scores_lr_clf['test_f1_macro'].mean()*100.0, scores_lr_clf['test_f1_macro'].std() * 2), flush=True)\n",
    "    print(\"Decision Tree       ,{:.2f}   ,   {:.2f}   ,  {:.2f}   , {:.2f}(+/- {:.2f})\".format(scores_dt_clf['test_accuracy'].mean()*100.0,scores_dt_clf['test_precision_macro'].mean()*100.0,scores_dt_clf['test_recall_macro'].mean()*100.0,scores_dt_clf['test_f1_macro'].mean()*100.0, scores_lr_clf['test_f1_macro'].std() * 2), flush=True)\n",
    "    print(\"Random Forest       ,{:.2f}   ,   {:.2f}   ,  {:.2f}   , {:.2f}(+/- {:.2f})\".format(scores_rf_clf['test_accuracy'].mean()*100.0,scores_rf_clf['test_precision_macro'].mean()*100.0,scores_rf_clf['test_recall_macro'].mean()*100.0,scores_rf_clf['test_f1_macro'].mean()*100.0, scores_lr_clf['test_f1_macro'].std() * 2), flush=True)\n",
    "    print(\"Adaboost            ,{:.2f}   ,   {:.2f}   ,  {:.2f}   , {:.2f}(+/- {:.2f})\".format(scores_ab_clf['test_accuracy'].mean()*100.0,scores_ab_clf['test_precision_macro'].mean()*100.0,scores_ab_clf['test_recall_macro'].mean()*100.0,scores_ab_clf['test_f1_macro'].mean()*100.0, scores_lr_clf['test_f1_macro'].std() * 2), flush=True)\n",
    "    print(\"NaiveBayes          ,{:.2f}   ,   {:.2f}   ,  {:.2f}   , {:.2f}(+/- {:.2f})\".format(scores_nb_clf['test_accuracy'].mean()*100.0,scores_nb_clf['test_precision_macro'].mean()*100.0,scores_nb_clf['test_recall_macro'].mean()*100.0,scores_nb_clf['test_f1_macro'].mean()*100.0, scores_lr_clf['test_f1_macro'].std() * 2), flush=True)\n",
    "    print(\"MLP                 ,{:.2f}   ,   {:.2f}   ,  {:.2f}   , {:.2f}(+/- {:.2f})\".format(scores_nn_clf['test_accuracy'].mean()*100.0,scores_nn_clf['test_precision_macro'].mean()*100.0,scores_nn_clf['test_recall_macro'].mean()*100.0,scores_nn_clf['test_f1_macro'].mean()*100.0, scores_lr_clf['test_f1_macro'].std() * 2), flush=True)\n",
    "    print(\"SVM                 ,{:.2f}   ,   {:.2f}   ,  {:.2f}   , {:.2f}(+/- {:.2f})\".format(scores_svm_clf['test_accuracy'].mean()*100.0,scores_svm_clf['test_precision_macro'].mean()*100.0,scores_svm_clf['test_recall_macro'].mean()*100.0,scores_svm_clf['test_f1_macro'].mean()*100.0, scores_lr_clf['test_f1_macro'].std() * 2), flush=True)\n",
    "    print()    \n",
    "\n",
    "#Doc2Vec model \n",
    "def doc2vec_evaluate(fullsetdf,subsetdf):\n",
    "    size = 300\n",
    "    window = 5\n",
    "    min_count = 1\n",
    "    workers = 4\n",
    "    sg = 1\n",
    "    df = preprocess(fullsetdf)\n",
    "    df.drop(df.columns[[0]], axis=1, inplace=True)\n",
    "    # Tokenize the text column to get the new column 'tokenized_text'\n",
    "    df['tokenized_text'] = [simple_preprocess(line, deacc=True) for line in df['sentence']]\n",
    "    documents = [TaggedDocument(doc, [i]) for i, doc in enumerate(df['tokenized_text'])]\n",
    "    #Initialize the model\n",
    "    doc2vec_model = Doc2Vec(documents, vector_size=size, window=window, min_count=min_count, workers=workers)\n",
    "    for index, row in df.iterrows():\n",
    "        model_vector = doc2vec_model.infer_vector(row['tokenized_text'])\n",
    "        if index == 0:\n",
    "            header = \",\".join(str(ele) for ele in range(size))\n",
    "            header = header.split(',')\n",
    "            doc2vec_df = pd.DataFrame([], columns = header)\n",
    "        #if type(model_vector) is list:  \n",
    "        line1 = \",\".join( [str(vector_element) for vector_element in model_vector] )\n",
    "        line1 = line1.split(',')\n",
    "        #else:\n",
    "        #    line1 = \",\".join([str(0) for i in range(size)])\n",
    "        a_series = pd.Series(line1, index = doc2vec_df.columns)\n",
    "        doc2vec_df =  doc2vec_df.append(a_series,ignore_index=True)\n",
    "    y_train = pd.DataFrame(fullsetdf['label'])\n",
    "    X_train = doc2vec_df\n",
    "    lr_clf = LogisticRegression()\n",
    "    dt_clf = DecisionTreeClassifier()\n",
    "    rf_clf = RandomForestClassifier()\n",
    "    #ab_clf = AdaBoostClassifier(n_estimators=100, random_state=0)\n",
    "    nb_clf = GaussianNB()\n",
    "    nn_clf = MLPClassifier(random_state=1, max_iter=300)\n",
    "    svm_clf = svm.SVC(gamma=0.001, C=100.)\n",
    "    scoring = ['accuracy','precision_macro', 'recall_macro', 'f1_macro']\n",
    "    print(\" *** Full dataset doc2vec features *** \")\n",
    "    scores_lr_clf = cross_validate( lr_clf, X_train, y_train, cv=10, scoring=scoring, return_train_score=False)\n",
    "    scores_dt_clf = cross_validate( dt_clf, X_train, y_train, cv=10, scoring=scoring, return_train_score=False)\n",
    "    scores_rf_clf = cross_validate( rf_clf, X_train, y_train, cv=10, scoring=scoring, return_train_score=False)\n",
    "    #scores_ab_clf = cross_validate( ab_clf, X_train, y_train, cv=10, scoring=scoring, return_train_score=False)\n",
    "    scores_nb_clf = cross_validate( nb_clf, X_train, y_train, cv=10, scoring=scoring, return_train_score=False)\n",
    "    scores_nn_clf = cross_validate( nn_clf, X_train, y_train, cv=10, scoring=scoring, return_train_score=False)\n",
    "    scores_svm_clf = cross_validate( svm_clf, X_train, y_train, cv=10, scoring=scoring, return_train_score=False)\n",
    "    print(\"MODELS              ,Accuracy , Precision  ,  Recall   ,  F-score          \", flush=True)\n",
    "    print(\"Logistic Regression ,{:.2f}   ,   {:.2f}   ,  {:.2f}   , {:.2f}(+/- {:.2f})\".format(scores_lr_clf['test_accuracy'].mean()*100.0,scores_lr_clf['test_precision_macro'].mean()*100.0,scores_lr_clf['test_recall_macro'].mean()*100.0,scores_lr_clf['test_f1_macro'].mean()*100.0, scores_lr_clf['test_f1_macro'].std() * 2), flush=True)\n",
    "    print(\"Decision Tree       ,{:.2f}   ,   {:.2f}   ,  {:.2f}   , {:.2f}(+/- {:.2f})\".format(scores_dt_clf['test_accuracy'].mean()*100.0,scores_dt_clf['test_precision_macro'].mean()*100.0,scores_dt_clf['test_recall_macro'].mean()*100.0,scores_dt_clf['test_f1_macro'].mean()*100.0, scores_lr_clf['test_f1_macro'].std() * 2), flush=True)\n",
    "    print(\"Random Forest       ,{:.2f}   ,   {:.2f}   ,  {:.2f}   , {:.2f}(+/- {:.2f})\".format(scores_rf_clf['test_accuracy'].mean()*100.0,scores_rf_clf['test_precision_macro'].mean()*100.0,scores_rf_clf['test_recall_macro'].mean()*100.0,scores_rf_clf['test_f1_macro'].mean()*100.0, scores_lr_clf['test_f1_macro'].std() * 2), flush=True)\n",
    "    #print(\"Adaboost            ,{:.2f}   ,   {:.2f}   ,  {:.2f}   , {:.2f}(+/- {:.2f})\".format(scores_ab_clf['test_accuracy'].mean()*100.0,scores_ab_clf['test_precision_macro'].mean()*100.0,scores_ab_clf['test_recall_macro'].mean()*100.0,scores_ab_clf['test_f1_macro'].mean()*100.0, scores_lr_clf['test_f1_macro'].std() * 2), flush=True)\n",
    "    print(\"NaiveBayes          ,{:.2f}   ,   {:.2f}   ,  {:.2f}   , {:.2f}(+/- {:.2f})\".format(scores_nb_clf['test_accuracy'].mean()*100.0,scores_nb_clf['test_precision_macro'].mean()*100.0,scores_nb_clf['test_recall_macro'].mean()*100.0,scores_nb_clf['test_f1_macro'].mean()*100.0, scores_lr_clf['test_f1_macro'].std() * 2), flush=True)\n",
    "    print(\"MLP                 ,{:.2f}   ,   {:.2f}   ,  {:.2f}   , {:.2f}(+/- {:.2f})\".format(scores_nn_clf['test_accuracy'].mean()*100.0,scores_nn_clf['test_precision_macro'].mean()*100.0,scores_nn_clf['test_recall_macro'].mean()*100.0,scores_nn_clf['test_f1_macro'].mean()*100.0, scores_lr_clf['test_f1_macro'].std() * 2), flush=True)\n",
    "    print(\"SVM                 ,{:.2f}   ,   {:.2f}   ,  {:.2f}   , {:.2f}(+/- {:.2f})\".format(scores_svm_clf['test_accuracy'].mean()*100.0,scores_svm_clf['test_precision_macro'].mean()*100.0,scores_svm_clf['test_recall_macro'].mean()*100.0,scores_svm_clf['test_f1_macro'].mean()*100.0, scores_lr_clf['test_f1_macro'].std() * 2), flush=True)\n",
    "    print()\n",
    "    #@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@#\n",
    "    ### SUBSET ###\n",
    "    #@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@#\n",
    "    df = preprocess(subsetdf)\n",
    "    df.drop(df.columns[[0]], axis=1, inplace=True)\n",
    "    # Tokenize the text column to get the new column 'tokenized_text'\n",
    "    df['tokenized_text'] = [simple_preprocess(line, deacc=True) for line in df['sentence']]\n",
    "    documents = [TaggedDocument(doc, [i]) for i, doc in enumerate(df['tokenized_text'])]\n",
    "    #Initialize the model\n",
    "    doc2vec_model = Doc2Vec(documents, vector_size=size, window=window, min_count=min_count, workers=workers)\n",
    "    for index, row in df.iterrows():\n",
    "        model_vector = doc2vec_model.infer_vector(row['tokenized_text'])\n",
    "        if index == 0:\n",
    "            header = \",\".join(str(ele) for ele in range(size))\n",
    "            header = header.split(',')\n",
    "            doc2vec_df = pd.DataFrame([], columns = header)\n",
    "            # Check if the line exists else it is vector of zeros\n",
    "        #if type(model_vector) is list:  \n",
    "        line1 = \",\".join( [str(vector_element) for vector_element in model_vector] )\n",
    "        #else:\n",
    "        #    line1 = \",\".join([str(0) for i in range(size)])\n",
    "        line1 = line1.split(',')\n",
    "        a_series = pd.Series(line1, index = doc2vec_df.columns)\n",
    "        doc2vec_df =  doc2vec_df.append(a_series,ignore_index=True)\n",
    "    y_train = pd.DataFrame(subsetdf['label'])\n",
    "    X_train = doc2vec_df\n",
    "    lr_clf = LogisticRegression()\n",
    "    dt_clf = DecisionTreeClassifier()\n",
    "    rf_clf = RandomForestClassifier()\n",
    "    #ab_clf = AdaBoostClassifier(n_estimators=100, random_state=0)\n",
    "    nb_clf = GaussianNB()\n",
    "    nn_clf = MLPClassifier(random_state=1, max_iter=300)\n",
    "    svm_clf = svm.SVC(gamma=0.001, C=100.)\n",
    "    scoring = ['accuracy','precision_macro', 'recall_macro', 'f1_macro']\n",
    "    print(\" *** Subset doc2vec features *** \")\n",
    "    scores_lr_clf = cross_validate( lr_clf, X_train, y_train, cv=10, scoring=scoring, return_train_score=False)\n",
    "    scores_dt_clf = cross_validate( dt_clf, X_train, y_train, cv=10, scoring=scoring, return_train_score=False)\n",
    "    scores_rf_clf = cross_validate( rf_clf, X_train, y_train, cv=10, scoring=scoring, return_train_score=False)\n",
    "    #scores_ab_clf = cross_validate( ab_clf, X_train, y_train, cv=10, scoring=scoring, return_train_score=False)\n",
    "    scores_nb_clf = cross_validate( nb_clf, X_train, y_train, cv=10, scoring=scoring, return_train_score=False)\n",
    "    scores_nn_clf = cross_validate( nn_clf, X_train, y_train, cv=10, scoring=scoring, return_train_score=False)\n",
    "    scores_svm_clf = cross_validate( svm_clf, X_train, y_train, cv=10, scoring=scoring, return_train_score=False)\n",
    "    print(\"MODELS              ,Accuracy , Precision  ,  Recall   ,  F-score          \", flush=True)\n",
    "    print(\"Logistic Regression ,{:.2f}   ,   {:.2f}   ,  {:.2f}   , {:.2f}(+/- {:.2f})\".format(scores_lr_clf['test_accuracy'].mean()*100.0,scores_lr_clf['test_precision_macro'].mean()*100.0,scores_lr_clf['test_recall_macro'].mean()*100.0,scores_lr_clf['test_f1_macro'].mean()*100.0, scores_lr_clf['test_f1_macro'].std() * 2), flush=True)\n",
    "    print(\"Decision Tree       ,{:.2f}   ,   {:.2f}   ,  {:.2f}   , {:.2f}(+/- {:.2f})\".format(scores_dt_clf['test_accuracy'].mean()*100.0,scores_dt_clf['test_precision_macro'].mean()*100.0,scores_dt_clf['test_recall_macro'].mean()*100.0,scores_dt_clf['test_f1_macro'].mean()*100.0, scores_lr_clf['test_f1_macro'].std() * 2), flush=True)\n",
    "    print(\"Random Forest       ,{:.2f}   ,   {:.2f}   ,  {:.2f}   , {:.2f}(+/- {:.2f})\".format(scores_rf_clf['test_accuracy'].mean()*100.0,scores_rf_clf['test_precision_macro'].mean()*100.0,scores_rf_clf['test_recall_macro'].mean()*100.0,scores_rf_clf['test_f1_macro'].mean()*100.0, scores_lr_clf['test_f1_macro'].std() * 2), flush=True)\n",
    "    #print(\"Adaboost            ,{:.2f}   ,   {:.2f}   ,  {:.2f}   , {:.2f}(+/- {:.2f})\".format(scores_ab_clf['test_accuracy'].mean()*100.0,scores_ab_clf['test_precision_macro'].mean()*100.0,scores_ab_clf['test_recall_macro'].mean()*100.0,scores_ab_clf['test_f1_macro'].mean()*100.0, scores_lr_clf['test_f1_macro'].std() * 2), flush=True)\n",
    "    print(\"NaiveBayes          ,{:.2f}   ,   {:.2f}   ,  {:.2f}   , {:.2f}(+/- {:.2f})\".format(scores_nb_clf['test_accuracy'].mean()*100.0,scores_nb_clf['test_precision_macro'].mean()*100.0,scores_nb_clf['test_recall_macro'].mean()*100.0,scores_nb_clf['test_f1_macro'].mean()*100.0, scores_lr_clf['test_f1_macro'].std() * 2), flush=True)\n",
    "    print(\"MLP                 ,{:.2f}   ,   {:.2f}   ,  {:.2f}   , {:.2f}(+/- {:.2f})\".format(scores_nn_clf['test_accuracy'].mean()*100.0,scores_nn_clf['test_precision_macro'].mean()*100.0,scores_nn_clf['test_recall_macro'].mean()*100.0,scores_nn_clf['test_f1_macro'].mean()*100.0, scores_lr_clf['test_f1_macro'].std() * 2), flush=True)\n",
    "    print(\"SVM                 ,{:.2f}   ,   {:.2f}   ,  {:.2f}   , {:.2f}(+/- {:.2f})\".format(scores_svm_clf['test_accuracy'].mean()*100.0,scores_svm_clf['test_precision_macro'].mean()*100.0,scores_svm_clf['test_recall_macro'].mean()*100.0,scores_svm_clf['test_f1_macro'].mean()*100.0, scores_lr_clf['test_f1_macro'].std() * 2), flush=True)\n",
    "    print()\n",
    "\n",
    "def word2vec_evaluate(fullsetdf,subsetdf):\n",
    "    size = 300\n",
    "    window = 5\n",
    "    min_count = 1\n",
    "    workers = 4\n",
    "    sg = 1\n",
    "    df = preprocess(fullsetdf)\n",
    "    df.drop(df.columns[[0]], axis=1, inplace=True)\n",
    "    # Tokenize the text column to get the new column 'tokenized_text'\n",
    "    df['tokenized_text'] = [simple_preprocess(line, deacc=True) for line in df['sentence']]\n",
    "    w2v_model = Word2Vec(df['tokenized_text'].values, min_count = min_count, size = size, workers = workers, window = window, sg = sg)\n",
    "    for index, row in df.iterrows():\n",
    "        model_vector = (np.mean([w2v_model[token] for token in row['tokenized_text']], axis=0)).tolist()\n",
    "        if index == 0:\n",
    "            header = \",\".join(str(ele) for ele in range(size))\n",
    "            header = header.split(\",\")\n",
    "            word2Vec_df = pd.DataFrame([], columns = header)\n",
    "           # Check if the line exists else it is vector of zeros\n",
    "        if type(model_vector) is list:  \n",
    "            line1 = \",\".join( [str(vector_element) for vector_element in model_vector] )\n",
    "    #else:\n",
    "       # line1 = \",\".join([str(0) for i in range(size)])\n",
    "#line1 = line1.split(',')\n",
    "    a_series = pd.Series(line1, index = word2Vec_df.columns)\n",
    "    word2Vec_df =  word2Vec_df.append(a_series,ignore_index=True)\n",
    "    y_train = pd.DataFrame(fullsetdf['label'])\n",
    "    X_train = word2Vec_df\n",
    "    lr_clf = LogisticRegression()\n",
    "    dt_clf = DecisionTreeClassifier()\n",
    "    rf_clf = RandomForestClassifier()\n",
    "    ab_clf = AdaBoostClassifier(n_estimators=100, random_state=0)\n",
    "    nb_clf = GaussianNB()\n",
    "    nn_clf = MLPClassifier(random_state=1, max_iter=300)\n",
    "    svm_clf = svm.SVC(gamma=0.001, C=100.)\n",
    "    scoring = ['accuracy','precision_macro', 'recall_macro', 'f1_macro']\n",
    "    print(\" *** Full dataset word2vec features *** \")\n",
    "    scores_lr_clf = cross_validate( lr_clf, X_train, y_train, cv=10, scoring=scoring, return_train_score=False)\n",
    "    scores_dt_clf = cross_validate( dt_clf, X_train, y_train, cv=10, scoring=scoring, return_train_score=False)\n",
    "    scores_rf_clf = cross_validate( rf_clf, X_train, y_train, cv=10, scoring=scoring, return_train_score=False)\n",
    "    scores_ab_clf = cross_validate( ab_clf, X_train, y_train, cv=10, scoring=scoring, return_train_score=False)\n",
    "    scores_nb_clf = cross_validate( nb_clf, X_train, y_train, cv=10, scoring=scoring, return_train_score=False)\n",
    "    scores_nn_clf = cross_validate( nn_clf, X_train, y_train, cv=10, scoring=scoring, return_train_score=False)\n",
    "    scores_svm_clf = cross_validate( svm_clf, X_train, y_train, cv=10, scoring=scoring, return_train_score=False)\n",
    "    print(\"MODELS              ,Accuracy , Precision  ,  Recall   ,  F-score          \", flush=True)\n",
    "    print(\"Logistic Regression ,{:.2f}   ,   {:.2f}   ,  {:.2f}   , {:.2f}(+/- {:.2f})\".format(scores_lr_clf['test_accuracy'].mean()*100.0,scores_lr_clf['test_precision_macro'].mean()*100.0,scores_lr_clf['test_recall_macro'].mean()*100.0,scores_lr_clf['test_f1_macro'].mean()*100.0, scores_lr_clf['test_f1_macro'].std() * 2), flush=True)\n",
    "    print(\"Decision Tree       ,{:.2f}   ,   {:.2f}   ,  {:.2f}   , {:.2f}(+/- {:.2f})\".format(scores_dt_clf['test_accuracy'].mean()*100.0,scores_dt_clf['test_precision_macro'].mean()*100.0,scores_dt_clf['test_recall_macro'].mean()*100.0,scores_dt_clf['test_f1_macro'].mean()*100.0, scores_lr_clf['test_f1_macro'].std() * 2), flush=True)\n",
    "    print(\"Random Forest       ,{:.2f}   ,   {:.2f}   ,  {:.2f}   , {:.2f}(+/- {:.2f})\".format(scores_rf_clf['test_accuracy'].mean()*100.0,scores_rf_clf['test_precision_macro'].mean()*100.0,scores_rf_clf['test_recall_macro'].mean()*100.0,scores_rf_clf['test_f1_macro'].mean()*100.0, scores_lr_clf['test_f1_macro'].std() * 2), flush=True)\n",
    "    print(\"Adaboost            ,{:.2f}   ,   {:.2f}   ,  {:.2f}   , {:.2f}(+/- {:.2f})\".format(scores_ab_clf['test_accuracy'].mean()*100.0,scores_ab_clf['test_precision_macro'].mean()*100.0,scores_ab_clf['test_recall_macro'].mean()*100.0,scores_ab_clf['test_f1_macro'].mean()*100.0, scores_lr_clf['test_f1_macro'].std() * 2), flush=True)\n",
    "    print(\"NaiveBayes          ,{:.2f}   ,   {:.2f}   ,  {:.2f}   , {:.2f}(+/- {:.2f})\".format(scores_nb_clf['test_accuracy'].mean()*100.0,scores_nb_clf['test_precision_macro'].mean()*100.0,scores_nb_clf['test_recall_macro'].mean()*100.0,scores_nb_clf['test_f1_macro'].mean()*100.0, scores_lr_clf['test_f1_macro'].std() * 2), flush=True)\n",
    "    print(\"MLP                 ,{:.2f}   ,   {:.2f}   ,  {:.2f}   , {:.2f}(+/- {:.2f})\".format(scores_nn_clf['test_accuracy'].mean()*100.0,scores_nn_clf['test_precision_macro'].mean()*100.0,scores_nn_clf['test_recall_macro'].mean()*100.0,scores_nn_clf['test_f1_macro'].mean()*100.0, scores_lr_clf['test_f1_macro'].std() * 2), flush=True)\n",
    "    print(\"SVM                 ,{:.2f}   ,   {:.2f}   ,  {:.2f}   , {:.2f}(+/- {:.2f})\".format(scores_svm_clf['test_accuracy'].mean()*100.0,scores_svm_clf['test_precision_macro'].mean()*100.0,scores_svm_clf['test_recall_macro'].mean()*100.0,scores_svm_clf['test_f1_macro'].mean()*100.0, scores_lr_clf['test_f1_macro'].std() * 2), flush=True)\n",
    "    print()\n",
    "    #@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@#\n",
    "    ### SUBSET ###\n",
    "    #@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@#\n",
    "    df = preprocess(subsetdf)\n",
    "    df.drop(df.columns[[0]], axis=1, inplace=True)\n",
    "    # Tokenize the text column to get the new column 'tokenized_text'\n",
    "    df['tokenized_text'] = [simple_preprocess(line, deacc=True) for line in df['sentence']]\n",
    "    w2v_model = Word2Vec(df['tokenized_text'].values, min_count = min_count, size = size, workers = workers, window = window, sg = sg)\n",
    "    for index, row in df.iterrows():\n",
    "        model_vector = (np.mean([w2v_model[token] for token in row['tokenized_text']], axis=0)).tolist()\n",
    "        if index == 0:\n",
    "            header = \",\".join(str(ele) for ele in range(1000))\n",
    "            header = header.split(\",\")\n",
    "            word2Vec_df = pd.DataFrame([], columns = header)\n",
    "            # Check if the line exists else it is vector of zeros\n",
    "        if type(model_vector) is list:  \n",
    "            line1 = \",\".join( [str(vector_element) for vector_element in model_vector] )\n",
    "        #else:\n",
    "        #    line1 = \",\".join([str(0) for i in range(1000)])\n",
    "        #line1 = line1.split(',')\n",
    "        a_series = pd.Series(line1, index = word2Vec_df.columns)\n",
    "        word2Vec_df =  word2Vec_df.append(a_series,ignore_index=True)\n",
    "    y_train = pd.DataFrame(subsetdf['label'])\n",
    "    X_pretrain = word2Vec_df\n",
    "    lr_clf = LogisticRegression()\n",
    "    dt_clf = DecisionTreeClassifier()\n",
    "    rf_clf = RandomForestClassifier()\n",
    "    ab_clf = AdaBoostClassifier(n_estimators=100, random_state=0)\n",
    "    nb_clf = GaussianNB()\n",
    "    nn_clf = MLPClassifier(random_state=1, max_iter=300)\n",
    "    svm_clf = svm.SVC(gamma=0.001, C=100.)\n",
    "    scoring = ['accuracy','precision_macro', 'recall_macro', 'f1_macro']\n",
    "    print(\" *** Subset word2vec features *** \")\n",
    "    scores_lr_clf = cross_validate( lr_clf, X_train, y_train, cv=10, scoring=scoring, return_train_score=False)\n",
    "    scores_dt_clf = cross_validate( dt_clf, X_train, y_train, cv=10, scoring=scoring, return_train_score=False)\n",
    "    scores_rf_clf = cross_validate( rf_clf, X_train, y_train, cv=10, scoring=scoring, return_train_score=False)\n",
    "    scores_ab_clf = cross_validate( ab_clf, X_train, y_train, cv=10, scoring=scoring, return_train_score=False)\n",
    "    scores_nb_clf = cross_validate( nb_clf, X_train, y_train, cv=10, scoring=scoring, return_train_score=False)\n",
    "    scores_nn_clf = cross_validate( nn_clf, X_train, y_train, cv=10, scoring=scoring, return_train_score=False)\n",
    "    scores_svm_clf = cross_validate( svm_clf, X_train, y_train, cv=10, scoring=scoring, return_train_score=False)\n",
    "    print(\"MODELS              ,Accuracy , Precision  ,  Recall   ,  F-score          \", flush=True)\n",
    "    print(\"Logistic Regression ,{:.2f}   ,   {:.2f}   ,  {:.2f}   , {:.2f}(+/- {:.2f})\".format(scores_lr_clf['test_accuracy'].mean()*100.0,scores_lr_clf['test_precision_macro'].mean()*100.0,scores_lr_clf['test_recall_macro'].mean()*100.0,scores_lr_clf['test_f1_macro'].mean()*100.0, scores_lr_clf['test_f1_macro'].std() * 2), flush=True)\n",
    "    print(\"Decision Tree       ,{:.2f}   ,   {:.2f}   ,  {:.2f}   , {:.2f}(+/- {:.2f})\".format(scores_dt_clf['test_accuracy'].mean()*100.0,scores_dt_clf['test_precision_macro'].mean()*100.0,scores_dt_clf['test_recall_macro'].mean()*100.0,scores_dt_clf['test_f1_macro'].mean()*100.0, scores_lr_clf['test_f1_macro'].std() * 2), flush=True)\n",
    "    print(\"Random Forest       ,{:.2f}   ,   {:.2f}   ,  {:.2f}   , {:.2f}(+/- {:.2f})\".format(scores_rf_clf['test_accuracy'].mean()*100.0,scores_rf_clf['test_precision_macro'].mean()*100.0,scores_rf_clf['test_recall_macro'].mean()*100.0,scores_rf_clf['test_f1_macro'].mean()*100.0, scores_lr_clf['test_f1_macro'].std() * 2), flush=True)\n",
    "    print(\"Adaboost            ,{:.2f}   ,   {:.2f}   ,  {:.2f}   , {:.2f}(+/- {:.2f})\".format(scores_ab_clf['test_accuracy'].mean()*100.0,scores_ab_clf['test_precision_macro'].mean()*100.0,scores_ab_clf['test_recall_macro'].mean()*100.0,scores_ab_clf['test_f1_macro'].mean()*100.0, scores_lr_clf['test_f1_macro'].std() * 2), flush=True)\n",
    "    print(\"NaiveBayes          ,{:.2f}   ,   {:.2f}   ,  {:.2f}   , {:.2f}(+/- {:.2f})\".format(scores_nb_clf['test_accuracy'].mean()*100.0,scores_nb_clf['test_precision_macro'].mean()*100.0,scores_nb_clf['test_recall_macro'].mean()*100.0,scores_nb_clf['test_f1_macro'].mean()*100.0, scores_lr_clf['test_f1_macro'].std() * 2), flush=True)\n",
    "    print(\"MLP                 ,{:.2f}   ,   {:.2f}   ,  {:.2f}   , {:.2f}(+/- {:.2f})\".format(scores_nn_clf['test_accuracy'].mean()*100.0,scores_nn_clf['test_precision_macro'].mean()*100.0,scores_nn_clf['test_recall_macro'].mean()*100.0,scores_nn_clf['test_f1_macro'].mean()*100.0, scores_lr_clf['test_f1_macro'].std() * 2), flush=True)\n",
    "    print(\"SVM                 ,{:.2f}   ,   {:.2f}   ,  {:.2f}   , {:.2f}(+/- {:.2f})\".format(scores_svm_clf['test_accuracy'].mean()*100.0,scores_svm_clf['test_precision_macro'].mean()*100.0,scores_svm_clf['test_recall_macro'].mean()*100.0,scores_svm_clf['test_f1_macro'].mean()*100.0, scores_lr_clf['test_f1_macro'].std() * 2), flush=True)\n",
    "    print()\n",
    "\n",
    "\n",
    "    \n",
    "def handcrafted_features_evaluate(fullsetdf,subsetdf):\n",
    "    X_train = fullsetdf[['nchars','nwords','bhash','nhash','blink','nlink','bat','nat','brt','bslang','bintj','tlex']]\n",
    "    y_train = fullsetdf['label']\n",
    "    lr_clf = LogisticRegression()\n",
    "    dt_clf = DecisionTreeClassifier()\n",
    "    rf_clf = RandomForestClassifier()\n",
    "    ab_clf = AdaBoostClassifier(n_estimators=100, random_state=0)\n",
    "    nb_clf = GaussianNB()\n",
    "    nn_clf = MLPClassifier(random_state=1, max_iter=300)\n",
    "    svm_clf = svm.SVC(gamma=0.001, C=100.)\n",
    "    scoring = ['accuracy','precision_macro', 'recall_macro', 'f1_macro']\n",
    "    print(\" *** Full dataset handcrafted features *** \")\n",
    "    scores_lr_clf = cross_validate( lr_clf, X_train, y_train, cv=10, scoring=scoring, return_train_score=False)\n",
    "    scores_dt_clf = cross_validate( dt_clf, X_train, y_train, cv=10, scoring=scoring, return_train_score=False)\n",
    "    scores_rf_clf = cross_validate( rf_clf, X_train, y_train, cv=10, scoring=scoring, return_train_score=False)\n",
    "    scores_ab_clf = cross_validate( ab_clf, X_train, y_train, cv=10, scoring=scoring, return_train_score=False)\n",
    "    scores_nb_clf = cross_validate( nb_clf, X_train, y_train, cv=10, scoring=scoring, return_train_score=False)\n",
    "    scores_nn_clf = cross_validate( nn_clf, X_train, y_train, cv=10, scoring=scoring, return_train_score=False)\n",
    "    scores_svm_clf = cross_validate( svm_clf, X_train, y_train, cv=10, scoring=scoring, return_train_score=False)\n",
    "    print(\"MODELS              ,Accuracy , Precision  ,  Recall   ,  F-score          \", flush=True)\n",
    "    print(\"Logistic Regression ,{:.2f}   ,   {:.2f}   ,  {:.2f}   , {:.2f}(+/- {:.2f})\".format(scores_lr_clf['test_accuracy'].mean()*100.0,scores_lr_clf['test_precision_macro'].mean()*100.0,scores_lr_clf['test_recall_macro'].mean()*100.0,scores_lr_clf['test_f1_macro'].mean()*100.0, scores_lr_clf['test_f1_macro'].std() * 2), flush=True)\n",
    "    print(\"Decision Tree       ,{:.2f}   ,   {:.2f}   ,  {:.2f}   , {:.2f}(+/- {:.2f})\".format(scores_dt_clf['test_accuracy'].mean()*100.0,scores_dt_clf['test_precision_macro'].mean()*100.0,scores_dt_clf['test_recall_macro'].mean()*100.0,scores_dt_clf['test_f1_macro'].mean()*100.0, scores_lr_clf['test_f1_macro'].std() * 2), flush=True)\n",
    "    print(\"Random Forest       ,{:.2f}   ,   {:.2f}   ,  {:.2f}   , {:.2f}(+/- {:.2f})\".format(scores_rf_clf['test_accuracy'].mean()*100.0,scores_rf_clf['test_precision_macro'].mean()*100.0,scores_rf_clf['test_recall_macro'].mean()*100.0,scores_rf_clf['test_f1_macro'].mean()*100.0, scores_lr_clf['test_f1_macro'].std() * 2), flush=True)\n",
    "    print(\"Adaboost            ,{:.2f}   ,   {:.2f}   ,  {:.2f}   , {:.2f}(+/- {:.2f})\".format(scores_ab_clf['test_accuracy'].mean()*100.0,scores_ab_clf['test_precision_macro'].mean()*100.0,scores_ab_clf['test_recall_macro'].mean()*100.0,scores_ab_clf['test_f1_macro'].mean()*100.0, scores_lr_clf['test_f1_macro'].std() * 2), flush=True)\n",
    "    print(\"NaiveBayes          ,{:.2f}   ,   {:.2f}   ,  {:.2f}   , {:.2f}(+/- {:.2f})\".format(scores_nb_clf['test_accuracy'].mean()*100.0,scores_nb_clf['test_precision_macro'].mean()*100.0,scores_nb_clf['test_recall_macro'].mean()*100.0,scores_nb_clf['test_f1_macro'].mean()*100.0, scores_lr_clf['test_f1_macro'].std() * 2), flush=True)\n",
    "    print(\"MLP                 ,{:.2f}   ,   {:.2f}   ,  {:.2f}   , {:.2f}(+/- {:.2f})\".format(scores_nn_clf['test_accuracy'].mean()*100.0,scores_nn_clf['test_precision_macro'].mean()*100.0,scores_nn_clf['test_recall_macro'].mean()*100.0,scores_nn_clf['test_f1_macro'].mean()*100.0, scores_lr_clf['test_f1_macro'].std() * 2), flush=True)\n",
    "    print(\"SVM                 ,{:.2f}   ,   {:.2f}   ,  {:.2f}   , {:.2f}(+/- {:.2f})\".format(scores_svm_clf['test_accuracy'].mean()*100.0,scores_svm_clf['test_precision_macro'].mean()*100.0,scores_svm_clf['test_recall_macro'].mean()*100.0,scores_svm_clf['test_f1_macro'].mean()*100.0, scores_lr_clf['test_f1_macro'].std() * 2), flush=True)\n",
    "    print()\n",
    "    #@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@#\n",
    "    ### SUBSET ###\n",
    "    #@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@#\n",
    "    X_train = subsetdf[['nchars','nwords','bhash','nhash','blink','nlink','bat','nat','brt','bslang','bintj','tlex','usr_vrf', 'num_followers', 'num_friends', 'num_tweets']]   \n",
    "    y_train = subsetdf['label']\n",
    "    lr_clf = LogisticRegression()\n",
    "    dt_clf = DecisionTreeClassifier()\n",
    "    rf_clf = RandomForestClassifier()\n",
    "    ab_clf = AdaBoostClassifier(n_estimators=100, random_state=0)\n",
    "    nb_clf = GaussianNB()\n",
    "    nn_clf = MLPClassifier(random_state=1, max_iter=300)\n",
    "    svm_clf = svm.SVC(gamma=0.001, C=100.)\n",
    "    scoring = ['accuracy','precision_macro', 'recall_macro', 'f1_macro']\n",
    "    print(\" *** Subset handcrafted features *** \")\n",
    "    scores_lr_clf = cross_validate( lr_clf, X_train, y_train, cv=10, scoring=scoring, return_train_score=False)\n",
    "    scores_dt_clf = cross_validate( dt_clf, X_train, y_train, cv=10, scoring=scoring, return_train_score=False)\n",
    "    scores_rf_clf = cross_validate( rf_clf, X_train, y_train, cv=10, scoring=scoring, return_train_score=False)\n",
    "    scores_ab_clf = cross_validate( ab_clf, X_train, y_train, cv=10, scoring=scoring, return_train_score=False)\n",
    "    scores_nb_clf = cross_validate( nb_clf, X_train, y_train, cv=10, scoring=scoring, return_train_score=False)\n",
    "    scores_nn_clf = cross_validate( nn_clf, X_train, y_train, cv=10, scoring=scoring, return_train_score=False)\n",
    "    scores_svm_clf = cross_validate( svm_clf, X_train, y_train, cv=10, scoring=scoring, return_train_score=False)\n",
    "    print(\"MODELS              ,Accuracy , Precision  ,  Recall   ,  F-score          \", flush=True)\n",
    "    print(\"Logistic Regression ,{:.2f}   ,   {:.2f}   ,  {:.2f}   , {:.2f}(+/- {:.2f})\".format(scores_lr_clf['test_accuracy'].mean()*100.0,scores_lr_clf['test_precision_macro'].mean()*100.0,scores_lr_clf['test_recall_macro'].mean()*100.0,scores_lr_clf['test_f1_macro'].mean()*100.0, scores_lr_clf['test_f1_macro'].std() * 2), flush=True)\n",
    "    print(\"Decision Tree       ,{:.2f}   ,   {:.2f}   ,  {:.2f}   , {:.2f}(+/- {:.2f})\".format(scores_dt_clf['test_accuracy'].mean()*100.0,scores_dt_clf['test_precision_macro'].mean()*100.0,scores_dt_clf['test_recall_macro'].mean()*100.0,scores_dt_clf['test_f1_macro'].mean()*100.0, scores_lr_clf['test_f1_macro'].std() * 2), flush=True)\n",
    "    print(\"Random Forest       ,{:.2f}   ,   {:.2f}   ,  {:.2f}   , {:.2f}(+/- {:.2f})\".format(scores_rf_clf['test_accuracy'].mean()*100.0,scores_rf_clf['test_precision_macro'].mean()*100.0,scores_rf_clf['test_recall_macro'].mean()*100.0,scores_rf_clf['test_f1_macro'].mean()*100.0, scores_lr_clf['test_f1_macro'].std() * 2), flush=True)\n",
    "    print(\"Adaboost            ,{:.2f}   ,   {:.2f}   ,  {:.2f}   , {:.2f}(+/- {:.2f})\".format(scores_ab_clf['test_accuracy'].mean()*100.0,scores_ab_clf['test_precision_macro'].mean()*100.0,scores_ab_clf['test_recall_macro'].mean()*100.0,scores_ab_clf['test_f1_macro'].mean()*100.0, scores_lr_clf['test_f1_macro'].std() * 2), flush=True)\n",
    "    print(\"NaiveBayes          ,{:.2f}   ,   {:.2f}   ,  {:.2f}   , {:.2f}(+/- {:.2f})\".format(scores_nb_clf['test_accuracy'].mean()*100.0,scores_nb_clf['test_precision_macro'].mean()*100.0,scores_nb_clf['test_recall_macro'].mean()*100.0,scores_nb_clf['test_f1_macro'].mean()*100.0, scores_lr_clf['test_f1_macro'].std() * 2), flush=True)\n",
    "    print(\"MLP                 ,{:.2f}   ,   {:.2f}   ,  {:.2f}   , {:.2f}(+/- {:.2f})\".format(scores_nn_clf['test_accuracy'].mean()*100.0,scores_nn_clf['test_precision_macro'].mean()*100.0,scores_nn_clf['test_recall_macro'].mean()*100.0,scores_nn_clf['test_f1_macro'].mean()*100.0, scores_lr_clf['test_f1_macro'].std() * 2), flush=True)\n",
    "    print(\"SVM                 ,{:.2f}   ,   {:.2f}   ,  {:.2f}   , {:.2f}(+/- {:.2f})\".format(scores_svm_clf['test_accuracy'].mean()*100.0,scores_svm_clf['test_precision_macro'].mean()*100.0,scores_svm_clf['test_recall_macro'].mean()*100.0,scores_svm_clf['test_f1_macro'].mean()*100.0, scores_lr_clf['test_f1_macro'].std() * 2), flush=True)\n",
    "    print()    \n",
    "\n",
    "\n",
    "def bow_plus_handcrafted_features_evaluate(fullsetdf,subsetdf):\n",
    "    boW_features = bow_features(fullsetdf)\n",
    "    H_train = fullsetdf[['nchars','nwords','bhash','nhash','blink','nlink','bat','nat','brt','bslang','bintj','tlex']]\n",
    "    X_train = pd.concat([H_train,boW_features],axis=1)\n",
    "    y_train = fullsetdf['label']\n",
    "    lr_clf = LogisticRegression()\n",
    "    dt_clf = DecisionTreeClassifier()\n",
    "    rf_clf = RandomForestClassifier()\n",
    "    ab_clf = AdaBoostClassifier(n_estimators=100, random_state=0)\n",
    "    nb_clf = GaussianNB()\n",
    "    nn_clf = MLPClassifier(random_state=1, max_iter=300)\n",
    "    svm_clf = svm.SVC(gamma=0.001, C=100.)\n",
    "    scoring = ['accuracy','precision_macro', 'recall_macro', 'f1_macro']\n",
    "    print(\" *** Full dataset Bag-Of-Words  + handcrafted features *** \")\n",
    "    scores_lr_clf = cross_validate( lr_clf, X_train, y_train, cv=10, scoring=scoring, return_train_score=False)\n",
    "    scores_dt_clf = cross_validate( dt_clf, X_train, y_train, cv=10, scoring=scoring, return_train_score=False)\n",
    "    scores_rf_clf = cross_validate( rf_clf, X_train, y_train, cv=10, scoring=scoring, return_train_score=False)\n",
    "    scores_ab_clf = cross_validate( ab_clf, X_train, y_train, cv=10, scoring=scoring, return_train_score=False)\n",
    "    scores_nb_clf = cross_validate( nb_clf, X_train, y_train, cv=10, scoring=scoring, return_train_score=False)\n",
    "    scores_nn_clf = cross_validate( nn_clf, X_train, y_train, cv=10, scoring=scoring, return_train_score=False)\n",
    "    scores_svm_clf = cross_validate( svm_clf, X_train, y_train, cv=10, scoring=scoring, return_train_score=False)\n",
    "    print(\"MODELS              ,Accuracy , Precision  ,  Recall   ,  F-score          \", flush=True)\n",
    "    print(\"Logistic Regression ,{:.2f}   ,   {:.2f}   ,  {:.2f}   , {:.2f}(+/- {:.2f})\".format(scores_lr_clf['test_accuracy'].mean()*100.0,scores_lr_clf['test_precision_macro'].mean()*100.0,scores_lr_clf['test_recall_macro'].mean()*100.0,scores_lr_clf['test_f1_macro'].mean()*100.0, scores_lr_clf['test_f1_macro'].std() * 2), flush=True)\n",
    "    print(\"Decision Tree       ,{:.2f}   ,   {:.2f}   ,  {:.2f}   , {:.2f}(+/- {:.2f})\".format(scores_dt_clf['test_accuracy'].mean()*100.0,scores_dt_clf['test_precision_macro'].mean()*100.0,scores_dt_clf['test_recall_macro'].mean()*100.0,scores_dt_clf['test_f1_macro'].mean()*100.0, scores_lr_clf['test_f1_macro'].std() * 2), flush=True)\n",
    "    print(\"Random Forest       ,{:.2f}   ,   {:.2f}   ,  {:.2f}   , {:.2f}(+/- {:.2f})\".format(scores_rf_clf['test_accuracy'].mean()*100.0,scores_rf_clf['test_precision_macro'].mean()*100.0,scores_rf_clf['test_recall_macro'].mean()*100.0,scores_rf_clf['test_f1_macro'].mean()*100.0, scores_lr_clf['test_f1_macro'].std() * 2), flush=True)\n",
    "    print(\"Adaboost            ,{:.2f}   ,   {:.2f}   ,  {:.2f}   , {:.2f}(+/- {:.2f})\".format(scores_ab_clf['test_accuracy'].mean()*100.0,scores_ab_clf['test_precision_macro'].mean()*100.0,scores_ab_clf['test_recall_macro'].mean()*100.0,scores_ab_clf['test_f1_macro'].mean()*100.0, scores_lr_clf['test_f1_macro'].std() * 2), flush=True)\n",
    "    print(\"NaiveBayes          ,{:.2f}   ,   {:.2f}   ,  {:.2f}   , {:.2f}(+/- {:.2f})\".format(scores_nb_clf['test_accuracy'].mean()*100.0,scores_nb_clf['test_precision_macro'].mean()*100.0,scores_nb_clf['test_recall_macro'].mean()*100.0,scores_nb_clf['test_f1_macro'].mean()*100.0, scores_lr_clf['test_f1_macro'].std() * 2), flush=True)\n",
    "    print(\"MLP                 ,{:.2f}   ,   {:.2f}   ,  {:.2f}   , {:.2f}(+/- {:.2f})\".format(scores_nn_clf['test_accuracy'].mean()*100.0,scores_nn_clf['test_precision_macro'].mean()*100.0,scores_nn_clf['test_recall_macro'].mean()*100.0,scores_nn_clf['test_f1_macro'].mean()*100.0, scores_lr_clf['test_f1_macro'].std() * 2), flush=True)\n",
    "    print(\"SVM                 ,{:.2f}   ,   {:.2f}   ,  {:.2f}   , {:.2f}(+/- {:.2f})\".format(scores_svm_clf['test_accuracy'].mean()*100.0,scores_svm_clf['test_precision_macro'].mean()*100.0,scores_svm_clf['test_recall_macro'].mean()*100.0,scores_svm_clf['test_f1_macro'].mean()*100.0, scores_lr_clf['test_f1_macro'].std() * 2), flush=True)\n",
    "    print()\n",
    "    #@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@#\n",
    "    ### SUBSET ###\n",
    "    #@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@#\n",
    "    boW_features = bow_features(subsetdf)\n",
    "    H_train = subsetdf[['nchars','nwords','bhash','nhash','blink','nlink','bat','nat','brt','bslang','bintj','tlex','usr_vrf', 'num_followers', 'num_friends', 'num_tweets']]   \n",
    "    X_train = pd.concat([H_train,boW_features],axis=1)\n",
    "    y_train = subsetdf['label']\n",
    "    lr_clf = LogisticRegression()\n",
    "    dt_clf = DecisionTreeClassifier()\n",
    "    rf_clf = RandomForestClassifier()\n",
    "    ab_clf = AdaBoostClassifier(n_estimators=100, random_state=0)\n",
    "    nb_clf = GaussianNB()\n",
    "    nn_clf = MLPClassifier(random_state=1, max_iter=300)\n",
    "    svm_clf = svm.SVC(gamma=0.001, C=100.)\n",
    "    scoring = ['accuracy','precision_macro', 'recall_macro', 'f1_macro']\n",
    "    print(\" *** Subset Bag-Of-Words + handcrafted features *** \")\n",
    "    scores_lr_clf = cross_validate( lr_clf, X_train, y_train, cv=10, scoring=scoring, return_train_score=False)\n",
    "    scores_dt_clf = cross_validate( dt_clf, X_train, y_train, cv=10, scoring=scoring, return_train_score=False)\n",
    "    scores_rf_clf = cross_validate( rf_clf, X_train, y_train, cv=10, scoring=scoring, return_train_score=False)\n",
    "    scores_ab_clf = cross_validate( ab_clf, X_train, y_train, cv=10, scoring=scoring, return_train_score=False)\n",
    "    scores_nb_clf = cross_validate( nb_clf, X_train, y_train, cv=10, scoring=scoring, return_train_score=False)\n",
    "    scores_nn_clf = cross_validate( nn_clf, X_train, y_train, cv=10, scoring=scoring, return_train_score=False)\n",
    "    scores_svm_clf = cross_validate( svm_clf, X_train, y_train, cv=10, scoring=scoring, return_train_score=False)\n",
    "    print(\"MODELS              ,Accuracy , Precision  ,  Recall   ,  F-score          \", flush=True)\n",
    "    print(\"Logistic Regression ,{:.2f}   ,   {:.2f}   ,  {:.2f}   , {:.2f}(+/- {:.2f})\".format(scores_lr_clf['test_accuracy'].mean()*100.0,scores_lr_clf['test_precision_macro'].mean()*100.0,scores_lr_clf['test_recall_macro'].mean()*100.0,scores_lr_clf['test_f1_macro'].mean()*100.0, scores_lr_clf['test_f1_macro'].std() * 2), flush=True)\n",
    "    print(\"Decision Tree       ,{:.2f}   ,   {:.2f}   ,  {:.2f}   , {:.2f}(+/- {:.2f})\".format(scores_dt_clf['test_accuracy'].mean()*100.0,scores_dt_clf['test_precision_macro'].mean()*100.0,scores_dt_clf['test_recall_macro'].mean()*100.0,scores_dt_clf['test_f1_macro'].mean()*100.0, scores_lr_clf['test_f1_macro'].std() * 2), flush=True)\n",
    "    print(\"Random Forest       ,{:.2f}   ,   {:.2f}   ,  {:.2f}   , {:.2f}(+/- {:.2f})\".format(scores_rf_clf['test_accuracy'].mean()*100.0,scores_rf_clf['test_precision_macro'].mean()*100.0,scores_rf_clf['test_recall_macro'].mean()*100.0,scores_rf_clf['test_f1_macro'].mean()*100.0, scores_lr_clf['test_f1_macro'].std() * 2), flush=True)\n",
    "    print(\"Adaboost            ,{:.2f}   ,   {:.2f}   ,  {:.2f}   , {:.2f}(+/- {:.2f})\".format(scores_ab_clf['test_accuracy'].mean()*100.0,scores_ab_clf['test_precision_macro'].mean()*100.0,scores_ab_clf['test_recall_macro'].mean()*100.0,scores_ab_clf['test_f1_macro'].mean()*100.0, scores_lr_clf['test_f1_macro'].std() * 2), flush=True)\n",
    "    print(\"NaiveBayes          ,{:.2f}   ,   {:.2f}   ,  {:.2f}   , {:.2f}(+/- {:.2f})\".format(scores_nb_clf['test_accuracy'].mean()*100.0,scores_nb_clf['test_precision_macro'].mean()*100.0,scores_nb_clf['test_recall_macro'].mean()*100.0,scores_nb_clf['test_f1_macro'].mean()*100.0, scores_lr_clf['test_f1_macro'].std() * 2), flush=True)\n",
    "    print(\"MLP                 ,{:.2f}   ,   {:.2f}   ,  {:.2f}   , {:.2f}(+/- {:.2f})\".format(scores_nn_clf['test_accuracy'].mean()*100.0,scores_nn_clf['test_precision_macro'].mean()*100.0,scores_nn_clf['test_recall_macro'].mean()*100.0,scores_nn_clf['test_f1_macro'].mean()*100.0, scores_lr_clf['test_f1_macro'].std() * 2), flush=True)\n",
    "    print(\"SVM                 ,{:.2f}   ,   {:.2f}   ,  {:.2f}   , {:.2f}(+/- {:.2f})\".format(scores_svm_clf['test_accuracy'].mean()*100.0,scores_svm_clf['test_precision_macro'].mean()*100.0,scores_svm_clf['test_recall_macro'].mean()*100.0,scores_svm_clf['test_f1_macro'].mean()*100.0, scores_lr_clf['test_f1_macro'].std() * 2), flush=True)\n",
    "    print()    \n",
    "    \n",
    "def bow_features(fullsetdf):\n",
    "    df = preprocess(fullsetdf)\n",
    "    #df.drop(df.columns[[0]], axis=1, inplace=True)\n",
    "    # Tokenize the text column to get the new column 'tokenized_text'\n",
    "    df['tokenized_text'] = [simple_preprocess(line, deacc=True) for line in df['sentence']]\n",
    "    vectorizer = TfidfVectorizer(analyzer = 'word', strip_accents= 'ascii',smooth_idf = True, use_idf=True,max_df = 10000, min_df = 5,  stop_words = 'english')\n",
    "    X = vectorizer.fit_transform(df['sentence'])\n",
    "    boW_features = pd.DataFrame(X.toarray(),columns=vectorizer.get_feature_names())\n",
    "    return boW_features\n",
    "    \n",
    "class FullDataset():\n",
    "    def __init__(self,filename,name):\n",
    "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "        if name == 'crisismmd':\n",
    "            self.df = pd.read_csv(filename,delimiter='\\t',quoting=csv.QUOTE_NONE,error_bad_lines=False,encoding='utf-8',lineterminator=\"\\n\")\n",
    "            #self.df = pd.read_csv('/home/joao/crisismmd.ORG.tsv',delimiter='\\t',encoding='utf-8',lineterminator=\"\\n\")\n",
    "            #self.df = pd.read_csv('/home/joao/crisismmd.subset.tsv',delimiter='\\t',encoding='utf-8',lineterminator=\"\\n\")\n",
    "        if name == 'covid':\n",
    "            self.df = pd.read_csv(filename,delimiter='\\t',quoting=csv.QUOTE_NONE,error_bad_lines=False,encoding='utf-8',lineterminator=\"\\n\")\n",
    "            #self.df = pd.read_csv('/home/joao/covid.ORG.tsv',delimiter='\\t',encoding='utf-8',lineterminator=\"\\n\")\n",
    "            #self.df = pd.read_csv('/home/joao/covid.subset.tsv',delimiter='\\t',encoding='utf-8',lineterminator=\"\\n\")\n",
    "        if name ==  'crisislext6':\n",
    "            self.df = pd.read_csv(filename,delimiter='\\t',encoding='utf-8',lineterminator=\"\\n\")\n",
    "            #self.df = pd.read_csv('/home/joao/crisislext6.ORG.tsv',delimiter='\\t',encoding='utf-8',lineterminator=\"\\n\")\n",
    "            #self.df = pd.read_csv('/home/joao/crisislext6.subset.tsv',delimiter='\\t',encoding='utf-8',lineterminator=\"\\n\")\n",
    "        if name == 'crisislext26':\n",
    "            self.df = pd.read_csv(filename,delimiter='\\t',encoding='utf-8',lineterminator=\"\\n\")\n",
    "            #self.df = pd.read_csv('/home/joao/crisislext26.ORG.tsv',delimiter='\\t',encoding='utf-8',lineterminator=\"\\n\")\n",
    "            #self.df = pd.read_csv('/home/joao/crisislext26.subset.tsv',delimiter='\\t',encoding='utf-8',lineterminator=\"\\n\")\n",
    "        #self.df = self.df[['tweet_id','sentence','label']]\n",
    "        self.sentences = self.df['sentence']\n",
    "        #'Unnamed: 0'\n",
    "        self.df.drop(self.df.columns[[0]], axis=1, inplace=True)\n",
    "        self.labels = self.df['label'].values\n",
    "        self.df['sentence'] = self.df['sentence'].str.replace(r'http(\\S)+', r'')\n",
    "        self.df['sentence'] = self.df['sentence'].str.replace(r'http(\\S)+', r'')\n",
    "        self.df['sentence'] = self.df['sentence'].str.replace(r'http ...', r'')\n",
    "        self.df['sentence'] = self.df['sentence'].str.replace(r'(RT|rt)[ ]*@[ ]*[\\S]+',r'')\n",
    "        self.df['sentence'] = self.df['sentence'].str.replace(r'@[\\S]+',r'')\n",
    "        self.df['sentence'] = self.df['sentence'].str.replace(r'_[\\S]?',r'')\n",
    "        self.df['sentence'] = self.df['sentence'].str.replace(r'[ ]{2, }',r' ')\n",
    "        self.df['sentence'] = self.df['sentence'].str.replace(r'&amp;?',r'and')\n",
    "        self.df['sentence'] = self.df['sentence'].str.replace(r'&lt;',r'<')\n",
    "        self.df['sentence'] = self.df['sentence'].str.replace(r'&gt;',r'>')\n",
    "        self.df['sentence'] = self.df['sentence'].str.replace(r'([\\w\\d]+)([^\\w\\d ]+)', r'\\1 \\2')\n",
    "        self.df['sentence'] = self.df['sentence'].str.replace(r'([^\\w\\d ]+)([\\w\\d]+)', r'\\1 \\2')\n",
    "        self.df['sentence'] = self.df['sentence'].str.lower()\n",
    "        self.df['sentence'] = self.df['sentence'].str.strip()\n",
    "        self.df['sentence'] = self.df['sentence'].apply(remove_special_chars)\n",
    "        self.df['sentence'] = self.df['sentence'].apply(remove_digit)\n",
    "        self.df['sentence'] = self.df['sentence'].apply(remove_stopwords)\n",
    "        self.df['sentence'] = self.df['sentence'].str.replace(\"\\'\", \"\")\n",
    "        self.df['sentence'] = self.df['sentence'].str.replace(\"\\\"\", \"\")\n",
    "        self.df['sentence'] = self.df['sentence'].apply(remove_non_english)\n",
    "        self.df['sentence'] = self.df['sentence'].apply(remove_nonUTF8)\n",
    "        self.df['sentence'] = self.df['sentence'].apply(remove_shortwords)\n",
    "        self.hand_crafted_features = self.df[['nchars', 'nwords','bhash','nhash','blink','nlink','bat','nat','brt','bslang','bintj','tlex']]\n",
    "        self.hand_crafted_features_DF = pd.DataFrame(self.hand_crafted_features, columns = ['nchars', 'nwords','bhash','nhash','blink','nlink','bat','nat','brt','bslang','bintj','tlex']).astype(float)\n",
    "        self.maxlen = 0\n",
    "        #if name == 'covid':\n",
    "        self.maxlen = 80\n",
    "        #else:\n",
    "        #    for sent in self.sentences:\n",
    "        #        input_ids = self.tokenizer.encode(sent, add_special_tokens=True)\n",
    "        #        self.maxlen = max(self.maxlen, len(input_ids))\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    def __getitem__(self, idx):\n",
    "        sentence = self.df.loc[idx, 'sentence']\n",
    "        label = self.df.loc[idx, 'label']\n",
    "        h_features = self.hand_crafted_features_DF.loc[idx,:]\n",
    "        h_tensor = torch.tensor(h_features).to(device)\n",
    "        tokens = self.tokenizer.tokenize(sentence)\n",
    "        if len(tokens) == 0:\n",
    "            tokens = ['']\n",
    "        encoded_dict = self.tokenizer.encode_plus(tokens, add_special_tokens = True, max_length = self.maxlen, pad_to_max_length = True,return_attention_mask = True)\n",
    "        tokens_ids = encoded_dict['input_ids']\n",
    "        tokens_ids_tensor = torch.tensor(tokens_ids).to(device) #Converting the list to a pytorch tensor\n",
    "        attn_mask = encoded_dict['attention_mask']\n",
    "        attn_mask_tensor = torch.tensor(attn_mask).to(device)\n",
    "        label_tensor = torch.tensor(label).to(device)\n",
    "        return tokens_ids_tensor,attn_mask_tensor,h_tensor,label_tensor\n",
    "        \n",
    "def main():\n",
    "    datasets = ['covid', 'crisislext6', 'crisislext26', 'crisismmd']\n",
    "    #datasets = ['covid']\n",
    "    for data in datasets :\n",
    "        print(\"=== {} ===\".format(data))\n",
    "        #bert_based_features_evaluate(data)\n",
    "        if data == 'covid':\n",
    "            fullsetdf = pd.read_csv('/home/joao/covid.ORG.tsv',delimiter='\\t',encoding='utf-8',lineterminator=\"\\n\")\n",
    "            subsetdf = pd.read_csv('/home/joao/covid.subset.tsv',delimiter='\\t',encoding='utf-8',lineterminator=\"\\n\")\n",
    "        if data ==  'crisislext6':\n",
    "            fullsetdf = pd.read_csv('/home/joao/crisislext6.ORG.tsv',delimiter='\\t',encoding='utf-8',lineterminator=\"\\n\")\n",
    "            subsetdf = pd.read_csv('/home/joao/crisislext6.subset.tsv',delimiter='\\t',encoding='utf-8',lineterminator=\"\\n\")\n",
    "        if data == 'crisislext26':\n",
    "            fullsetdf = pd.read_csv('/home/joao/crisislext26.ORG.tsv',delimiter='\\t',encoding='utf-8',lineterminator=\"\\n\")\n",
    "            subsetdf = pd.read_csv('/home/joao/crisislext26.subset.tsv',delimiter='\\t',encoding='utf-8',lineterminator=\"\\n\")\n",
    "        if data == 'crisismmd':\n",
    "            fullsetdf = pd.read_csv('/home/joao/crisismmd.ORG.tsv',delimiter='\\t',quoting=csv.QUOTE_NONE,error_bad_lines=False, encoding='utf-8',lineterminator=\"\\n\")\n",
    "            subsetdf = pd.read_csv('/home/joao/crisismmd.subset.tsv',delimiter='\\t',quoting=csv.QUOTE_NONE,error_bad_lines=False,encoding='utf-8',lineterminator=\"\\n\")\n",
    "        #bert_based_features_evaluate(data)\n",
    "        #fast_text_evaluate(fullsetdf,subsetdf)\n",
    "        ### Bag of Words Features\n",
    "        #fullset_boW_df = boW_features(fullsetdf)    \n",
    "        #subsetdf_boW_df = boW_features(subsetdf)    \n",
    "        ### Word2Vec features\n",
    "        #fullset_Word2Vec_df = Word2Vec_features(fullsetdf)    \n",
    "        #subsetdf_Word2Vec_df = Word2Vec_features(subsetdf)    \n",
    "        ### Doc2Vec features\n",
    "        #fullset_Doc2Vec_df = Doc2Vec_features(fullsetdf)    \n",
    "        #subsetdf_Doc2Vec_df = Doc2Vec_features(subsetdf)  \n",
    "        #handcrafted_features_evaluate(fullsetdf,subsetdf)\n",
    "        #bow_evaluate(fullsetdf,subsetdf)\n",
    "        #bow_plus_handcrafted_features_evaluate(fullsetdf,subsetdf)\n",
    "        #word2vec_evaluate(fullsetdf,subsetdf)\n",
    "        #doc2vec_evaluate(fullsetdf,subsetdf)\n",
    "        glove_evaluate(fullsetdf,subsetdf)\n",
    "        \n",
    "        \n",
    "                \n",
    "main()\n",
    "\n",
    "\n",
    "###def error_analysis(fullsetdf,subsetdf):\n",
    "fullsetdf = pd.read_csv('/home/joao/crisismmd.ORG.tsv',delimiter='\\t',encoding='utf-8',lineterminator=\"\\n\")\n",
    "X_train = fullsetdf[['nchars','nwords','bhash','nhash','blink','nlink','bat','nat','brt','bslang','bintj','tlex']]\n",
    "y_train = fullsetdf['label']\n",
    "rf_clf = RandomForestClassifier()\n",
    "rf_clf.fit(X_train,y_train)\n",
    "y_pred = cross_val_predict(rf_clf, X_train, y_train, cv=10)\n",
    "conf_mat = confusion_matrix(y_train, y_pred)\n",
    "fullsetdf['ypred'] = y_pred\n",
    "fullsetdf[['sentence','label','ypred']].to_csv('/home/joao/crisismmd.err.csv',sep='\\t', encoding='utf-8')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### method to calculate prediction with FastText\n",
    "def fast_text_evaluate(fullsetdf,subsetdf):\n",
    "    ft = fasttext.load_model('cc.en.300.bin')\n",
    "    df = preprocess(fullsetdf)\n",
    "    df['tokenized_text'] = [simple_preprocess(line, deacc=True) for line in df['sentence']]\n",
    "    words = []\n",
    "    idx = 0\n",
    "    word2idx = {}\n",
    "    emb_dim = 300\n",
    "    vectors = []\n",
    "    fasttextDF = pd.DataFrame([], columns = list(range(emb_dim)))\n",
    "    for i, tokens  in enumerate(df['tokenized_text']):\n",
    "        target_vocab = tokens\n",
    "        matrix_len = len(target_vocab)\n",
    "        weights_matrix = np.zeros((matrix_len, emb_dim))\n",
    "        words_found = 0\n",
    "        for i, word in enumerate(target_vocab):\n",
    "            try: \n",
    "                weights_matrix[i] = ft.get_word_vector(word)\n",
    "                words_found += 1\n",
    "            except KeyError:\n",
    "                weights_matrix[i] = np.random.normal(scale=0.6, size=(emb_dim, ))\n",
    "        weights_matrix = np.mean(weights_matrix,axis=0)\n",
    "        fasttextDF = fasttextDF.append([weights_matrix])\n",
    "    fasttextDF = fasttextDF.reset_index(drop=True)\n",
    "    fasttextDF = fasttextDF.replace(np.nan,0)\n",
    "    y_train = pd.DataFrame(fullsetdf['label'])\n",
    "    X_train = fasttextDF\n",
    "    lr_clf = LogisticRegression()\n",
    "    dt_clf = DecisionTreeClassifier()\n",
    "    rf_clf = RandomForestClassifier()\n",
    "    ab_clf = AdaBoostClassifier(n_estimators=100, random_state=0)\n",
    "    nb_clf = GaussianNB()\n",
    "    nn_clf = MLPClassifier(random_state=1, max_iter=300)\n",
    "    svm_clf = svm.SVC(gamma=0.001, C=100.)\n",
    "    scoring = ['accuracy','precision_macro', 'recall_macro', 'f1_macro']\n",
    "    print(\" *** Full dataset fasttext features *** \")\n",
    "    scores_lr_clf = cross_validate( lr_clf, X_train, y_train, cv=10, scoring=scoring, return_train_score=False)\n",
    "    scores_dt_clf = cross_validate( dt_clf, X_train, y_train, cv=10, scoring=scoring, return_train_score=False)\n",
    "    scores_rf_clf = cross_validate( rf_clf, X_train, y_train, cv=10, scoring=scoring, return_train_score=False)\n",
    "    scores_ab_clf = cross_validate( ab_clf, X_train, y_train, cv=10, scoring=scoring, return_train_score=False)\n",
    "    scores_nb_clf = cross_validate( nb_clf, X_train, y_train, cv=10, scoring=scoring, return_train_score=False)\n",
    "    scores_nn_clf = cross_validate( nn_clf, X_train, y_train, cv=10, scoring=scoring, return_train_score=False)\n",
    "    scores_svm_clf = cross_validate( svm_clf, X_train, y_train, cv=10, scoring=scoring, return_train_score=False)\n",
    "    print(\"MODELS              ,Accuracy , Precision  ,  Recall   ,  F-score          \", flush=True)\n",
    "    print(\"Logistic Regression ,{:.2f}   ,   {:.2f}   ,  {:.2f}   , {:.2f}(+/- {:.2f})\".format(scores_lr_clf['test_accuracy'].mean()*100.0,scores_lr_clf['test_precision_macro'].mean()*100.0,scores_lr_clf['test_recall_macro'].mean()*100.0,scores_lr_clf['test_f1_macro'].mean()*100.0, scores_lr_clf['test_f1_macro'].std() * 2), flush=True)\n",
    "    print(\"Decision Tree       ,{:.2f}   ,   {:.2f}   ,  {:.2f}   , {:.2f}(+/- {:.2f})\".format(scores_dt_clf['test_accuracy'].mean()*100.0,scores_dt_clf['test_precision_macro'].mean()*100.0,scores_dt_clf['test_recall_macro'].mean()*100.0,scores_dt_clf['test_f1_macro'].mean()*100.0, scores_lr_clf['test_f1_macro'].std() * 2), flush=True)\n",
    "    print(\"Random Forest       ,{:.2f}   ,   {:.2f}   ,  {:.2f}   , {:.2f}(+/- {:.2f})\".format(scores_rf_clf['test_accuracy'].mean()*100.0,scores_rf_clf['test_precision_macro'].mean()*100.0,scores_rf_clf['test_recall_macro'].mean()*100.0,scores_rf_clf['test_f1_macro'].mean()*100.0, scores_lr_clf['test_f1_macro'].std() * 2), flush=True)\n",
    "    print(\"Adaboost            ,{:.2f}   ,   {:.2f}   ,  {:.2f}   , {:.2f}(+/- {:.2f})\".format(scores_ab_clf['test_accuracy'].mean()*100.0,scores_ab_clf['test_precision_macro'].mean()*100.0,scores_ab_clf['test_recall_macro'].mean()*100.0,scores_ab_clf['test_f1_macro'].mean()*100.0, scores_lr_clf['test_f1_macro'].std() * 2), flush=True)\n",
    "    print(\"NaiveBayes          ,{:.2f}   ,   {:.2f}   ,  {:.2f}   , {:.2f}(+/- {:.2f})\".format(scores_nb_clf['test_accuracy'].mean()*100.0,scores_nb_clf['test_precision_macro'].mean()*100.0,scores_nb_clf['test_recall_macro'].mean()*100.0,scores_nb_clf['test_f1_macro'].mean()*100.0, scores_lr_clf['test_f1_macro'].std() * 2), flush=True)\n",
    "    print(\"MLP                 ,{:.2f}   ,   {:.2f}   ,  {:.2f}   , {:.2f}(+/- {:.2f})\".format(scores_nn_clf['test_accuracy'].mean()*100.0,scores_nn_clf['test_precision_macro'].mean()*100.0,scores_nn_clf['test_recall_macro'].mean()*100.0,scores_nn_clf['test_f1_macro'].mean()*100.0, scores_lr_clf['test_f1_macro'].std() * 2), flush=True)\n",
    "    print(\"SVM                 ,{:.2f}   ,   {:.2f}   ,  {:.2f}   , {:.2f}(+/- {:.2f})\".format(scores_svm_clf['test_accuracy'].mean()*100.0,scores_svm_clf['test_precision_macro'].mean()*100.0,scores_svm_clf['test_recall_macro'].mean()*100.0,scores_svm_clf['test_f1_macro'].mean()*100.0, scores_lr_clf['test_f1_macro'].std() * 2), flush=True)\n",
    "    print()\n",
    "    #@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@#\n",
    "    ### SUBSET ###\n",
    "    #@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@#\n",
    "    df = preprocess(subsetdf)\n",
    "    df['tokenized_text'] = [simple_preprocess(line, deacc=True) for line in df['sentence']]\n",
    "    words = []\n",
    "    idx = 0\n",
    "    word2idx = {}\n",
    "    emb_dim = 300\n",
    "    vectors = []\n",
    "    fasttextDF = pd.DataFrame([], columns = list(range(emb_dim)))\n",
    "    for i, tokens  in enumerate(df['tokenized_text']):\n",
    "        target_vocab = tokens\n",
    "        matrix_len = len(target_vocab)\n",
    "        weights_matrix = np.zeros((matrix_len, emb_dim))\n",
    "        words_found = 0\n",
    "        for i, word in enumerate(target_vocab):\n",
    "            try: \n",
    "                weights_matrix[i] = ft.get_word_vector(word)\n",
    "                words_found += 1\n",
    "            except KeyError:\n",
    "                weights_matrix[i] = np.random.normal(scale=0.6, size=(emb_dim, ))\n",
    "        weights_matrix = np.mean(weights_matrix,axis=0)\n",
    "        fasttextDF = fasttextDF.append([weights_matrix])\n",
    "    fasttextDF = fasttextDF.reset_index(drop=True)\n",
    "    fasttextDF = fasttextDF.replace(np.nan,0)\n",
    "    y_train = pd.DataFrame(subsetdf['label'])\n",
    "    X_train = fasttextDF\n",
    "    lr_clf = LogisticRegression()\n",
    "    dt_clf = DecisionTreeClassifier()\n",
    "    rf_clf = RandomForestClassifier()\n",
    "    ab_clf = AdaBoostClassifier(n_estimators=100, random_state=0)\n",
    "    nb_clf = GaussianNB()\n",
    "    nn_clf = MLPClassifier(random_state=1, max_iter=300)\n",
    "    svm_clf = svm.SVC(gamma=0.001, C=100.)\n",
    "    scoring = ['accuracy','precision_macro', 'recall_macro', 'f1_macro']\n",
    "    print(\" *** Subset Fasttext features *** \")\n",
    "    scores_lr_clf = cross_validate( lr_clf, X_train, y_train, cv=10, scoring=scoring, return_train_score=False)\n",
    "    scores_dt_clf = cross_validate( dt_clf, X_train, y_train, cv=10, scoring=scoring, return_train_score=False)\n",
    "    scores_rf_clf = cross_validate( rf_clf, X_train, y_train, cv=10, scoring=scoring, return_train_score=False)\n",
    "    scores_ab_clf = cross_validate( ab_clf, X_train, y_train, cv=10, scoring=scoring, return_train_score=False)\n",
    "    scores_nb_clf = cross_validate( nb_clf, X_train, y_train, cv=10, scoring=scoring, return_train_score=False)\n",
    "    scores_nn_clf = cross_validate( nn_clf, X_train, y_train, cv=10, scoring=scoring, return_train_score=False)\n",
    "    scores_svm_clf = cross_validate( svm_clf, X_train, y_train, cv=10, scoring=scoring, return_train_score=False)\n",
    "    print(\"MODELS              ,Accuracy , Precision  ,  Recall   ,  F-score          \", flush=True)\n",
    "    print(\"Logistic Regression ,{:.2f}   ,   {:.2f}   ,  {:.2f}   , {:.2f}(+/- {:.2f})\".format(scores_lr_clf['test_accuracy'].mean()*100.0,scores_lr_clf['test_precision_macro'].mean()*100.0,scores_lr_clf['test_recall_macro'].mean()*100.0,scores_lr_clf['test_f1_macro'].mean()*100.0, scores_lr_clf['test_f1_macro'].std() * 2), flush=True)\n",
    "    print(\"Decision Tree       ,{:.2f}   ,   {:.2f}   ,  {:.2f}   , {:.2f}(+/- {:.2f})\".format(scores_dt_clf['test_accuracy'].mean()*100.0,scores_dt_clf['test_precision_macro'].mean()*100.0,scores_dt_clf['test_recall_macro'].mean()*100.0,scores_dt_clf['test_f1_macro'].mean()*100.0, scores_lr_clf['test_f1_macro'].std() * 2), flush=True)\n",
    "    print(\"Random Forest       ,{:.2f}   ,   {:.2f}   ,  {:.2f}   , {:.2f}(+/- {:.2f})\".format(scores_rf_clf['test_accuracy'].mean()*100.0,scores_rf_clf['test_precision_macro'].mean()*100.0,scores_rf_clf['test_recall_macro'].mean()*100.0,scores_rf_clf['test_f1_macro'].mean()*100.0, scores_lr_clf['test_f1_macro'].std() * 2), flush=True)\n",
    "    #print(\"Adaboost            ,{:.2f}   ,   {:.2f}   ,  {:.2f}   , {:.2f}(+/- {:.2f})\".format(scores_ab_clf['test_accuracy'].mean()*100.0,scores_ab_clf['test_precision_macro'].mean()*100.0,scores_ab_clf['test_recall_macro'].mean()*100.0,scores_ab_clf['test_f1_macro'].mean()*100.0, scores_lr_clf['test_f1_macro'].std() * 2), flush=True)\n",
    "    print(\"NaiveBayes          ,{:.2f}   ,   {:.2f}   ,  {:.2f}   , {:.2f}(+/- {:.2f})\".format(scores_nb_clf['test_accuracy'].mean()*100.0,scores_nb_clf['test_precision_macro'].mean()*100.0,scores_nb_clf['test_recall_macro'].mean()*100.0,scores_nb_clf['test_f1_macro'].mean()*100.0, scores_lr_clf['test_f1_macro'].std() * 2), flush=True)\n",
    "    print(\"MLP                 ,{:.2f}   ,   {:.2f}   ,  {:.2f}   , {:.2f}(+/- {:.2f})\".format(scores_nn_clf['test_accuracy'].mean()*100.0,scores_nn_clf['test_precision_macro'].mean()*100.0,scores_nn_clf['test_recall_macro'].mean()*100.0,scores_nn_clf['test_f1_macro'].mean()*100.0, scores_lr_clf['test_f1_macro'].std() * 2), flush=True)\n",
    "    print(\"SVM                 ,{:.2f}   ,   {:.2f}   ,  {:.2f}   , {:.2f}(+/- {:.2f})\".format(scores_svm_clf['test_accuracy'].mean()*100.0,scores_svm_clf['test_precision_macro'].mean()*100.0,scores_svm_clf['test_recall_macro'].mean()*100.0,scores_svm_clf['test_f1_macro'].mean()*100.0, scores_lr_clf['test_f1_macro'].std() * 2), flush=True)\n",
    "    print()\n",
    "\n",
    "### method to calculate prediction with Glove embeddings\n",
    "#http://nlp.stanford.edu/data/wordvecs/glove.6B.zip\n",
    "def glove_evaluate(fullsetdf,subsetdf):\n",
    "    df = preprocess(fullsetdf)\n",
    "    df['tokenized_text'] = [simple_preprocess(line, deacc=True) for line in df['sentence']]\n",
    "    glove_path = \"/home/joao/\"\n",
    "    words = []\n",
    "    idx = 0\n",
    "    word2idx = {}\n",
    "    emb_dim = 300\n",
    "    vectors = []\n",
    "    with open(f'{glove_path}/glove.6B.300d.txt', 'rb') as f:\n",
    "        for l in f:\n",
    "            line = l.decode().split()\n",
    "            word = line[0]\n",
    "            words.append(word)\n",
    "            word2idx[word] = idx\n",
    "            idx += 1\n",
    "            vect = np.array(line[1:]).astype(np.float)\n",
    "            vectors.append(vect)\n",
    "    glove = {w: vectors[word2idx[w]] for w in words}\n",
    "    gloveDF = pd.DataFrame([], columns = list(range(emb_dim)))\n",
    "    for i, tokens  in enumerate(df['tokenized_text']):\n",
    "        target_vocab = tokens\n",
    "        matrix_len = len(target_vocab)\n",
    "        weights_matrix = np.zeros((matrix_len, emb_dim))\n",
    "        words_found = 0\n",
    "        for i, word in enumerate(target_vocab):\n",
    "            try: \n",
    "                weights_matrix[i] = glove[word]\n",
    "                words_found += 1\n",
    "            except KeyError:\n",
    "                weights_matrix[i] = np.random.normal(scale=0.6, size=(emb_dim, ))\n",
    "        weights_matrix = np.mean(weights_matrix,axis=0)\n",
    "        gloveDF = gloveDF.append([weights_matrix])\n",
    "    gloveDF = gloveDF.reset_index(drop=True)\n",
    "    gloveDF = gloveDF.replace(np.nan,0)\n",
    "    y_train = pd.DataFrame(fullsetdf['label'])\n",
    "    X_train = gloveDF\n",
    "    lr_clf = LogisticRegression()\n",
    "    dt_clf = DecisionTreeClassifier()\n",
    "    rf_clf = RandomForestClassifier()\n",
    "    ab_clf = AdaBoostClassifier(n_estimators=100, random_state=0)\n",
    "    nb_clf = GaussianNB()\n",
    "    nn_clf = MLPClassifier(random_state=1, max_iter=300)\n",
    "    svm_clf = svm.SVC(gamma=0.001, C=100.)\n",
    "    scoring = ['accuracy','precision_macro', 'recall_macro', 'f1_macro']\n",
    "    print(\" *** Full dataset glove features *** \")\n",
    "    scores_lr_clf = cross_validate( lr_clf, X_train, y_train, cv=10, scoring=scoring, return_train_score=False)\n",
    "    scores_dt_clf = cross_validate( dt_clf, X_train, y_train, cv=10, scoring=scoring, return_train_score=False)\n",
    "    scores_rf_clf = cross_validate( rf_clf, X_train, y_train, cv=10, scoring=scoring, return_train_score=False)\n",
    "    scores_ab_clf = cross_validate( ab_clf, X_train, y_train, cv=10, scoring=scoring, return_train_score=False)\n",
    "    scores_nb_clf = cross_validate( nb_clf, X_train, y_train, cv=10, scoring=scoring, return_train_score=False)\n",
    "    scores_nn_clf = cross_validate( nn_clf, X_train, y_train, cv=10, scoring=scoring, return_train_score=False)\n",
    "    scores_svm_clf = cross_validate( svm_clf, X_train, y_train, cv=10, scoring=scoring, return_train_score=False)\n",
    "    print(\"MODELS              ,Accuracy , Precision  ,  Recall   ,  F-score          \", flush=True)\n",
    "    print(\"Logistic Regression ,{:.2f}   ,   {:.2f}   ,  {:.2f}   , {:.2f}(+/- {:.2f})\".format(scores_lr_clf['test_accuracy'].mean()*100.0,scores_lr_clf['test_precision_macro'].mean()*100.0,scores_lr_clf['test_recall_macro'].mean()*100.0,scores_lr_clf['test_f1_macro'].mean()*100.0, scores_lr_clf['test_f1_macro'].std() * 2), flush=True)\n",
    "    print(\"Decision Tree       ,{:.2f}   ,   {:.2f}   ,  {:.2f}   , {:.2f}(+/- {:.2f})\".format(scores_dt_clf['test_accuracy'].mean()*100.0,scores_dt_clf['test_precision_macro'].mean()*100.0,scores_dt_clf['test_recall_macro'].mean()*100.0,scores_dt_clf['test_f1_macro'].mean()*100.0, scores_lr_clf['test_f1_macro'].std() * 2), flush=True)\n",
    "    print(\"Random Forest       ,{:.2f}   ,   {:.2f}   ,  {:.2f}   , {:.2f}(+/- {:.2f})\".format(scores_rf_clf['test_accuracy'].mean()*100.0,scores_rf_clf['test_precision_macro'].mean()*100.0,scores_rf_clf['test_recall_macro'].mean()*100.0,scores_rf_clf['test_f1_macro'].mean()*100.0, scores_lr_clf['test_f1_macro'].std() * 2), flush=True)\n",
    "    print(\"Adaboost            ,{:.2f}   ,   {:.2f}   ,  {:.2f}   , {:.2f}(+/- {:.2f})\".format(scores_ab_clf['test_accuracy'].mean()*100.0,scores_ab_clf['test_precision_macro'].mean()*100.0,scores_ab_clf['test_recall_macro'].mean()*100.0,scores_ab_clf['test_f1_macro'].mean()*100.0, scores_lr_clf['test_f1_macro'].std() * 2), flush=True)\n",
    "    print(\"NaiveBayes          ,{:.2f}   ,   {:.2f}   ,  {:.2f}   , {:.2f}(+/- {:.2f})\".format(scores_nb_clf['test_accuracy'].mean()*100.0,scores_nb_clf['test_precision_macro'].mean()*100.0,scores_nb_clf['test_recall_macro'].mean()*100.0,scores_nb_clf['test_f1_macro'].mean()*100.0, scores_lr_clf['test_f1_macro'].std() * 2), flush=True)\n",
    "    print(\"MLP                 ,{:.2f}   ,   {:.2f}   ,  {:.2f}   , {:.2f}(+/- {:.2f})\".format(scores_nn_clf['test_accuracy'].mean()*100.0,scores_nn_clf['test_precision_macro'].mean()*100.0,scores_nn_clf['test_recall_macro'].mean()*100.0,scores_nn_clf['test_f1_macro'].mean()*100.0, scores_lr_clf['test_f1_macro'].std() * 2), flush=True)\n",
    "    print(\"SVM                 ,{:.2f}   ,   {:.2f}   ,  {:.2f}   , {:.2f}(+/- {:.2f})\".format(scores_svm_clf['test_accuracy'].mean()*100.0,scores_svm_clf['test_precision_macro'].mean()*100.0,scores_svm_clf['test_recall_macro'].mean()*100.0,scores_svm_clf['test_f1_macro'].mean()*100.0, scores_lr_clf['test_f1_macro'].std() * 2), flush=True)\n",
    "    print()\n",
    "    #@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@#\n",
    "    ### SUBSET ###\n",
    "    #@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@#\n",
    "    df = preprocess(subsetdf)\n",
    "    df['tokenized_text'] = [simple_preprocess(line, deacc=True) for line in df['sentence']]\n",
    "    gloveDF = pd.DataFrame([], columns = list(range(emb_dim)))\n",
    "    for i, tokens  in enumerate(df['tokenized_text']):\n",
    "        target_vocab = tokens\n",
    "        matrix_len = len(target_vocab)\n",
    "        weights_matrix = np.zeros((matrix_len, emb_dim))\n",
    "        words_found = 0\n",
    "        for i, word in enumerate(target_vocab):\n",
    "            try: \n",
    "                weights_matrix[i] = glove[word]\n",
    "                words_found += 1\n",
    "            except KeyError:\n",
    "                weights_matrix[i] = np.random.normal(scale=0.6, size=(emb_dim, ))\n",
    "        weights_matrix = np.mean(weights_matrix,axis=0)\n",
    "        gloveDF = gloveDF.append([weights_matrix])\n",
    "    gloveDF = gloveDF.reset_index(drop=True)\n",
    "    gloveDF = gloveDF.replace(np.nan,0)\n",
    "    y_train = pd.DataFrame(subsetdf['label'])\n",
    "    X_train = gloveDF\n",
    "    lr_clf = LogisticRegression()\n",
    "    dt_clf = DecisionTreeClassifier()\n",
    "    rf_clf = RandomForestClassifier()\n",
    "    ab_clf = AdaBoostClassifier(n_estimators=100, random_state=0)\n",
    "    nb_clf = GaussianNB()\n",
    "    nn_clf = MLPClassifier(random_state=1, max_iter=300)\n",
    "    svm_clf = svm.SVC(gamma=0.001, C=100.)\n",
    "    scoring = ['accuracy','precision_macro', 'recall_macro', 'f1_macro']\n",
    "    print(\" *** Subset glove features *** \")\n",
    "    scores_lr_clf = cross_validate( lr_clf, X_train, y_train, cv=10, scoring=scoring, return_train_score=False)\n",
    "    scores_dt_clf = cross_validate( dt_clf, X_train, y_train, cv=10, scoring=scoring, return_train_score=False)\n",
    "    scores_rf_clf = cross_validate( rf_clf, X_train, y_train, cv=10, scoring=scoring, return_train_score=False)\n",
    "    scores_ab_clf = cross_validate( ab_clf, X_train, y_train, cv=10, scoring=scoring, return_train_score=False)\n",
    "    scores_nb_clf = cross_validate( nb_clf, X_train, y_train, cv=10, scoring=scoring, return_train_score=False)\n",
    "    scores_nn_clf = cross_validate( nn_clf, X_train, y_train, cv=10, scoring=scoring, return_train_score=False)\n",
    "    scores_svm_clf = cross_validate( svm_clf, X_train, y_train, cv=10, scoring=scoring, return_train_score=False)\n",
    "    print(\"MODELS              ,Accuracy , Precision  ,  Recall   ,  F-score          \", flush=True)\n",
    "    print(\"Logistic Regression ,{:.2f}   ,   {:.2f}   ,  {:.2f}   , {:.2f}(+/- {:.2f})\".format(scores_lr_clf['test_accuracy'].mean()*100.0,scores_lr_clf['test_precision_macro'].mean()*100.0,scores_lr_clf['test_recall_macro'].mean()*100.0,scores_lr_clf['test_f1_macro'].mean()*100.0, scores_lr_clf['test_f1_macro'].std() * 2), flush=True)\n",
    "    print(\"Decision Tree       ,{:.2f}   ,   {:.2f}   ,  {:.2f}   , {:.2f}(+/- {:.2f})\".format(scores_dt_clf['test_accuracy'].mean()*100.0,scores_dt_clf['test_precision_macro'].mean()*100.0,scores_dt_clf['test_recall_macro'].mean()*100.0,scores_dt_clf['test_f1_macro'].mean()*100.0, scores_lr_clf['test_f1_macro'].std() * 2), flush=True)\n",
    "    print(\"Random Forest       ,{:.2f}   ,   {:.2f}   ,  {:.2f}   , {:.2f}(+/- {:.2f})\".format(scores_rf_clf['test_accuracy'].mean()*100.0,scores_rf_clf['test_precision_macro'].mean()*100.0,scores_rf_clf['test_recall_macro'].mean()*100.0,scores_rf_clf['test_f1_macro'].mean()*100.0, scores_lr_clf['test_f1_macro'].std() * 2), flush=True)\n",
    "    print(\"Adaboost            ,{:.2f}   ,   {:.2f}   ,  {:.2f}   , {:.2f}(+/- {:.2f})\".format(scores_ab_clf['test_accuracy'].mean()*100.0,scores_ab_clf['test_precision_macro'].mean()*100.0,scores_ab_clf['test_recall_macro'].mean()*100.0,scores_ab_clf['test_f1_macro'].mean()*100.0, scores_lr_clf['test_f1_macro'].std() * 2), flush=True)\n",
    "    print(\"NaiveBayes          ,{:.2f}   ,   {:.2f}   ,  {:.2f}   , {:.2f}(+/- {:.2f})\".format(scores_nb_clf['test_accuracy'].mean()*100.0,scores_nb_clf['test_precision_macro'].mean()*100.0,scores_nb_clf['test_recall_macro'].mean()*100.0,scores_nb_clf['test_f1_macro'].mean()*100.0, scores_lr_clf['test_f1_macro'].std() * 2), flush=True)\n",
    "    print(\"MLP                 ,{:.2f}   ,   {:.2f}   ,  {:.2f}   , {:.2f}(+/- {:.2f})\".format(scores_nn_clf['test_accuracy'].mean()*100.0,scores_nn_clf['test_precision_macro'].mean()*100.0,scores_nn_clf['test_recall_macro'].mean()*100.0,scores_nn_clf['test_f1_macro'].mean()*100.0, scores_lr_clf['test_f1_macro'].std() * 2), flush=True)\n",
    "    print(\"SVM                 ,{:.2f}   ,   {:.2f}   ,  {:.2f}   , {:.2f}(+/- {:.2f})\".format(scores_svm_clf['test_accuracy'].mean()*100.0,scores_svm_clf['test_precision_macro'].mean()*100.0,scores_svm_clf['test_recall_macro'].mean()*100.0,scores_svm_clf['test_f1_macro'].mean()*100.0, scores_lr_clf['test_f1_macro'].std() * 2), flush=True)\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Gensim method for calculation TF idf\n",
    "def bag_of_Words_evaluate(fullsetdf,subsetdf):\n",
    "    df = preprocess(fullsetdf)\n",
    "    # Tokenize the text column to get the new column 'tokenized_text'\n",
    "    df['tokenized_text'] = [simple_preprocess(line, deacc=True) for line in df['sentence']]\n",
    "    mydict = corpora.Dictionary(df['tokenized_text'])\n",
    "    corpus = [mydict.doc2bow(line) for line in df['tokenized_text']]\n",
    "    # TF-IDF Model\n",
    "    tfidf_model = TfidfModel(corpus)\n",
    "    tfidf_filename = '/home/joao/tfidf.csv'\n",
    "    vocab_len = len(mydict.token2id)\n",
    "    with open(tfidf_filename, 'w') as tfidf_file:\n",
    "        for index, row in df.iterrows():\n",
    "            doc = mydict.doc2bow(row['tokenized_text'])\n",
    "            label = row['label']\n",
    "            features = gensim.matutils.corpus2csc([tfidf_model[doc]], num_terms=vocab_len).toarray()[:,0]\n",
    "            if index == 0:\n",
    "                tfidf_file.write(header+str(\",label\"))\n",
    "                tfidf_file.write(\"\\n\")\n",
    "            line1 = \",\".join( [str(vector_element) for vector_element in features] )\n",
    "            tfidf_file.write(line1+str(\",\")+str(label))\n",
    "            tfidf_file.write('\\n')\n",
    "        \n",
    "    # Read the TFIDF vectors\n",
    "    tfidf_df = pd.read_csv('/home/joao/tfidf.csv',low_memory=False)\n",
    "    X_train = tfidf_df.iloc[:,:-1]\n",
    "    y_train = tfidf_df['label']\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
