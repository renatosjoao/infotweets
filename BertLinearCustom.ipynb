{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "import csv\n",
    "import datetime\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "from gensim.models import TfidfModel\n",
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "import json\n",
    "import logging\n",
    "import math \n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import re\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score, cross_validate\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, confusion_matrix\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import KFold, StratifiedKFold, StratifiedShuffleSplit\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset, TensorDataset, random_split,SequentialSampler, IterableDataset\n",
    "import transformers\n",
    "from transformers import BertTokenizer, BertModel, BertForSequenceClassification, AdamW, BertConfig, get_linear_schedule_with_warmup\n",
    "import warnings\n",
    "\n",
    "nltk.download('words')\n",
    "nltk.download('punkt')\n",
    "stopwords.words('english')\n",
    "\n",
    "logging.basicConfig(level=logging.ERROR)\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "#needs to be GLOBAL\n",
    "words = set(nltk.corpus.words.words())\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "torch.cuda.set_device(1)\n",
    "torch.cuda.current_device()\n",
    "\n",
    "# Specify loss function\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        \n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
    "\n",
    "def format_time(elapsed):\n",
    "    # Round to the nearest second.\n",
    "    elapsed_rounded = int(round((elapsed)))    \n",
    "    # Format as hh:mm:ss\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))\n",
    "\n",
    "def remove_digit(text):\n",
    "    return re.sub(r'\\d+', '', text)\n",
    "\n",
    "def remove_non_english(text):\n",
    "    text = [w for w in nltk.wordpunct_tokenize(text) if w in text or not w.isalpha()]\n",
    "    return ' '.join(text)\n",
    "\n",
    "def remove_special_chars(text):\n",
    "    return re.sub(\"(\\\\d|\\\\W)+\",\" \",text)    \n",
    "\n",
    "def remove_shortwords(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    text = [i for i in tokens if len(i) > 2]\n",
    "    return ' '.join(text)\n",
    "\n",
    "def remove_nonUTF8(data):\n",
    "    return bytes(data, 'utf-8').decode('utf-8', 'ignore')\n",
    "\n",
    "def preprocess(df):\n",
    "    df['sentence'] = df['sentence'].str.replace(r'http(\\S)+', r'')\n",
    "    df['sentence'] = df['sentence'].str.replace(r'http(\\S)+', r'')\n",
    "    df['sentence'] = df['sentence'].str.replace(r'http ...', r'')\n",
    "    df['sentence'] = df['sentence'].str.replace(r'(RT|rt)[ ]*@[ ]*[\\S]+',r'')\n",
    "    df['sentence'] = df['sentence'].str.replace(r'@[\\S]+',r'')\n",
    "    df['sentence'] = df['sentence'].str.replace(r'_[\\S]?',r'')\n",
    "    df['sentence'] = df['sentence'].str.replace(r'[ ]{2, }',r' ')\n",
    "    df['sentence'] = df['sentence'].str.replace(r'&amp;?',r'and')\n",
    "    df['sentence'] = df['sentence'].str.replace(r'&lt;',r'<')\n",
    "    df['sentence'] = df['sentence'].str.replace(r'&gt;',r'>')\n",
    "    df['sentence'] = df['sentence'].str.replace(r'([\\w\\d]+)([^\\w\\d ]+)', r'\\1 \\2')\n",
    "    df['sentence'] = df['sentence'].str.replace(r'([^\\w\\d ]+)([\\w\\d]+)', r'\\1 \\2')\n",
    "    df['sentence'] = df['sentence'].str.lower()\n",
    "    df['sentence'] = df['sentence'].str.strip()\n",
    "    df['sentence'] = df['sentence'].apply(remove_stopwords)\n",
    "    df['sentence'] = df['sentence'].apply(remove_digit)\n",
    "    df['sentence'] = df['sentence'].apply(remove_non_english)\n",
    "    df['sentence'] = df['sentence'].apply(remove_special_chars)\n",
    "    df['sentence'] = df['sentence'].apply(remove_nonUTF8)\n",
    "    df['sentence'] = df['sentence'].str.replace(\"\\'\", \"\")\n",
    "    df['sentence'] = df['sentence'].str.replace(\"\\\"\", \"\")\n",
    "    df['sentence'] = df['sentence'].apply(remove_shortwords)\n",
    "    return df\n",
    "\n",
    "def bow_features(fullsetdf):\n",
    "    df = preprocess(fullsetdf)\n",
    "    #df.drop(df.columns[[0]], axis=1, inplace=True)\n",
    "    # Tokenize the text column to get the new column 'tokenized_text'\n",
    "    df['tokenized_text'] = [simple_preprocess(line, deacc=True) for line in df['sentence']]\n",
    "    vectorizer = TfidfVectorizer(analyzer = 'word', strip_accents= 'ascii',smooth_idf = True, use_idf=True,max_df = 10000, min_df = 5,  stop_words = 'english')\n",
    "    X = vectorizer.fit_transform(df['sentence'])\n",
    "    boW_features = pd.DataFrame(X.toarray(),columns=vectorizer.get_feature_names())\n",
    "    return boW_features\n",
    "\n",
    "def initialize_model(dataloader, epochs=10):\n",
    "    model  = customBertBinaryClassifier()\n",
    "    model.cuda()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "    #optimizer = AdamW(model.parameters(),lr=0.0005,betas=(0.9, 0.999), eps=1e-8,weight_decay=0.01)\n",
    "    #optimizer = AdamW(params, lr=0.001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0.01, amsgrad=False)\n",
    "    total_steps = len(dataloader) * epochs\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer,num_warmup_steps=10000,num_training_steps=total_steps)\n",
    "    return model, optimizer, scheduler\n",
    "\n",
    "def remove_special_chars(text):\n",
    "    return re.sub(\"(\\\\d|\\\\W)+\",\" \",text)\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    return text.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "def remove_shortwords(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    text = [i for i in tokens if len(i) > 2]\n",
    "    return ' '.join(text)\n",
    "\n",
    "def remove_nonUTF8(data):\n",
    "    return bytes(data, 'utf-8').decode('utf-8', 'ignore')\n",
    "\n",
    "def remove_stop_words(text):\n",
    "    #stop_words = set(stopwords.words('english'))\n",
    "    tokens = word_tokenize(text)\n",
    "    text = [i for i in tokens if not i in stop_words]\n",
    "    return ' '.join(text)\n",
    "\n",
    "def stem_words(text):\n",
    "    ps = PorterStemmer() \n",
    "    words = word_tokenize(text)\n",
    "    text = [ps.stem(w) for w in words]    \n",
    "    return ' '.join(text)\n",
    "\n",
    "def remove_digit(text):\n",
    "    return re.sub(r'\\d+', '', text)\n",
    "\n",
    "def remove_non_english(text):\n",
    "    text = [w for w in nltk.wordpunct_tokenize(text) if w in text or not w.isalpha()]\n",
    "    return ' '.join(text)\n",
    "\n",
    "def train_and_evalCV(model, dataset, cv, epochs, batch_size):\n",
    "    training_stats = []\n",
    "    error_stats = []\n",
    "    stats = []\n",
    "    cm = []\n",
    "    total_t0 = time.time()\n",
    "    train_dataloader = DataLoader(dataset,sampler = SequentialSampler(dataset), batch_size = batch_size )\n",
    "    kf = KFold(n_splits=cv)\n",
    "    for train_index, test_index in kf.split(dataset): # For each fold...\n",
    "        train_dataset = torch.utils.data.Subset(dataset,train_index)\n",
    "        val_dataset  = torch.utils.data.Subset(dataset, test_index)\n",
    "        train_dataloader = DataLoader(train_dataset, sampler = SequentialSampler(train_dataset),batch_size = batch_size)\n",
    "        val_dataloader = DataLoader(val_dataset, sampler = SequentialSampler(val_dataset),batch_size = batch_size)    \n",
    "        model, optimizer, scheduler = initialize_model(train_dataloader,epochs)\n",
    "        model.cuda()\n",
    "        for epoch_i in range(0, epochs):  # For each epoch...\n",
    "            #print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "            t0 = time.time()\n",
    "            total_train_loss = 0\n",
    "            model.train()\n",
    "            for step, batch in enumerate(train_dataloader):\n",
    "                b_input_ids, b_attn_mask, hand_features, b_labels= tuple(t.to(device) for t in batch)\n",
    "                model.zero_grad()\n",
    "                logits = model(b_input_ids, b_attn_mask,hand_features)\n",
    "                loss = loss_fn(logits, b_labels)\n",
    "                total_train_loss += loss.item()\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "            avg_train_loss = total_train_loss / len(train_dataloader)\n",
    "            training_time = format_time(time.time() - t0)\n",
    "            training_stats.append({'epoch': epoch_i + 1,'Training Loss': avg_train_loss,'Training Time': training_time})\n",
    "            print({'epoch': epoch_i + 1,'Training Loss': avg_train_loss,'Training Time': training_time})\n",
    "#    sss = StratifiedShuffleSplit(n_splits=cv, test_size=0.5, random_state=0)\n",
    "#    skf = StratifiedKFold(n_splits=cv)\n",
    "        predictions , true_labels = [], []\n",
    "        model.eval()\n",
    "        total_val_loss = 0\n",
    "        for step, batch in enumerate(val_dataloader):\n",
    "            b_input_ids, b_attn_mask, hand_features, b_labels = tuple(t.to(device) for t in batch)\n",
    "            with torch.no_grad():\n",
    "                logits = model(b_input_ids, b_attn_mask, hand_features)\n",
    "            loss = loss_fn(logits, b_labels)\n",
    "            total_val_loss += loss.item()\n",
    "            logits = logits.to('cpu').numpy()\n",
    "            label_ids = b_labels.to('cpu').numpy()\n",
    "            pred_flat =  np.argmax(logits, axis=1).flatten()\n",
    "            labels_flat = label_ids.flatten()\n",
    "            true_labels = np.concatenate((true_labels,labels_flat))\n",
    "            predictions = np.concatenate((predictions,pred_flat))\n",
    "        error_stats.append({'test_index':test_index,'true_labels':true_labels ,'predictions': predictions})\n",
    "        stats.append(precision_recall_fscore_support(true_labels, predictions, average='macro'))\n",
    "        avg_val_loss = total_val_loss / len(val_dataloader)\n",
    "        print({'Validation Loss': avg_val_loss})\n",
    "        print(precision_recall_fscore_support(true_labels, predictions, average='macro'))\n",
    "        print(classification_report(true_labels,predictions))\n",
    "    print()\n",
    "    print(stats)\n",
    "    print()\n",
    "    aggP,aggR,aggF = 0.0,0.0, 0.0\n",
    "    listP = []\n",
    "    listR = []\n",
    "    listF = []\n",
    "    for scores in stats:\n",
    "        aggP+=scores[0]\n",
    "        listP.append(scores[0])\n",
    "        aggR+=scores[1]\n",
    "        listR.append(scores[1])\n",
    "        aggF+=scores[2]\n",
    "        listF.append(scores[2])\n",
    "    avgP = aggP/len(stats)\n",
    "    avgR = aggR/len(stats)\n",
    "    avgF = aggF/len(stats)\n",
    "    print(\"P: {:.3f}, R: {:.3f}, F: {:.3f} (+/- {:.3f}) \".format(np.mean(listP), np.mean(listR), np.mean(listF),np.std(listF)*2/100.0))\n",
    "    #matrix = np.mean(cm, axis=0,dtype=int)\n",
    "    #plt.figure(figsize=(16,14))     \n",
    "    #sns.heatmap(matrix, xticklabels=[0,1], yticklabels=[0,1], annot=True)\n",
    "    #plt.title(\"CONFUSION MATRIX : \")\n",
    "    #plt.ylabel('True Label')\n",
    "    #plt.xlabel('Predicted label')\n",
    "    #plt.savefig('/home/joao/cmatrix.png')\n",
    "    #plt.show()\n",
    "    #print()\n",
    "    #with open('/home/joao/error_stats.txt', 'w') as file:\n",
    "    #    file.write(pickle.dumps(error_stats))\n",
    "    \n",
    "class FullDataset():\n",
    "    def __init__(self,filename,name):\n",
    "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "        if name == 'crisismmd':\n",
    "            self.df = pd.read_csv(filename,delimiter='\\t',quoting=csv.QUOTE_NONE,error_bad_lines=False,encoding='utf-8',lineterminator=\"\\n\")\n",
    "            #self.df = pd.read_csv('/home/joao/crisismmd.ORG.tsv',delimiter='\\t',encoding='utf-8',lineterminator=\"\\n\")\n",
    "            #self.df = pd.read_csv('/home/joao/crisismmd.subset.tsv',delimiter='\\t',encoding='utf-8',lineterminator=\"\\n\")\n",
    "        if name == 'covid':\n",
    "            self.df = pd.read_csv(filename,delimiter='\\t',quoting=csv.QUOTE_NONE,error_bad_lines=False,encoding='utf-8',lineterminator=\"\\n\")\n",
    "            #self.df = pd.read_csv('/home/joao/covid.ORG.tsv',delimiter='\\t',encoding='utf-8',lineterminator=\"\\n\")\n",
    "            #self.df = pd.read_csv('/home/joao/covid.subset.tsv',delimiter='\\t',encoding='utf-8',lineterminator=\"\\n\")\n",
    "        if name ==  'crisislext6':\n",
    "            self.df = pd.read_csv(filename,delimiter='\\t',encoding='utf-8',lineterminator=\"\\n\")\n",
    "            #self.df = pd.read_csv('/home/joao/crisislext6.ORG.tsv',delimiter='\\t',encoding='utf-8',lineterminator=\"\\n\")\n",
    "            #self.df = pd.read_csv('/home/joao/crisislext6.subset.tsv',delimiter='\\t',encoding='utf-8',lineterminator=\"\\n\")\n",
    "        if name == 'crisislext26':\n",
    "            self.df = pd.read_csv(filename,delimiter='\\t',encoding='utf-8',lineterminator=\"\\n\")\n",
    "            #self.df = pd.read_csv('/home/joao/crisislext26.ORG.tsv',delimiter='\\t',encoding='utf-8',lineterminator=\"\\n\")\n",
    "            #self.df = pd.read_csv('/home/joao/crisislext26.subset.tsv',delimiter='\\t',encoding='utf-8',lineterminator=\"\\n\")\n",
    "        #self.df = self.df[['tweet_id','sentence','label']]\n",
    "        self.sentences = self.df['sentence']\n",
    "        #'Unnamed: 0'\n",
    "        self.df.drop(self.df.columns[[0]], axis=1, inplace=True)\n",
    "        self.labels = self.df['label'].values\n",
    "        self.df['sentence'] = self.df['sentence'].str.replace(r'http(\\S)+', r'')\n",
    "        self.df['sentence'] = self.df['sentence'].str.replace(r'http(\\S)+', r'')\n",
    "        self.df['sentence'] = self.df['sentence'].str.replace(r'http ...', r'')\n",
    "        self.df['sentence'] = self.df['sentence'].str.replace(r'(RT|rt)[ ]*@[ ]*[\\S]+',r'')\n",
    "        self.df['sentence'] = self.df['sentence'].str.replace(r'@[\\S]+',r'')\n",
    "        self.df['sentence'] = self.df['sentence'].str.replace(r'_[\\S]?',r'')\n",
    "        self.df['sentence'] = self.df['sentence'].str.replace(r'[ ]{2, }',r' ')\n",
    "        self.df['sentence'] = self.df['sentence'].str.replace(r'&amp;?',r'and')\n",
    "        self.df['sentence'] = self.df['sentence'].str.replace(r'&lt;',r'<')\n",
    "        self.df['sentence'] = self.df['sentence'].str.replace(r'&gt;',r'>')\n",
    "        self.df['sentence'] = self.df['sentence'].str.replace(r'([\\w\\d]+)([^\\w\\d ]+)', r'\\1 \\2')\n",
    "        self.df['sentence'] = self.df['sentence'].str.replace(r'([^\\w\\d ]+)([\\w\\d]+)', r'\\1 \\2')\n",
    "        self.df['sentence'] = self.df['sentence'].str.lower()\n",
    "        self.df['sentence'] = self.df['sentence'].str.strip()\n",
    "        self.df['sentence'] = self.df['sentence'].apply(remove_special_chars)\n",
    "        self.df['sentence'] = self.df['sentence'].apply(remove_digit)\n",
    "        self.df['sentence'] = self.df['sentence'].apply(remove_stopwords)\n",
    "        self.df['sentence'] = self.df['sentence'].str.replace(\"\\'\", \"\")\n",
    "        self.df['sentence'] = self.df['sentence'].str.replace(\"\\\"\", \"\")\n",
    "        self.df['sentence'] = self.df['sentence'].apply(remove_non_english)\n",
    "        self.df['sentence'] = self.df['sentence'].apply(remove_nonUTF8)\n",
    "        self.df['sentence'] = self.df['sentence'].apply(remove_shortwords)\n",
    "        self.hand_crafted_features = self.df[['nchars', 'nwords','bhash','nhash','blink','nlink','bat','nat','brt','bslang','bintj','tlex']]\n",
    "        self.hand_crafted_features_DF = pd.DataFrame(self.hand_crafted_features, columns = ['nchars', 'nwords','bhash','nhash','blink','nlink','bat','nat','brt','bslang','bintj','tlex']).astype(float)\n",
    "        self.maxlen = 80\n",
    "        #if name == 'covid':\n",
    "        #    self.maxlen = 80\n",
    "        #else:\n",
    "        #    for sent in self.sentences:\n",
    "        #        input_ids = self.tokenizer.encode(sent, add_special_tokens=True)\n",
    "        #        self.maxlen = max(self.maxlen, len(input_ids))\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    def __getitem__(self, idx):\n",
    "        sentence = self.df.loc[idx, 'sentence']\n",
    "        label = self.df.loc[idx, 'label']\n",
    "        h_features = self.hand_crafted_features_DF.loc[idx,:]\n",
    "        h_tensor = torch.tensor(h_features).to(device)\n",
    "        tokens = self.tokenizer.tokenize(sentence)\n",
    "        if len(tokens) == 0:\n",
    "            tokens = ['']\n",
    "        encoded_dict = self.tokenizer.encode_plus(tokens, add_special_tokens = True, max_length = self.maxlen, pad_to_max_length = True,return_attention_mask = True)\n",
    "        tokens_ids = encoded_dict['input_ids']\n",
    "        tokens_ids_tensor = torch.tensor(tokens_ids).to(device) #Converting the list to a pytorch tensor\n",
    "        attn_mask = encoded_dict['attention_mask']\n",
    "        attn_mask_tensor = torch.tensor(attn_mask).to(device)\n",
    "        label_tensor = torch.tensor(label).to(device)\n",
    "        return tokens_ids_tensor,attn_mask_tensor,h_tensor,label_tensor\n",
    "\n",
    "class customBertBinaryClassifier(nn.Module):\n",
    "    def __init__(self, B_in = 768, B_out = 768, H_in = 12, H_out = 12):\n",
    "        super(customBertBinaryClassifier, self).__init__()\n",
    "        self.bert_layer = BertModel.from_pretrained('bert-base-uncased')\n",
    "        C_in = B_out + H_out \n",
    "        C_out = 2\n",
    "        self.linear_B = nn.Linear(B_in, B_out)\n",
    "        self.linear_H = nn.Linear(H_in, H_out)\n",
    "        self.drop = nn.Dropout(p=0.5)\n",
    "        #self.norm = nn.LayerNorm(H_in,eps = 1e-12, elementwise_affine = True)\n",
    "        self.classifier = nn.Linear(C_in, C_out)\n",
    "    def forward(self, seq, attn_masks, hand_features):\n",
    "        outputs = self.bert_layer(input_ids=seq,attention_mask=attn_masks)\n",
    "        last_hidden_state_cls = outputs[0][:, 0, :]\n",
    "        logits_B = self.drop(last_hidden_state_cls.float())\n",
    "        logits_B = self.linear_B(logits_B.float())\n",
    "        #logits_H = self.norm(hand_features.float())\n",
    "        logits_H = self.linear_H(hand_features.float())\n",
    "        cat_features = torch.cat([logits_B.float(),logits_H.float()], dim=1)\n",
    "        #feat = self.drop(cat_features.float())\n",
    "        output = self.classifier(cat_features.float())\n",
    "        return output\n",
    "\n",
    "   \n",
    "def main():\n",
    "    datasets = ['covid', 'crisislext6', 'crisislext26', 'crisismmd']\n",
    "    #datasets = ['covid']\n",
    "    for data in datasets :\n",
    "        print(\"=== {} ===\".format(data))\n",
    "        if data == 'covid':\n",
    "            dataset = FullDataset('/home/joao/covid.ORG.tsv','covid')\n",
    "            dataset = FullDataset('/home/joao/covid.subset.tsv','covid')\n",
    "            #fullsetdf = pd.read_csv('/home/joao/covid.ORG.tsv',delimiter='\\t',encoding='utf-8',lineterminator=\"\\n\")\n",
    "            #subsetdf = pd.read_csv('/home/joao/covid.subset.tsv',delimiter='\\t',encoding='utf-8',lineterminator=\"\\n\")\n",
    "        if data ==  'crisislext6':\n",
    "            dataset = FullDataset('/home/joao/crisislext6.ORG.tsv','crisislext6')\n",
    "            dataset = FullDataset('/home/joao/crisislext6.subset.tsv','crisislext6')\n",
    "            #fullsetdf = pd.read_csv('/home/joao/crisislext6.ORG.tsv',delimiter='\\t',encoding='utf-8',lineterminator=\"\\n\")\n",
    "            #subsetdf = pd.read_csv('/home/joao/crisislext6.subset.tsv',delimiter='\\t',encoding='utf-8',lineterminator=\"\\n\")\n",
    "        if data == 'crisislext26':\n",
    "            #dataset = FullDataset('/home/joao/crisislext26.ORG.tsv','crisislext26')\n",
    "            dataset = FullDataset('/home/joao/crisislext26.ORG.tsv','crisislext26')\n",
    "            dataset = FullDataset('/home/joao/crisislext26.subset.tsv','crisislext26')\n",
    "            #fullsetdf = pd.read_csv('/home/joao/crisislext26.ORG.tsv',delimiter='\\t',encoding='utf-8',lineterminator=\"\\n\")\n",
    "            #subsetdf = pd.read_csv('/home/joao/crisislext26.subset.tsv',delimiter='\\t',encoding='utf-8',lineterminator=\"\\n\")\n",
    "        if data == 'crisismmd':\n",
    "            dataset = FullDataset('/home/joao/crisismmd.ORG.tsv','crisismmd')\n",
    "            dataset = FullDataset('/home/joao/crisismmd.subset.tsv','crisismmd')\n",
    "            #fullsetdf = pd.read_csv('/home/joao/crisismmd.ORG.tsv',delimiter='\\t',quoting=csv.QUOTE_NONE,error_bad_lines=False, encoding='utf-8',lineterminator=\"\\n\")\n",
    "            #subsetdf = pd.read_csv('/home/joao/crisismmd.subset.tsv',delimiter='\\t',quoting=csv.QUOTE_NONE,error_bad_lines=False,encoding='utf-8',lineterminator=\"\\n\")\n",
    "batch_size = 16\n",
    "n_epochs= 20 \n",
    "cv = 10\n",
    "#complete data set\n",
    "dataloader = DataLoader(dataset,sampler = SequentialSampler(dataset), batch_size = batch_size )\n",
    "set_seed(42)    # Set seed for reproducibility\n",
    "model, optimizer, scheduler = initialize_model(dataloader,epochs=batch_size)\n",
    "train_and_evalCV(model, dataset, cv=cv, epochs=n_epochs, batch_size = batch_size)\n",
    "###\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "### SUBSET containing USER based features\n",
    "###\n",
    "class SubsetDataset():\n",
    "    def __init__(self,filename,name):\n",
    "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "        if name == 'crisismmd':\n",
    "            self.df = pd.read_csv(filename,delimiter='\\t',quoting=csv.QUOTE_NONE,error_bad_lines=False,encoding='utf-8',lineterminator=\"\\n\")\n",
    "        if name == 'covid':\n",
    "            self.df = pd.read_csv(filename,delimiter='\\t',quoting=csv.QUOTE_NONE,error_bad_lines=False,encoding='utf-8',lineterminator=\"\\n\")\n",
    "        if name ==  'crisislext6':\n",
    "            self.df = pd.read_csv(filename,delimiter='\\t',encoding='utf-8',lineterminator=\"\\n\")\n",
    "        if name == 'crisislext26':\n",
    "            self.df = pd.read_csv(filename,delimiter='\\t',encoding='utf-8',lineterminator=\"\\n\")\n",
    "        self.sentences = self.df['sentence']\n",
    "        #'Unnamed: 0'\n",
    "        self.df.drop(self.df.columns[[0]], axis=1, inplace=True)\n",
    "        self.labels = self.df['label'].values\n",
    "        self.df['sentence'] = self.df['sentence'].str.replace(r'http(\\S)+', r'')\n",
    "        self.df['sentence'] = self.df['sentence'].str.replace(r'http(\\S)+', r'')\n",
    "        self.df['sentence'] = self.df['sentence'].str.replace(r'http ...', r'')\n",
    "        self.df['sentence'] = self.df['sentence'].str.replace(r'(RT|rt)[ ]*@[ ]*[\\S]+',r'')\n",
    "        self.df['sentence'] = self.df['sentence'].str.replace(r'@[\\S]+',r'')\n",
    "        self.df['sentence'] = self.df['sentence'].str.replace(r'_[\\S]?',r'')\n",
    "        self.df['sentence'] = self.df['sentence'].str.replace(r'[ ]{2, }',r' ')\n",
    "        self.df['sentence'] = self.df['sentence'].str.replace(r'&amp;?',r'and')\n",
    "        self.df['sentence'] = self.df['sentence'].str.replace(r'&lt;',r'<')\n",
    "        self.df['sentence'] = self.df['sentence'].str.replace(r'&gt;',r'>')\n",
    "        self.df['sentence'] = self.df['sentence'].str.replace(r'([\\w\\d]+)([^\\w\\d ]+)', r'\\1 \\2')\n",
    "        self.df['sentence'] = self.df['sentence'].str.replace(r'([^\\w\\d ]+)([\\w\\d]+)', r'\\1 \\2')\n",
    "        self.df['sentence'] = self.df['sentence'].str.lower()\n",
    "        self.df['sentence'] = self.df['sentence'].str.strip()\n",
    "        self.df['sentence'] = self.df['sentence'].apply(remove_special_chars)\n",
    "        self.df['sentence'] = self.df['sentence'].apply(remove_digit)\n",
    "        self.df['sentence'] = self.df['sentence'].apply(remove_stopwords)\n",
    "        self.df['sentence'] = self.df['sentence'].str.replace(\"\\'\", \"\")\n",
    "        self.df['sentence'] = self.df['sentence'].str.replace(\"\\\"\", \"\")\n",
    "        self.df['sentence'] = self.df['sentence'].apply(remove_non_english)\n",
    "        self.df['sentence'] = self.df['sentence'].apply(remove_nonUTF8)\n",
    "        self.df['sentence'] = self.df['sentence'].apply(remove_shortwords)\n",
    "        self.hand_crafted_features = self.df[['nchars', 'nwords','bhash','nhash','blink','nlink','bat','nat','brt','bslang','bintj','tlex','usr_vrf','num_followers','num_friends','num_tweets']]\n",
    "        self.hand_crafted_features_DF = pd.DataFrame(self.hand_crafted_features)\n",
    "        self.maxlen = 0\n",
    "        if name == 'covid':\n",
    "            self.maxlen = 256\n",
    "        else:\n",
    "            for sent in self.sentences:\n",
    "                input_ids = self.tokenizer.encode(sent, add_special_tokens=True)\n",
    "                self.maxlen = max(self.maxlen, len(input_ids))\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    def __getitem__(self, idx):\n",
    "        sentence = self.df.loc[idx, 'sentence']\n",
    "        label = self.df.loc[idx, 'label']\n",
    "        h_features = self.hand_crafted_features_DF.loc[idx,:]\n",
    "        h_tensor = torch.tensor(h_features).to(device)\n",
    "        tokens = self.tokenizer.tokenize(sentence)\n",
    "        if len(tokens) == 0:\n",
    "            tokens = ['']\n",
    "        encoded_dict = self.tokenizer.encode_plus(tokens, add_special_tokens = True, max_length = self.maxlen, pad_to_max_length = True,return_attention_mask = True)\n",
    "        tokens_ids = encoded_dict['input_ids']\n",
    "        tokens_ids_tensor = torch.tensor(tokens_ids).to(device)\n",
    "        attn_mask = encoded_dict['attention_mask']\n",
    "        attn_mask_tensor = torch.tensor(attn_mask).to(device)\n",
    "        label_tensor = torch.tensor(label).to(device)\n",
    "        return tokens_ids_tensor,attn_mask_tensor,h_tensor,label_tensor\n",
    "    \n",
    "class customBertBinaryClassifier(nn.Module):\n",
    "    def __init__(self, B_in = 768, B_out = 768, H_in = 16, H_out = 16):\n",
    "        super(customBertBinaryClassifier, self).__init__()\n",
    "        self.bert_layer = BertModel.from_pretrained('bert-base-uncased')\n",
    "        C_in = B_out + H_out \n",
    "        C_out = 2\n",
    "        self.linear_B = nn.Linear(B_in, B_out)\n",
    "        self.linear_H = nn.Linear(H_in, H_out)\n",
    "        self.drop = nn.Dropout(p=0.1)\n",
    "        self.norm = nn.LayerNorm(H_in,eps = 1e-12, elementwise_affine = True)\n",
    "        self.classifier = nn.Linear(C_in, C_out)\n",
    "    def forward(self, seq, attn_masks, hand_features):\n",
    "        outputs = self.bert_layer(input_ids=seq,attention_mask=attn_masks)\n",
    "        last_hidden_state_cls = outputs[0][:, 0, :]\n",
    "        logits_B = self.drop(last_hidden_state_cls.float())\n",
    "        logits_B = self.linear_B(logits_B)\n",
    "        logits_H = self.norm(hand_features.float())\n",
    "        logits_H = self.linear_H(logits_H)\n",
    "        cat_features = torch.cat([logits_B.float(),logits_H.float()], dim=1)\n",
    "        #feat = self.drop(cat_features.float())\n",
    "        output = self.classifier(cat_features.float())\n",
    "        return output\n",
    "    \n",
    "def main():\n",
    "    datasets = ['covid', 'crisislext6', 'crisislext26', 'crisismmd']\n",
    "    #datasets = ['covid']\n",
    "    for data in datasets :\n",
    "        print(\"=== {} ===\".format(data))\n",
    "        if data == 'covid':\n",
    "            dataset = SubsetDataset('/home/joao/covid.subset.tsv','covid')\n",
    "            #fullsetdf = pd.read_csv('/home/joao/covid.ORG.tsv',delimiter='\\t',encoding='utf-8',lineterminator=\"\\n\")\n",
    "            #subsetdf = pd.read_csv('/home/joao/covid.subset.tsv',delimiter='\\t',encoding='utf-8',lineterminator=\"\\n\")\n",
    "        if data ==  'crisislext6':\n",
    "            dataset = SubsetDataset('/home/joao/crisislext6.subset.tsv','crisislext6')\n",
    "            #fullsetdf = pd.read_csv('/home/joao/crisislext6.ORG.tsv',delimiter='\\t',encoding='utf-8',lineterminator=\"\\n\")\n",
    "            #subsetdf = pd.read_csv('/home/joao/crisislext6.subset.tsv',delimiter='\\t',encoding='utf-8',lineterminator=\"\\n\")\n",
    "        if data == 'crisislext26':\n",
    "            dataset = SubsetDataset('/home/joao/crisislext26.subset.tsv','crisislext26')\n",
    "            #fullsetdf = pd.read_csv('/home/joao/crisislext26.ORG.tsv',delimiter='\\t',encoding='utf-8',lineterminator=\"\\n\")\n",
    "            #subsetdf = pd.read_csv('/home/joao/crisislext26.subset.tsv',delimiter='\\t',encoding='utf-8',lineterminator=\"\\n\")\n",
    "        if data == 'crisismmd':\n",
    "            dataset = SubsetDataset('/home/joao/crisismmd.subset.tsv','crisismmd')\n",
    "            #fullsetdf = pd.read_csv('/home/joao/crisismmd.ORG.tsv',delimiter='\\t',quoting=csv.QUOTE_NONE,error_bad_lines=False, encoding='utf-8',lineterminator=\"\\n\")\n",
    "            #subsetdf = pd.read_csv('/home/joao/crisismmd.subset.tsv',delimiter='\\t',quoting=csv.QUOTE_NONE,error_bad_lines=False,encoding='utf-8',lineterminator=\"\\n\")\n",
    "        batch_size = 16\n",
    "        n_epochs=10\n",
    "        cv =10\n",
    "        #complete data set\n",
    "        dataloader = DataLoader(dataset,sampler = SequentialSampler(dataset), batch_size = batch_size )\n",
    "        set_seed(42)    # Set seed for reproducibility\n",
    "        model, optimizer, scheduler = initialize_model(dataloader,epochs=batch_size)\n",
    "        train_and_evalCV(model, dataset, cv=cv, epochs=n_epochs, batch_size = batch_size)\n",
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
