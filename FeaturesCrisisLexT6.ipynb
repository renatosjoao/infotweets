{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading \n",
      "Index(['tweet id', ' tweet', ' label'], dtype='object')\n",
      "\n",
      "Number of sentences in the original dataset: 10,008\n",
      "\n",
      "Index(['sentence', 'label'], dtype='object')\n",
      "1    6138\n",
      "0    3870\n",
      "Name: label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score, train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import make_moons, make_circles, make_classification\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, confusion_matrix\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "import tensorflow as tf\n",
    "import torch \n",
    "from torch.utils.data import DataLoader, Dataset, TensorDataset\n",
    "from torch.utils.data.sampler import SequentialSampler\n",
    "import transformers as ppb \n",
    "from transformers import BertTokenizer\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "# Loading the complete dataset into a pandas dataframe.\n",
    "print(\"Loading \")\n",
    "#df = pd.read_csv(\"/home/renato/Datasets/CrisisLexT6-v1.0/CrisisLexT6/crisisLexT6.csv\", encoding='utf-8')\n",
    "#df = pd.read_csv(\"/home/renato/Datasets/CrisisLexT6-v1.0/CrisisLexT6/2012_Sandy_Hurricane/2012_Sandy_Hurricane-ontopic_offtopic.csv\", encoding='utf-8')\n",
    "\n",
    "df = pd.read_csv(\"/home/joao/crisisLexT6.csv\", encoding='utf-8')\n",
    "\n",
    "print()\n",
    "print('Number of sentences in the original dataset: {:,}\\n'.format(df.shape[0]))\n",
    "\n",
    "#Relabelling the columns titles to remove white spaces\n",
    "df = df.rename(columns={' tweet': 'sentence'})\n",
    "df = df.rename(columns={' label': 'label'})\n",
    "\n",
    "df['label'].replace('on-topic', 1)\n",
    "df['label'] = df['label'].replace('on-topic', 1)\n",
    "\n",
    "df['label'].replace('off-topic', 0)\n",
    "df['label'] = df['label'].replace('off-topic', 0)\n",
    "\n",
    "\n",
    "labels = df['label'].values\n",
    "sentences = df['sentence']\n",
    "\n",
    "\n",
    "#Dropping useless columns as I will only be using the tweet text and the corresponding label\n",
    "df = df[['sentence','label']]\n",
    "print(df.keys())\n",
    "print(df['label'].value_counts())\n",
    "\n",
    "df.head(5)\n",
    "\n",
    "\n",
    "def lexical_diversity(text):\n",
    "    return len(set(text.split())) / len(text.split()) \n",
    "\n",
    "#Returns the number of characters in a string.\n",
    "df['nchars'] = df['sentence'].str.len()\n",
    "\n",
    "#Returns the number of words in a string.\n",
    "df['nwords'] = df['sentence'].str.split().str.len()\n",
    "\n",
    "# Checks whether the sentence contains # hashtags\n",
    "df['bhash'] = df[\"sentence\"].str.contains(pat = '#',flags=re.IGNORECASE, regex = True).astype(int) \n",
    "\n",
    "# Count occurrences of #\n",
    "df['nhash'] = df[\"sentence\"].str.count('#') \n",
    "\n",
    "# Check whether the sentence contains URLs\n",
    "df['blink']  = df[\"sentence\"].str.contains(pat = 'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', flags=re.IGNORECASE, regex = True) .astype(int)\n",
    "\n",
    "# Count occurrences of URLs\n",
    "df['nlink'] = df[\"sentence\"].str.count(pat = 'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', flags=re.IGNORECASE) \n",
    "\n",
    "#Checks whether the sentence contains @\n",
    "df['bat'] = df[\"sentence\"].str.contains(pat = '@',flags=re.IGNORECASE, regex = True).astype(int) \n",
    "\n",
    "#Count occurrences of  @\n",
    "df['nat'] = df[\"sentence\"].str.count(pat = '@') \n",
    "\n",
    "#Checks whether the sentence has retweet or not  \n",
    "df['rt'] = df[\"sentence\"].str.contains(pat = '@rt|rt@',flags=re.IGNORECASE, regex = True).astype(int) \n",
    "\n",
    "# Checks whether the sentence contains phone number\n",
    "#df['phone'] = df[\"sentence\"].str.contains(pat = '\\(?([0-9]{3})\\)?([ .-]?)([0-9]{3})\\2([0-9]{4})',flags=re.IGNORECASE, regex = True).astype(int) \n",
    "\n",
    "\n",
    "df['dlex'] = df[\"sentence\"].apply(lexical_diversity)\n",
    "\n",
    "# Lowering case\n",
    "df[\"sentence\"] = df[\"sentence\"].str.lower()\n",
    "\n",
    "# List of  US slangs.\n",
    "slangList = ['ASAP','BBIAB','BBL','BBS','BF','BFF','BFFL','BRB','CYA','DS','FAQ','FB','FITBLR','FLBP','FML','FTFY','FTW','FYI','G2G','GF','GR8','GTFO','HBIC','HML','HRU','HTH','IDK','IGHT','IMO','IMHO','IMY','IRL','ISTG','JK','JMHO','KTHX','L8R','LMAO','LMFAO','LMK','LOL','MWF','NM','NOOB','NP','NSFW','OOAK','OFC','OMG','ORLY','OTOH','RN','ROFL','RUH','SFW','SOML','SOZ','STFU','TFTI','TIL','TMI','TTFN','TTYL','TWSS','U','W/','WB','W/O','WYD','WTH','WTF','WYM','WYSIWYG','Y','YMMV','YW','YWA']\n",
    "slangList = [x.lower() for x in slangList]\n",
    "\n",
    "#happy emojis\n",
    "happy_emojis = [':\\)', ';\\)', '\\(:']\n",
    "\n",
    "#sad emojis\n",
    "sad_emojis = [':\\(', ';\\(', '\\):']\n",
    "\n",
    "punctuation = ['.',',','...','?','!',':',';']    \n",
    "#','-','+','*','_','=','/','','%',' &','{','}','[',']','(',')','\n",
    "\n",
    "#Checks if the sentence contains slang\n",
    "mask = df.iloc[:, 0].str.contains(r'\\b(?:{})\\b'.format('|'.join(slangList)))\n",
    "df1 = df[~mask]\n",
    "df['slang'] = mask.astype(int) \n",
    "\n",
    "#Checks if the sentence contains happy emojis\n",
    "mask = df.iloc[:, 0].str.contains(r'\\b(?:{})\\b'.format('|'.join(happy_emojis)), regex = True)\n",
    "df1 = df[~mask]\n",
    "df['hemojis'] = mask.astype(int) \n",
    "\n",
    "#Checks if the sentence contains happy emojis\n",
    "mask = df.iloc[:, 0].str.contains(r'\\b(?:{})\\b'.format('|'.join(sad_emojis)), regex = True)\n",
    "df1 = df[~mask]\n",
    "df['semojis'] = mask.astype(int) \n",
    "\n",
    "features =  df[['nchars', 'nwords','bhash','nhash','blink','nlink','bat','nat','rt','slang','dlex']]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"None of [Index(['nchars', 'nwords', 'bhash', 'nhash', 'blink', 'nlink', 'bat', 'nat',\\n       'rt', 'slang', 'dlex'],\\n      dtype='object')] are in the [columns]\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-ec8afe8e9241>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'nchars'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'nwords'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'bhash'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'nhash'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'blink'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'nlink'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'bat'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'nat'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'rt'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'slang'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'dlex'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#features = df\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#df[['label']]  = df[['label']].replace(to_replace=r'on-topic', value='1', regex=True)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2804\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2805\u001b[0m                 \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2806\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_listlike_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraise_missing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2807\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2808\u001b[0m         \u001b[0;31m# take() does not accept boolean indexers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_get_listlike_indexer\u001b[0;34m(self, key, axis, raise_missing)\u001b[0m\n\u001b[1;32m   1551\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1552\u001b[0m         self._validate_read_indexer(\n\u001b[0;32m-> 1553\u001b[0;31m             \u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_axis_number\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraise_missing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mraise_missing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1554\u001b[0m         )\n\u001b[1;32m   1555\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_validate_read_indexer\u001b[0;34m(self, key, indexer, axis, raise_missing)\u001b[0m\n\u001b[1;32m   1638\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmissing\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1639\u001b[0m                 \u001b[0maxis_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_axis_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1640\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"None of [{key}] are in the [{axis_name}]\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1641\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1642\u001b[0m             \u001b[0;31m# We (temporarily) allow for some missing keys with .loc, except in\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"None of [Index(['nchars', 'nwords', 'bhash', 'nhash', 'blink', 'nlink', 'bat', 'nat',\\n       'rt', 'slang', 'dlex'],\\n      dtype='object')] are in the [columns]\""
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "                        \n",
    "### BERT\n",
    "\n",
    "                        #### Doing all the text pre processing\n",
    "        \n",
    "\n",
    "\n",
    "# Get the GPU device name.\n",
    "device_name = tf.test.gpu_device_name()\n",
    "# The device name should look like the following:\n",
    "if device_name == '/device:GPU:0':\n",
    "    print('Found GPU at: {}'.format(device_name))\n",
    "else:\n",
    "    raise SystemError('GPU device not found')\n",
    "    \n",
    "\n",
    "# If there's a GPU available...\n",
    "if torch.cuda.is_available():    \n",
    "    device = torch.device(\"cuda\")\n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")   \n",
    "\n",
    "#labels = df['label']\n",
    "sentences = df['sentence']\n",
    "sentences.head()\n",
    "\n",
    "### Remove URL, RT, mention(@)\n",
    "df.ProcessedText = df.sentence.str.replace(r'http(\\S)+', r'')\n",
    "df.ProcessedText = df.ProcessedText.str.replace(r'http ...', r'')\n",
    "df.ProcessedText[df.ProcessedText.str.contains(r'http')]\n",
    "df.ProcessedText = df.ProcessedText.str.replace(r'(RT|rt)[ ]*@[ ]*[\\S]+',r'')\n",
    "df.ProcessedText[df.ProcessedText.str.contains(r'RT[ ]?@')]\n",
    "df.ProcessedText = df.ProcessedText.str.replace(r'@[\\S]+',r'')\n",
    "df.ProcessedText = df.ProcessedText.str.replace(r'_[\\S]?',r'')\n",
    "\n",
    "#Remove extra space\n",
    "df.ProcessedText = df.ProcessedText.str.replace(r'[ ]{2, }',r' ')\n",
    "\n",
    "#Removing &, < and >\n",
    "df.ProcessedText = df.ProcessedText.str.replace(r'&amp;?',r'and')\n",
    "\n",
    "#Remove extra space\n",
    "df.ProcessedText = df.ProcessedText.str.replace(r'&lt;',r'<')\n",
    "df.ProcessedText = df.ProcessedText.str.replace(r'&gt;',r'>')\n",
    "\n",
    "#Insert space between words and punctuation marks\n",
    "df.ProcessedText = df.ProcessedText.str.replace(r'([\\w\\d]+)([^\\w\\d ]+)', r'\\1 \\2')\n",
    "df.ProcessedText = df.ProcessedText.str.replace(r'([^\\w\\d ]+)([\\w\\d]+)', r'\\1 \\2')\n",
    "\n",
    "#Lowercased and strip\n",
    "df.ProcessedText = df.ProcessedText.str.lower()\n",
    "df.ProcessedText = df.ProcessedText.str.strip()\n",
    "\n",
    "sentences = df.ProcessedText\n",
    "\n",
    "# Load the BERT tokenizer.\n",
    "print('Loading BERT tokenizer...')\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "\n",
    "max_len = 0\n",
    "# For every sentence...\n",
    "for sent in sentences:\n",
    "    # Tokenize the text and add `[CLS]` and `[SEP]` tokens.\n",
    "    input_ids = tokenizer.encode(sent, add_special_tokens=True)\n",
    "    # Update the maximum sentence length.\n",
    "    max_len = max(max_len, len(input_ids))\n",
    "    \n",
    "    \n",
    "print('Max sentence length: ', max_len)\n",
    "\n",
    "# Load pretrained model/tokenizer\n",
    "model_class, tokenizer_class, pretrained_weights = (ppb.BertModel, ppb.BertTokenizer, 'bert-base-uncased')\n",
    "model = model_class.from_pretrained(pretrained_weights)\n",
    "\n",
    "# Tell pytorch to run this model on the GPU.\n",
    "model.cuda()\n",
    "model.to(device)\n",
    "\n",
    "# Tokenization\n",
    "tokenized = sentences.apply((lambda x: tokenizer.encode(x,add_special_tokens=True)))\n",
    "#Padding\n",
    "padded = np.array([i + [0]*(max_len-len(i)) for i in tokenized.values])\n",
    "# Masking\n",
    "attention_mask = np.where(padded != 0, 1, 0)\n",
    "\n",
    "## Now deep learning !\n",
    "####  DEEP LEARNING\n",
    "input_ids = torch.tensor(padded).to(device)\n",
    "attention_mask = torch.tensor(attention_mask).to(device)\n",
    "#labels = torch.tensor(labels).to(device)\n",
    "labels = torch.tensor(df[\"label\"].values).to(device)\n",
    "\n",
    "dataset = TensorDataset(input_ids, attention_mask, labels)\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "# Create the DataLoaders for our training and validation sets.\n",
    "# We'll take training samples in random order. \n",
    "dataloader = DataLoader(dataset, sampler = SequentialSampler(dataset),batch_size = batch_size)\n",
    "\n",
    "dfLabels = pd.DataFrame()\n",
    "dfFeatures = pd.DataFrame()\n",
    "\n",
    "# For each batch of training data...\n",
    "for batch in dataloader:\n",
    "    with torch.no_grad():\n",
    "        # Unpack this training batch from our dataloader. \n",
    "        # As we unpack the batch, we'll also copy each tensor to the GPU using the `to` method.\n",
    "        #\n",
    "        # `batch` contains three pytorch tensors:\n",
    "        #   [0]: input ids \n",
    "        #   [1]: attention masks\n",
    "        #   [2]: labels \n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "        # Always clear any previously calculated gradients before performing a\n",
    "        # backward pass. PyTorch doesn't do this automatically because \n",
    "        # accumulating the gradients is \"convenient while training RNNs\". \n",
    "        # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n",
    "        #model.zero_grad() \n",
    "        last_hidden_states = model(b_input_ids,attention_mask = b_input_mask)\n",
    "        features = last_hidden_states[0][:,0,:]#Let's slice only the part of the output that we need. That is the output corresponding the first token of each sentence.  The way BERT does sentence classification, is that it adds a token called [CLS] (for classification) at the beginning of every sentence. The output corresponding to that token can be thought of as an embedding for the entire sentence.\n",
    "        features = features.cpu().detach().numpy()        \n",
    "        labels = b_labels.cpu().detach().numpy()\n",
    "        dfLabels = dfLabels.append(pd.DataFrame(labels),ignore_index = True)\n",
    "        dfFeatures = dfFeatures.append(pd.DataFrame(features),ignore_index = True)\n",
    "        \n",
    "# Model 2. Train and Test Split \n",
    "# The output from BERT is going to be input to SKLEARN\n",
    "\n",
    "train_features, test_features, train_labels, test_labels = train_test_split(dfFeatures, dfLabels,test_size=0.33, random_state=42)\n",
    "\n",
    "combine_df = pd.concat([dfFeatures, features],axis=1)\n",
    "\n",
    "train_features, test_features, train_labels, test_labels = train_test_split(combine_df, dfLabels,test_size=0.33, random_state=42)\n",
    "\n",
    "train_features, test_features, train_labels, test_labels = train_test_split(features, labels, test_size=0.33, random_state=42)\n",
    "\n",
    "# Now we are going to train Logistic Regression model\n",
    "# We now train the LogisticRegression model. If you've chosen to do the gridsearch, you can plug the value of C into the model declaration (e.g. LogisticRegression(C=5.2)).\n",
    "\n",
    "lr_clf = LogisticRegression()\n",
    "lr_clf.fit(train_features, train_labels)\n",
    "\n",
    "dt_clf = DecisionTreeClassifier()\n",
    "dt_clf.fit(train_features, train_labels)\n",
    "\n",
    "svm_clf = svm.SVC(gamma=0.001, C=100.)\n",
    "svm_clf.fit(train_features, train_labels)\n",
    "\n",
    "rf_clf = RandomForestClassifier()\n",
    "rf_clf.fit(train_features, train_labels)\n",
    "\n",
    "ab_clf = AdaBoostClassifier(n_estimators=100, random_state=0)\n",
    "ab_clf.fit(train_features, train_labels)\n",
    "\n",
    "nb_clf = GaussianNB()\n",
    "nb_clf.fit(train_features, train_labels)\n",
    "\n",
    "nn_clf = MLPClassifier(random_state=1, max_iter=300)\n",
    "nn_clf.fit(train_features, train_labels)\n",
    "\n",
    "#Evaluating Model #2\n",
    "#So how well does our model do in classifying sentences? One way is to check the accuracy against the testing dataset:\n",
    "lr_clf.score(test_features, test_labels)\n",
    "dt_clf.score(test_features, test_labels)\n",
    "rf_clf.score(test_features, test_labels)\n",
    "ab_clf.score(test_features, test_labels)\n",
    "nb_clf.score(test_features, test_labels)\n",
    "nn_clf.score(test_features, test_labels)\n",
    "svm_clf.score(test_features, test_labels)\n",
    "\n",
    "y_pred = lr_clf.predict(test_features)\n",
    "y_pred = dt_clf.predict(test_features)\n",
    "y_pred = rf_clf.predict(test_features)\n",
    "y_pred = ab_clf.predict(test_features)\n",
    "y_pred = nb_clf.predict(test_features)\n",
    "y_pred = nn_clf.predict(test_features)\n",
    "y_pred = svm_clf.predict(test_features)\n",
    "\n",
    "print('Accuracy:', accuracy_score(test_labels, y_pred))\n",
    "print('F1 score:', f1_score(test_labels, y_pred, average='macro'))\n",
    "print('Recall:', recall_score(test_labels, y_pred, average='macro'))\n",
    "print('Precision:', precision_score(test_labels, y_pred, average='macro'))\n",
    "print('\\n clasification report:\\n', classification_report(test_labels,y_pred))\n",
    "print('\\n confussion matrix:\\n',confusion_matrix(test_labels, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "# Logistic regression get importance\n",
    "importance = lr_clf.coef_[0]\n",
    "# summarize feature importance\n",
    "for i,v in enumerate(importance):\n",
    "    print('Feature: %0d, Score: %.5f' % (i,v))\n",
    "\n",
    "importance = dt_clf.feature_importances_\n",
    "# summarize feature importance\n",
    "for i,v in enumerate(importance):\n",
    "    print('Feature: %0d, Score: %.5f' % (i,v))\n",
    "    \n",
    "# get importance\n",
    "importance = rf_clf.feature_importances_\n",
    "# summarize feature importance\n",
    "for i,v in enumerate(importance):\n",
    "    print('Feature: %0d, Score: %.5f' % (i,v))\n",
    "    \n",
    "    \n",
    "results = permutation_importance(rf_clf, test_features, test_labels, n_repeats=10,random_state=42, n_jobs=2,scoring='accuracy')\n",
    "# get importance\n",
    "importance = results.importances_mean\n",
    "# summarize feature importance\n",
    "for i,v in enumerate(importance):\n",
    "    print('Feature: %0d, Score: %.5f' % (i,v))\n",
    "\n",
    "    \n",
    "\n",
    "    ### RF feature importance###\n",
    "importances = rf_clf.feature_importances_\n",
    "std = np.std([tree.feature_importances_ for tree in rf_clf.estimators_],axis=0)\n",
    "indices = np.argsort(importances)[::-1]\n",
    "for f in range(train_features.shape[1]):\n",
    "    print(\"%d. feature %d (%f)\" % (f + 1, indices[f], importances[indices[f]]))\n",
    "# Plot the impurity-based feature importances of the forest\n",
    "plt.figure()\n",
    "plt.title(\"Feature importances\")\n",
    "plt.bar(range(X.shape[1]), importances[indices],color=\"r\", yerr=std[indices], align=\"center\")\n",
    "plt.xticks(range(X.shape[1]), indices)\n",
    "plt.xlim([-1, X.shape[1]])\n",
    "plt.show()\n",
    "\n",
    "\n",
    "result = permutation_importance(rf, X_test, y_test, n_repeats=10,random_state=42, n_jobs=2)\n",
    "sorted_idx = result.importances_mean.argsort()\n",
    "fig, ax = plt.subplots()\n",
    "ax.boxplot(result.importances[sorted_idx].T,vert=False, labels=X_test.columns[sorted_idx])\n",
    "ax.set_title(\"Permutation Importances (test set)\")\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dfFeatures' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-828a7762056e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;31m# The output from BERT is going to be input to SKLEARN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m \u001b[0mtrain_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdfFeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdfLabels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.33\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;31m# Now we are going to train Logistic Regression model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dfFeatures' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "## We try to combine the features \n",
    "#import pandas as pd\n",
    "#\n",
    "#df1 = pd.DataFrame({'A': ['A0', 'A1'],\n",
    "#                    'B': ['B0', 'B1'],\n",
    "#                    'C': ['A4', 'A5'],\n",
    "#                   })\n",
    "\n",
    "\n",
    "#df2 = pd.DataFrame({'D': ['B4', 'B5']})\n",
    "#result = pd.concat([df1, df2],axis=1)\n",
    "#result\n",
    "#df = pd.DataFrame(data=sentences, columns=[\"features\"])\n",
    "\n",
    "\n",
    "#metrics.accuracy_score(y_true, y_pred, \\*[, …]) Accuracy classification score.\n",
    "#metrics.auc(x, y) Compute Area Under the Curve (AUC) using the trapezoidal rule\n",
    "#metrics.average_precision_score(y_true, …) Compute average precision (AP) from prediction scores\n",
    "#metrics.balanced_accuracy_score(y_true, …) Compute the balanced accuracy\n",
    "#metrics.brier_score_loss(y_true, y_prob, \\*) Compute the Brier score.\n",
    "#metrics.classification_report(y_true, y_pred, \\*) Build a text report showing the main classification metrics.\n",
    "#metrics.cohen_kappa_score(y1, y2, \\*[, …]) Cohen’s kappa: a statistic that measures inter-annotator agreement.\n",
    "#metrics.confusion_matrix(y_true, y_pred, \\*) Compute confusion matrix to evaluate the accuracy of a classification.\n",
    "#metrics.dcg_score(y_true, y_score, \\*[, k, …])Compute Discounted Cumulative Gain.\n",
    "#metrics.f1_score(y_true, y_pred, \\*[, …]) Compute the F1 score, also known as balanced F-score or F-measure\n",
    "#metrics.fbeta_score(y_true, y_pred, \\*, beta) Compute the F-beta score\n",
    "#metrics.hamming_loss(y_true, y_pred, \\*[, …])Compute the average Hamming loss.\n",
    "#metrics.hinge_loss(y_true, pred_decision, \\*)Average hinge loss (non-regularized)\n",
    "#metrics.jaccard_score(y_true, y_pred, \\*[, …])Jaccard similarity coefficient score\n",
    "#metrics.log_loss(y_true, y_pred, \\*[, eps, …]) Log loss, aka logistic loss or cross-entropy loss.\n",
    "#metrics.matthews_corrcoef(y_true, y_pred, \\*)Compute the Matthews correlation coefficient (MCC)\n",
    "#metrics.multilabel_confusion_matrix(y_true, …)Compute a confusion matrix for each class or sample\n",
    "#metrics.ndcg_score(y_true, y_score, \\*[, k, …])Compute Normalized Discounted Cumulative Gain.\n",
    "#metrics.precision_recall_curve(y_true, …)Compute precision-recall pairs for different probability thresholds\n",
    "#metrics.precision_recall_fscore_support(…)Compute precision, recall, F-measure and support for each class\n",
    "#metrics.precision_score(y_true, y_pred, \\*)Compute the precision\n",
    "#metrics.recall_score(y_true, y_pred, \\*[, …])Compute the recall\n",
    "#metrics.roc_auc_score(y_true, y_score, \\*[, …])Compute Area Under the Receiver Operating Characteristic Curve (ROC AUC) from prediction scores.\n",
    "#metrics.roc_curve(y_true, y_score, \\*[, …])Compute Receiver operating characteristic (ROC)\n",
    "#metrics.zero_one_loss(y_true, y_pred, \\*[, …])Zero-one classification loss.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "      <th>C</th>\n",
       "      <th>D</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A0</td>\n",
       "      <td>B0</td>\n",
       "      <td>A4</td>\n",
       "      <td>B4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A1</td>\n",
       "      <td>B1</td>\n",
       "      <td>A5</td>\n",
       "      <td>B5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    A   B   C   D\n",
       "0  A0  B0  A4  B4\n",
       "1  A1  B1  A5  B5"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, random_split\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "\n",
    "df = pd.read_csv(\"/home/joao/2012_Sandy_Hurricane-ontopic_offtopic.csv\", encoding='utf-8')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Create a 90-10 train-validation split. Calculate the number of samples to include in each set.\n",
    "train_size = int(0.9 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "\n",
    "# Divide the dataset by randomly selecting samples.\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "print('{:>5,} training samples'.format(train_size))\n",
    "print('{:>5,} validation samples'.format(val_size))\n",
    "\n",
    "\n",
    "# The DataLoader needs to know our batch size for training, so we specify it here. For fine-tuning BERT on a specific task, the authors recommend a batch size of 16 or 32.\n",
    "batch_size = 32\n",
    "\n",
    "# Create the DataLoaders for our training and validation sets.\n",
    "# We'll take training samples in random order.\n",
    "train_dataloader = DataLoader(train_dataset,sampler = RandomSampler(train_dataset), batch_size = batch_size )\n",
    "\n",
    "# For validation the order doesn't matter, so we'll just read them sequentially.\n",
    "validation_dataloader = DataLoader(val_dataset,sampler = SequentialSampler(val_dataset),batch_size = batch_size)\n",
    "\n",
    "\n",
    "\n",
    "# Get all of the model's parameters as a list of tuples.\n",
    "params = list(model.named_parameters())\n",
    "\n",
    "\n",
    "print('The BERT model has {:} different named parameters.\\n'.format(len(params)))\n",
    "\n",
    "print('==== Embedding Layer ====\\n')\n",
    "\n",
    "for p in params[0:5]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
    "\n",
    "print('\\n==== First Transformer ====\\n')\n",
    "\n",
    "for p in params[5:21]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
    "\n",
    "print('\\n==== Output Layer ====\\n')\n",
    "\n",
    "for p in params[-4:]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
    "\n",
    "  \n",
    "\n",
    "\n",
    "for step, batch in enumerate(train_dataloader):\n",
    "    b_input_ids = batch[0].to(device,dtype=torch.int64)\n",
    "    b_input_mask = batch[1].to(device,dtype=torch.int64)\n",
    "    b_labels = batch[2].to(device,dtype=torch.int64)\n",
    "    model.zero_grad()\n",
    "    \n",
    "    model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\",num_labels = 2,output_attentions = False,output_hidden_states = True,)\n",
    "\n",
    "\n",
    "# For each epoch...\n",
    "for epoch_i in range(0, 2):\n",
    "    model.train()\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "        model.zero_grad()\n",
    "        OUTPUT = model(b_input_ids,token_type_ids=None, attention_mask=b_input_mask,labels=b_labels)\n",
    "       \n",
    "       \n",
    "       \n",
    "       \n",
    "       len(OUTPUT)\n",
    "       OUTPUT[0].shape\n",
    "       OUTPUT[1].shape\n",
    "       OUTPUT[2].shape\n",
    "       \n",
    "       OUTPUT[0]\n",
    "       OUTPUT[1]\n",
    "       OUTPUT[2]\n",
    "       \n",
    "       loss, logits = model(b_input_ids,token_type_ids=None, attention_mask=b_input_mask,labels=b_labels)\n",
    "       loss, logits = model(b_input_ids,token_type_ids=None, attention_mask=b_input_mask,labels=b_labels)\n",
    "       loss, logits = model(b_input_ids,token_type_ids=None, attention_mask=b_input_mask,labels=b_labels)\n",
    "\n",
    "      "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
